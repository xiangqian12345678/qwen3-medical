# 分词训练

    1.什么时候需要训练分词？
        领域语料强烈偏置：   医疗/法律/专利/代码
        中文-多语言混合比例变化： 比如国外的大模型用于中文场景
        大量新词/长实体：    药名、公司名、专利号
    2.什么时候不需要训练分词？
        微调时候SFT-RL
        语域与 base model 高度一致
    3.总结一句话
        只要需要增量预训练基本都需要重新训练分词

# 分词方式

    1.BPE
        从小到大，不停合并
        通过不断合并“最常一起出现的相邻符号对”，把字符级表示逐步升级成更长、更有意义的子词单元。
    2.Unigram
        从大到小，不停删除
        假设文本由若干子词独立生成，先给一个“足够大”的候选子词集合，然后不断删除对整体语言模型概率贡献最小的子词。
    3.WordPiece
        从小到大，但用概率打分决定合并
        在给定词表约束下，选择一组子词，使整个语料在语言模型下的似然（概率）最大
    4.分词对比
        | 算法       | 适合             | 核心思想                         |
        | --------- | ---------------- |------------------------------- |
        | BPE       | 稳定、工业界主流    | 高频 pair 合并（压缩驱动）         |
        | Unigram   | 多样性好，常用于中文 | 最大化 LM 概率的合并（概率驱动）     |
        | WordPiece | 老但稳定           | **最大化 LM 概率的剪枝（删子词）**  |

# 分词选择

    BPE算法
    为什么大模型普遍选择BPE分词？
        大模型普遍选择 BPE（尤其是 Byte-level BPE），不是因为它最优，而是因为它在“规模、稳定性、工程复杂度、跨语言”之间达到了最硬核的平衡点。
    1.工程可扩展性
        能在万亿 token 规模下，稳定、可控、可复现地训练
        训练过程：纯统计 + 贪心，无 EM，无全局优化
        复杂度：时间、空间都可控
        行为：  确定性强，换机器/换批次，结果几乎一致
    2.Byte-level BPE
        彻底消灭 OOV： 
            最小单位是 byte（0–255）
            任意 Unicode 字符都可表示： emoji/罕见字/拼写错误/垃圾字符
        跨语言“无假设”：
            训练语料直接喂
    3.训练稳定性
        Unigram / WordPiece 的隐患： 
            概率建模
            子词边界不稳定 
            多切分路径
        token 边界硬
            同一字符串 → 唯一切分
            embedding 学习更稳定

# 分词评估

    Step 1：离线 Token 分析
        平均 token 数
        top token 观察
        拆分长度分布
    
    Step 2：训练稳定性
        loss 曲线
        embedding norm
    
    Step 3：成本评估
        平均 seq len
        训练 / 推理吞吐
    
    Step 4：业务指标
        搜索召回
        长尾 query 表现

# 训练实现

    1.训练领域词汇
        sh run_train.sh
    2.合并分词
        sh run_merge.sh

# 参考文档

    https://github.com/google/sentencepiece/blob/master/python/sentencepiece_python_module_example.ipynb
    命令： rum_train.sh

# 待扩展

    BPE训练评估开发