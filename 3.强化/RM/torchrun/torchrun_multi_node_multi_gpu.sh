#!/bin/bash
set -e

# =========================================================
# RM å¤šèŠ‚ç‚¹å¤š GPU åˆ†å¸ƒå¼è®­ç»ƒè„šæœ¬
# åŸºäº torchrun + DeepSpeed ZeRO1/ZeRO2/ZeRO3
# =========================================================

# ================== åˆ†å¸ƒå¼å‚æ•° ==================
ZERO_STAGE=${1:-"0"}                 # ZeROé˜¶æ®µ: 0(å…³é—­) / 1 / 2 / 3
NUM_NODES=${2:-"1"}                  # èŠ‚ç‚¹æ•°é‡
GPUS_PER_NODE=${3:-"1"}              # æ¯èŠ‚ç‚¹ GPU æ•°é‡
MASTER_ADDR=${4:-"localhost"}        # ä¸»èŠ‚ç‚¹åœ°å€
MASTER_PORT=${5:-"29501"}            # ä¸»èŠ‚ç‚¹ç«¯å£
NODE_RANK=${NODE_RANK:-"0"}          # å½“å‰èŠ‚ç‚¹ rankï¼ˆé€šè¿‡ç¯å¢ƒå˜é‡ä¼ å…¥ï¼‰

WORLD_SIZE=$NUM_NODES
NPROC_PER_NODE=$GPUS_PER_NODE

# ================== æ¨¡å‹ & æ•°æ® ==================
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
PROJECT_ROOT="$(cd "$SCRIPT_DIR/../.." && pwd)"

MODEL_PATH="$PROJECT_ROOT/model/Qwen/Qwen3-0.6B"
TOKENIZER_PATH="$PROJECT_ROOT/output/tokenizers_merge"
TRAIN_FILE_DIR="$PROJECT_ROOT/data/reward"
VAL_FILE_DIR="$PROJECT_ROOT/data/reward"
OUTPUT_DIR="$PROJECT_ROOT/output/rm_adapter"
CACHE_DIR="$PROJECT_ROOT/output/cache"

# ================== è®­ç»ƒè¶…å‚ ==================
PER_DEVICE_TRAIN_BATCH_SIZE=1
PER_DEVICE_EVAL_BATCH_SIZE=1
GRADIENT_ACCUMULATION_STEPS=8
NUM_TRAIN_EPOCHS=1
LEARNING_RATE=2e-5
SAVE_STEPS=500
SAVE_TOTAL_LIMIT=3
LOGGING_STEPS=10

# ================== DeepSpeed é…ç½® ==================
DS_CONFIG=""
case "$ZERO_STAGE" in
    1)
        DS_CONFIG="zero1.json"
        ;;
    2)
        DS_CONFIG="zero2.json"
        ;;
    3)
        DS_CONFIG="zero3.json"
        ;;
    0)
        echo "âŒ ZeROå…³é—­ï¼Œä¸ä½¿ç”¨ DeepSpeed"
        ;;
    *)
        echo "âŒ ä¸æ”¯æŒçš„ ZERO_STAGE: $ZERO_STAGE"
        exit 1
        ;;
esac

if [[ "$ZERO_STAGE" != "0" && ! -f "$DS_CONFIG" ]]; then
    echo "âŒ DeepSpeed é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: $DS_CONFIG"
    exit 1
fi

# ================== æ˜¾ç¤ºé…ç½® ==================
echo "================================================="
echo " RM åˆ†å¸ƒå¼è®­ç»ƒé…ç½®"
echo "-------------------------------------------------"
echo " ZeRO Stage         : $ZERO_STAGE"
echo " Num Nodes          : $NUM_NODES"
echo " GPUs / Node        : $GPUS_PER_NODE"
echo " World Size         : $WORLD_SIZE"
echo " Node Rank          : $NODE_RANK"
echo " Master Addr        : $MASTER_ADDR"
echo " Master Port        : $MASTER_PORT"
echo " Output Dir         : $OUTPUT_DIR"
if [[ "$ZERO_STAGE" != "0" ]]; then
    echo " DeepSpeed Config   : $DS_CONFIG"
else
    echo " DeepSpeed          : æœªå¯ç”¨"
fi
echo "================================================="

mkdir -p "$OUTPUT_DIR"
export MASTER_ADDR
export MASTER_PORT
export NODE_RANK
export WORLD_SIZE

if [[ "$NUM_NODES" -eq 1 ]]; then
    export CUDA_VISIBLE_DEVICES=$(seq -s, 0 $((GPUS_PER_NODE-1)))
fi

# ================== æ„å»ºè®­ç»ƒå‚æ•° ==================
# DeepSpeed + LoRA ç»„åˆæ—¶ä¸æ”¯æŒ gradient_checkpointingï¼Œä¼šå¯¼è‡´æ¢¯åº¦é‡å¤å½’çº¦é”™è¯¯
if [[ "$ZERO_STAGE" != "0" ]]; then
    GRADIENT_CHECKPOINTING="False"
    echo "ğŸ”§ DeepSpeed æ¨¡å¼: ç¦ç”¨ gradient_checkpointing (é¿å… LoRA å‚æ•°æ¢¯åº¦é‡å¤å½’çº¦)"
else
    GRADIENT_CHECKPOINTING="True"
fi

TRAIN_ARGS="--model_name_or_path $MODEL_PATH \
    --tokenizer_name_or_path $TOKENIZER_PATH \
    --train_file_dir $TRAIN_FILE_DIR \
    --validation_file_dir $VAL_FILE_DIR \
    --per_device_train_batch_size $PER_DEVICE_TRAIN_BATCH_SIZE \
    --per_device_eval_batch_size $PER_DEVICE_EVAL_BATCH_SIZE \
    --gradient_accumulation_steps $GRADIENT_ACCUMULATION_STEPS \
    --num_train_epochs $NUM_TRAIN_EPOCHS \
    --learning_rate $LEARNING_RATE \
    --save_steps $SAVE_STEPS \
    --save_total_limit $SAVE_TOTAL_LIMIT \
    --logging_steps $LOGGING_STEPS \
    --output_dir $OUTPUT_DIR \
    --overwrite_output_dir \
    --report_to none \
    --cache_dir $CACHE_DIR \
    --do_train \
    --do_eval \
    --use_peft True \
    --seed 42 \
    --max_train_samples 1000 \
    --max_eval_samples 10 \
    --max_source_length 1024 \
    --max_target_length 256 \
    --target_modules all \
    --lora_rank 8 \
    --lora_alpha 16 \
    --lora_dropout 0.05 \
    --bf16 True \
    --dtype bfloat16 \
    --ddp_find_unused_parameters False \
    --remove_unused_columns False \
    --gradient_checkpointing $GRADIENT_CHECKPOINTING"

# ================== DeepSpeed æ¨¡å¼ä¸‹ç§»é™¤ device_map ==================
if [[ "$ZERO_STAGE" != "0" ]]; then
    # ç§»é™¤ --device_map å‚æ•°ï¼Œå› ä¸º DeepSpeed ä¼šè‡ªåŠ¨ç®¡ç†è®¾å¤‡åˆ†é…
    TRAIN_ARGS=$(echo "$TRAIN_ARGS" | sed -e 's/--device_map auto//g' -e 's/--device_map=auto//g')
fi

# ================== å¯åŠ¨è®­ç»ƒ ==================
if [[ "$ZERO_STAGE" != "0" ]]; then
    echo "ä½¿ç”¨DeepSpeedå¯åŠ¨è®­ç»ƒ"
    TRAIN_CMD="deepspeed --num_gpus=$GPUS_PER_NODE \
        --num_nodes=$NUM_NODES \
        --master_port=$MASTER_PORT \
        --module reward_modeling \
        $TRAIN_ARGS \
        --deepspeed $DS_CONFIG"
else
    echo "ä½¿ç”¨torchrunå¯åŠ¨è®­ç»ƒ"
    TRAIN_CMD="torchrun \
        --nnodes=$WORLD_SIZE \
        --nproc_per_node=$NPROC_PER_NODE \
        --node_rank=$NODE_RANK \
        --master_addr=$MASTER_ADDR \
        --master_port=$MASTER_PORT \
        reward_modeling.py \
        $TRAIN_ARGS"
fi

echo "æ‰§è¡Œè®­ç»ƒå‘½ä»¤: $TRAIN_CMD"
eval $TRAIN_CMD

if [ $? -eq 0 ]; then
    echo ""
    echo "âœ… è®­ç»ƒå®Œæˆï¼Œè¾“å‡ºç›®å½•: $OUTPUT_DIR"
    echo "è®­ç»ƒæ—¥å¿—: ${OUTPUT_DIR}/trainer_state.json"
    echo "æ¨¡å‹æƒé‡: ${OUTPUT_DIR}/adapter_model.safetensors"
    echo "è®­ç»ƒé…ç½®: ${OUTPUT_DIR}/adapter_config.json"
else
    echo ""
    echo "âŒ è®­ç»ƒå¤±è´¥ï¼Œè¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯"
    exit 1
fi
