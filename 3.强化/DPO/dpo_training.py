
import os
from copy import deepcopy
from dataclasses import dataclass, field
from glob import glob
from typing import Dict, Optional, List

import torch
from datasets import load_dataset, DatasetDict, concatenate_datasets
from loguru import logger
from peft import LoraConfig, TaskType
from transformers import (
    AutoConfig,
    AutoModelForCausalLM,
    AutoTokenizer,
    HfArgumentParser,
    TrainingArguments,
    BitsAndBytesConfig,
)
from transformers.integrations import is_deepspeed_zero3_enabled
from trl import DPOTrainer, DPOConfig

from template import get_conv_template

os.environ["TOKENIZERS_PARALLELISM"] = "FALSE"
os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"


@dataclass
class ModelArguments:
    """
    æ¨¡å‹ç›¸å…³å‚æ•°ï¼ŒåŒ…æ‹¬æ¨¡å‹è·¯å¾„ã€é‡åŒ–é…ç½®ã€è®¾å¤‡æ˜ å°„ç­‰
    """
    model_name_or_path: Optional[str] = field(
        default=None, metadata={"help": "æ¨¡å‹æƒé‡åˆå§‹åŒ–çš„checkpointè·¯å¾„æˆ–åç§°ã€‚"}
    )
    tokenizer_name_or_path: Optional[str] = field(
        default=None, metadata={"help": "ç”¨äºåˆå§‹åŒ–tokenizerçš„è·¯å¾„æˆ–åç§°ã€‚"}
    )
    load_in_8bit: bool = field(default=False, metadata={"help": "æ˜¯å¦ä»¥8bitæ¨¡å¼åŠ è½½æ¨¡å‹ä»¥èŠ‚çœæ˜¾å­˜ã€‚"})
    load_in_4bit: bool = field(default=False, metadata={"help": "æ˜¯å¦ä»¥4bitæ¨¡å¼åŠ è½½æ¨¡å‹ä»¥è¿›ä¸€æ­¥èŠ‚çœæ˜¾å­˜ã€‚"})
    cache_dir: Optional[str] = field(
        default=None,
        metadata={"help": "å­˜æ”¾ä» HuggingFace ä¸‹è½½çš„é¢„è®­ç»ƒæ¨¡å‹çš„ç¼“å­˜ç›®å½•ã€‚"},
    )
    use_fast_tokenizer: bool = field(
        default=False,
        metadata={"help": "æ˜¯å¦ä½¿ç”¨ fast tokenizerï¼ˆåŸºäºtokenizersåº“ï¼‰ä»¥æé«˜tokenizationé€Ÿåº¦ã€‚"},
    )
    dtype: Optional[str] = field(
        default=None,
        metadata={
            "help": (
                "è¦†ç›–é»˜è®¤çš„ torch dtypeï¼Œç”¨äºåŠ è½½æ¨¡å‹æƒé‡ã€‚ä¼ å…¥ `auto` æ—¶ä¼šè‡ªåŠ¨æ ¹æ®æ¨¡å‹æƒé‡ç±»å‹é€‰æ‹©ã€‚"
            ),
            "choices": ["auto", "bfloat16", "float16", "float32"],
        },
    )
    device_map: Optional[str] = field(
        default="auto",
        metadata={"help": "æ¨¡å‹æ˜ å°„åˆ°çš„è®¾å¤‡ï¼Œå¯é€‰ 'auto' è‡ªåŠ¨é€‰æ‹©ã€‚"},
    )
    trust_remote_code: bool = field(
        default=True,
        metadata={"help": "æ˜¯å¦ä¿¡ä»»è¿œç¨‹ä»£ç ï¼Œå½“ä»è¿œç¨‹checkpointåŠ è½½æ¨¡å‹æ—¶ç”Ÿæ•ˆã€‚"},
    )

    def __post_init__(self):
        if self.model_name_or_path is None:
            raise ValueError("å¿…é¡»æŒ‡å®šæœ‰æ•ˆçš„ model_name_or_path æ‰èƒ½è¿è¡Œè®­ç»ƒã€‚")


@dataclass
class DatasetArguments:
    """
    æ•°æ®é›†ç›¸å…³å‚æ•°ï¼ŒåŒ…æ‹¬æ•°æ®æºã€é•¿åº¦é™åˆ¶ã€é¢„å¤„ç†ç­‰
    """
    dataset_name: Optional[str] = field(
        default=None, metadata={"help": "ä½¿ç”¨ HuggingFace datasets åº“åŠ è½½æ•°æ®é›†çš„åç§°ã€‚"}
    )
    dataset_config_name: Optional[str] = field(
        default=None, metadata={"help": "æ•°æ®é›†çš„é…ç½®åç§°ã€‚"}
    )
    train_file_dir: Optional[str] = field(default=None, metadata={"help": "è®­ç»ƒæ•°æ®çš„jsonlæ–‡ä»¶ç›®å½•ã€‚"})
    validation_file_dir: Optional[str] = field(default=None, metadata={"help": "éªŒè¯æ•°æ®çš„jsonlæ–‡ä»¶ç›®å½•ã€‚"})
    template_name: Optional[str] = field(default="vicuna", metadata={"help": "Promptæ¨¡æ¿åç§°ï¼Œå¦‚vicunaã€‚"})
    per_device_train_batch_size: Optional[int] = field(default=4, metadata={"help": "æ¯ä¸ªè®¾å¤‡çš„è®­ç»ƒbatchå¤§å°ã€‚"})
    per_device_eval_batch_size: Optional[int] = field(default=1, metadata={"help": "æ¯ä¸ªè®¾å¤‡çš„éªŒè¯batchå¤§å°ã€‚"})
    max_source_length: Optional[int] = field(default=2048, metadata={"help": "è¾“å…¥æ–‡æœ¬æœ€å¤§é•¿åº¦ã€‚"})
    max_target_length: Optional[int] = field(default=512, metadata={"help": "è¾“å‡ºæ–‡æœ¬æœ€å¤§é•¿åº¦ã€‚"})
    min_target_length: Optional[int] = field(default=4, metadata={"help": "è¾“å‡ºæ–‡æœ¬æœ€å°é•¿åº¦ã€‚"})
    max_train_samples: Optional[int] = field(
        default=None,
        metadata={"help": "ä¸ºäº†è°ƒè¯•æˆ–åŠ å¿«è®­ç»ƒï¼Œé™åˆ¶è®­ç»ƒæ ·æœ¬æ•°é‡ã€‚"},
    )
    max_eval_samples: Optional[int] = field(
        default=None,
        metadata={"help": "ä¸ºäº†è°ƒè¯•æˆ–åŠ å¿«è®­ç»ƒï¼Œé™åˆ¶éªŒè¯æ ·æœ¬æ•°é‡ã€‚"},
    )
    overwrite_cache: bool = field(
        default=False, metadata={"help": "æ˜¯å¦è¦†ç›–å·²æœ‰ç¼“å­˜çš„æ•°æ®é›†ã€‚"}
    )
    validation_split_percentage: Optional[int] = field(
        default=1,
        metadata={"help": "å¦‚æœè®­ç»ƒé›†ä¸­æ²¡æœ‰éªŒè¯é›†ï¼Œåˆ™æŒ‰æ­¤æ¯”ä¾‹åˆ’åˆ†éªŒè¯é›†ã€‚"},
    )
    preprocessing_num_workers: Optional[int] = field(
        default=4, metadata={"help": "æ•°æ®é¢„å¤„ç†ä½¿ç”¨çš„è¿›ç¨‹æ•°é‡ã€‚"},
    )


@dataclass
class TrainingArguments:
    """
    è®­ç»ƒç›¸å…³å‚æ•°ï¼ŒåŒ…æ‹¬ä¼˜åŒ–å™¨é…ç½®ã€LoRAè®¾ç½®ã€è®­ç»ƒç­–ç•¥ç­‰
    """
    use_peft: bool = field(default=True, metadata={"help": "æ˜¯å¦ä½¿ç”¨PEFTï¼ˆå‚æ•°é«˜æ•ˆå¾®è°ƒï¼‰ã€‚"})
    qlora: bool = field(default=False, metadata={"help": "æ˜¯å¦ä½¿ç”¨QLoRAé‡åŒ–å¾®è°ƒã€‚"})
    target_modules: Optional[str] = field(default=None, metadata={"help": "LoRAå¾®è°ƒç›®æ ‡æ¨¡å—åç§°ã€‚"})
    lora_rank: Optional[int] = field(default=8, metadata={"help": "LoRAçŸ©é˜µçš„ç§©ã€‚"})
    lora_dropout: Optional[float] = field(default=0.05, metadata={"help": "LoRAçš„dropoutæ¦‚ç‡ã€‚"})
    lora_alpha: Optional[float] = field(default=16.0, metadata={"help": "LoRAçš„ç¼©æ”¾ç³»æ•°alphaã€‚"})
    peft_path: Optional[str] = field(default=None, metadata={"help": "PEFTæ¨¡å‹è·¯å¾„ï¼Œå¯åŠ è½½å·²æœ‰å¾®è°ƒæ¨¡å‹ã€‚"})
    do_train: bool = field(default=False, metadata={"help": "æ˜¯å¦æ‰§è¡Œè®­ç»ƒè¿‡ç¨‹ã€‚"})
    do_eval: bool = field(default=False, metadata={"help": "æ˜¯å¦åœ¨éªŒè¯é›†ä¸Šæ‰§è¡Œè¯„ä¼°ã€‚"})
    learning_rate: Optional[float] = field(default=5e-4, metadata={"help": "å­¦ä¹ ç‡ã€‚"})
    lr_scheduler_type: Optional[str] = field(default="cosine", metadata={"help": "å­¦ä¹ ç‡è°ƒåº¦ç±»å‹ï¼Œå¦‚cosineã€‚"})
    warmup_steps: Optional[int] = field(default=100, metadata={"help": "é¢„çƒ­æ­¥æ•°ã€‚"})
    weight_decay: Optional[float] = field(default=0.05, metadata={"help": "æƒé‡è¡°å‡ç³»æ•°ã€‚"})
    adam_beta1: Optional[float] = field(default=0.9, metadata={"help": "Adamä¼˜åŒ–å™¨çš„beta1å‚æ•°ã€‚"})
    adam_beta2: Optional[float] = field(default=0.95, metadata={"help": "Adamä¼˜åŒ–å™¨çš„beta2å‚æ•°ã€‚"})
    optim: Optional[str] = field(default="adamw_torch", metadata={"help": "ä¼˜åŒ–å™¨ç±»å‹ã€‚"})
    fp16: Optional[bool] = field(default=True, metadata={"help": "æ˜¯å¦ä½¿ç”¨FP16è®­ç»ƒã€‚"})
    bf16: Optional[bool] = field(default=False, metadata={"help": "æ˜¯å¦ä½¿ç”¨BF16è®­ç»ƒã€‚"})
    gradient_checkpointing: Optional[bool] = field(
        default=True, metadata={"help": "æ˜¯å¦å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹ä»¥èŠ‚çœæ˜¾å­˜ã€‚"}
    )
    gradient_accumulation_steps: Optional[int] = field(
        default=4, metadata={"help": "æ¢¯åº¦ç´¯ç§¯æ­¥æ•°ï¼Œç›¸å½“äºå¢å¤§batch sizeã€‚"}
    )
    save_steps: Optional[int] = field(default=50, metadata={"help": "æ¯éš”å¤šå°‘æ­¥ä¿å­˜ä¸€æ¬¡æ¨¡å‹ã€‚"})
    eval_steps: Optional[int] = field(default=50, metadata={"help": "æ¯éš”å¤šå°‘æ­¥è¿›è¡Œä¸€æ¬¡è¯„ä¼°ã€‚"})
    logging_steps: Optional[int] = field(default=1, metadata={"help": "æ¯éš”å¤šå°‘æ­¥è®°å½•ä¸€æ¬¡æ—¥å¿—ã€‚"})
    output_dir: Optional[str] = field(default="outputs-dpo", metadata={"help": "æ¨¡å‹è¾“å‡ºä¿å­˜ç›®å½•ã€‚"})
    max_steps: Optional[int] = field(default=200, metadata={"help": "è®­ç»ƒæ€»æ­¥æ•°ã€‚"})
    eval_strategy: Optional[str] = field(default="steps", metadata={"help": "è¯„ä¼°ç­–ç•¥ï¼Œå¦‚æŒ‰æ­¥æ•°æˆ–æŒ‰epochã€‚"})
    remove_unused_columns: Optional[bool] = field(
        default=False,
        metadata={"help": "å¦‚æœä½¿ç”¨datasets.Datasetï¼Œæ˜¯å¦ç§»é™¤æœªä½¿ç”¨çš„åˆ—ã€‚"},
    )
    report_to: Optional[str] = field(default="tensorboard", metadata={"help": "æ—¥å¿—ä¸ŠæŠ¥å¹³å°ï¼Œå¦‚wandbæˆ–tensorboardã€‚"})
    deepspeed: Optional[str] = field(default=None, metadata={"help": "DeepSpeedé…ç½®æ–‡ä»¶è·¯å¾„ã€‚"})
    local_rank: int = field(default=-1, metadata={"help": "æœ¬åœ°è¿›ç¨‹æ’åï¼Œç”¨äºåˆ†å¸ƒå¼è®­ç»ƒã€‚"})


@dataclass
class ScriptArguments:
    """
    è„šæœ¬ä¸»å‚æ•°ç±»ï¼Œç»„åˆæ‰€æœ‰å‚æ•°ç±»åˆ«
    
    å‚æ•°åˆ†ä¸ºå‡ ä¸ªä¸»è¦ç±»åˆ«ï¼š
    1. æ¨¡å‹ç›¸å…³å‚æ•°ï¼ˆModel argumentsï¼‰
    2. æ•°æ®é›†ç›¸å…³å‚æ•°ï¼ˆDataset argumentsï¼‰  
    3. è®­ç»ƒç›¸å…³å‚æ•°ï¼ˆTraining argumentsï¼‰
    """
    model_args: ModelArguments = field(default_factory=ModelArguments)
    dataset_args: DatasetArguments = field(default_factory=DatasetArguments)
    training_args: TrainingArguments = field(default_factory=TrainingArguments)

    # ä¸ºäº†ä¿æŒå‘åå…¼å®¹æ€§ï¼Œæä¾›å±æ€§è®¿é—®
    @property
    def model_name_or_path(self):
        return self.model_args.model_name_or_path
    
    @property
    def tokenizer_name_or_path(self):
        return self.model_args.tokenizer_name_or_path
    
    @property
    def load_in_8bit(self):
        return self.model_args.load_in_8bit
    
    @property
    def load_in_4bit(self):
        return self.model_args.load_in_4bit
    
    @property
    def cache_dir(self):
        return self.model_args.cache_dir
    
    @property
    def use_fast_tokenizer(self):
        return self.model_args.use_fast_tokenizer
    
    @property
    def dtype(self):
        return self.model_args.dtype
    
    @property
    def device_map(self):
        return self.model_args.device_map
    
    @property
    def trust_remote_code(self):
        return self.model_args.trust_remote_code
    
    @property
    def dataset_name(self):
        return self.dataset_args.dataset_name
    
    @property
    def dataset_config_name(self):
        return self.dataset_args.dataset_config_name
    
    @property
    def train_file_dir(self):
        return self.dataset_args.train_file_dir
    
    @property
    def validation_file_dir(self):
        return self.dataset_args.validation_file_dir
    
    @property
    def template_name(self):
        return self.dataset_args.template_name
    
    @property
    def per_device_train_batch_size(self):
        return self.dataset_args.per_device_train_batch_size
    
    @property
    def per_device_eval_batch_size(self):
        return self.dataset_args.per_device_eval_batch_size
    
    @property
    def max_source_length(self):
        return self.dataset_args.max_source_length
    
    @property
    def max_target_length(self):
        return self.dataset_args.max_target_length
    
    @property
    def min_target_length(self):
        return self.dataset_args.min_target_length
    
    @property
    def max_train_samples(self):
        return self.dataset_args.max_train_samples
    
    @property
    def max_eval_samples(self):
        return self.dataset_args.max_eval_samples
    
    @property
    def overwrite_cache(self):
        return self.dataset_args.overwrite_cache
    
    @property
    def validation_split_percentage(self):
        return self.dataset_args.validation_split_percentage
    
    @property
    def preprocessing_num_workers(self):
        return self.dataset_args.preprocessing_num_workers
    
    @property
    def use_peft(self):
        return self.training_args.use_peft
    
    @property
    def qlora(self):
        return self.training_args.qlora
    
    @property
    def target_modules(self):
        return self.training_args.target_modules
    
    @property
    def lora_rank(self):
        return self.training_args.lora_rank
    
    @property
    def lora_dropout(self):
        return self.training_args.lora_dropout
    
    @property
    def lora_alpha(self):
        return self.training_args.lora_alpha
    
    @property
    def peft_path(self):
        return self.training_args.peft_path
    
    @property
    def do_train(self):
        return self.training_args.do_train
    
    @property
    def do_eval(self):
        return self.training_args.do_eval
    
    @property
    def learning_rate(self):
        return self.training_args.learning_rate
    
    @property
    def lr_scheduler_type(self):
        return self.training_args.lr_scheduler_type
    
    @property
    def warmup_steps(self):
        return self.training_args.warmup_steps
    
    @property
    def weight_decay(self):
        return self.training_args.weight_decay
    
    @property
    def adam_beta1(self):
        return self.training_args.adam_beta1
    
    @property
    def adam_beta2(self):
        return self.training_args.adam_beta2
    
    @property
    def optim(self):
        return self.training_args.optim
    
    @property
    def fp16(self):
        return self.training_args.fp16
    
    @property
    def bf16(self):
        return self.training_args.bf16
    
    @property
    def gradient_checkpointing(self):
        return self.training_args.gradient_checkpointing
    
    @property
    def gradient_accumulation_steps(self):
        return self.training_args.gradient_accumulation_steps
    
    @property
    def save_steps(self):
        return self.training_args.save_steps
    
    @property
    def eval_steps(self):
        return self.training_args.eval_steps
    
    @property
    def logging_steps(self):
        return self.training_args.logging_steps
    
    @property
    def output_dir(self):
        return self.training_args.output_dir
    
    @property
    def max_steps(self):
        return self.training_args.max_steps
    
    @property
    def eval_strategy(self):
        return self.training_args.eval_strategy
    
    @property
    def remove_unused_columns(self):
        return self.training_args.remove_unused_columns
    
    @property
    def report_to(self):
        return self.training_args.report_to
    
    @property
    def deepspeed(self):
        return self.training_args.deepspeed
    
    @property
    def local_rank(self):
        return self.training_args.local_rank


def print_trainable_parameters(model):
    """
    Prints the number of trainable parameters in the model.
    """
    trainable_params = 0
    all_param = 0
    for _, param in model.named_parameters():
        all_param += param.numel()
        if param.requires_grad:
            trainable_params += param.numel()
    logger.info(
        f"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}"
    )


def find_all_linear_names(peft_model, int4=False, int8=False):
    """Find all linear layer names in the model. reference from qlora paper."""
    cls = torch.nn.Linear
    if int4 or int8:
        import bitsandbytes as bnb
        if int4:
            cls = bnb.nn.Linear4bit
        elif int8:
            cls = bnb.nn.Linear8bitLt
    lora_module_names = set()
    for name, module in peft_model.named_modules():
        if isinstance(module, cls):
            # last layer is not add to lora_module_names
            if 'lm_head' in name:
                continue
            if 'output_layer' in name:
                continue
            names = name.split('.')
            lora_module_names.add(names[0] if len(names) == 1 else names[-1])
    return sorted(lora_module_names)


# =========================================================
# å‚æ•°è§£æ
# =========================================================

def parse_args():
    """
    è§£æå‘½ä»¤è¡Œå‚æ•°

    ä½¿ç”¨ HfArgumentParser å°†åˆ†ç¦»çš„å‚æ•°ç±»
    æ˜ å°„ä¸ºå‘½ä»¤è¡Œå‚æ•°ï¼Œç»„åˆæˆScriptArgumentså¯¹è±¡ã€‚

    Returns:
        args (ScriptArguments): è§£æåçš„è®­ç»ƒå‚æ•°
    """
    parser = HfArgumentParser((ModelArguments, DatasetArguments, TrainingArguments))
    model_args, dataset_args, training_args = parser.parse_args_into_dataclasses()
    
    # ç»„åˆæ‰€æœ‰å‚æ•°åˆ°ä¸»å‚æ•°ç±»
    args = ScriptArguments(
        model_args=model_args,
        dataset_args=dataset_args,
        training_args=training_args
    )
    logger.info(f"Parse args: {args}")
    return args


# =========================================================
# Tokenizer & Prompt Template
# =========================================================

def load_tokenizer_and_template(args):
    """
    åŠ è½½ tokenizer ä¸å¯¹è¯ prompt æ¨¡æ¿ï¼Œå¹¶è¡¥é½ DPO/SFT æ‰€éœ€çš„ç‰¹æ®Š token

    ä¸»è¦å¤„ç†ï¼š
    1. eos_tokenï¼šDPO è®­ç»ƒå¿…é¡»å­˜åœ¨
    2. bos_tokenï¼šLLaMA / Qwen ç­‰æ¨¡å‹å¯èƒ½éœ€è¦
    3. pad_tokenï¼šDataCollator / DPOTrainer éœ€è¦

    Args:
        args: è®­ç»ƒå‚æ•°

    Returns:
        tokenizer (PreTrainedTokenizer)
        prompt_template: å¯¹è¯æ¨¡æ¿å¯¹è±¡
    """
    tokenizer_kwargs = {
        "cache_dir": args.cache_dir,
        "use_fast": args.use_fast_tokenizer,
        "trust_remote_code": args.trust_remote_code,
    }

    tokenizer_name = args.tokenizer_name_or_path or args.model_name_or_path
    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name, **tokenizer_kwargs)

    # åŠ è½½å¯¹è¯æ¨¡æ¿ï¼ˆFastChat / è‡ªå®šä¹‰ï¼‰
    prompt_template = get_conv_template(args.template_name)

    # ------------------------------
    # eos_tokenï¼šDPO / SFT å¼ºä¾èµ–
    # ------------------------------
    if tokenizer.eos_token_id is None:
        tokenizer.eos_token = prompt_template.stop_str
        tokenizer.add_special_tokens({"eos_token": tokenizer.eos_token})
        logger.info(f"Add eos_token: {tokenizer.eos_token}")

    # ------------------------------
    # bos_tokenï¼šLLaMA ç³»æ¨¡å‹å¸¸è§éœ€æ±‚
    # ------------------------------
    if tokenizer.bos_token_id is None:
        tokenizer.add_special_tokens({"bos_token": tokenizer.eos_token})
        tokenizer.bos_token_id = tokenizer.eos_token_id

    # ------------------------------
    # pad_tokenï¼šbatch padding å¿…é¡»
    # ------------------------------
    if tokenizer.pad_token_id is None:
        tokenizer.pad_token = tokenizer.unk_token or tokenizer.eos_token

    return tokenizer, prompt_template

# =========================================================
# å·¥å…·å‡½æ•°
# =========================================================

def parse_comma_list(value: Optional[str]) -> Optional[List[str]]:
    """è§£æé€—å·åˆ†éš”å­—ç¬¦ä¸²ä¸º list"""
    if value is None:
        return None
    return [v.strip() for v in value.split(",") if v.strip()]


def merge_dataset_dicts(datasets: List[DatasetDict]) -> DatasetDict:
    """
    å°†å¤šä¸ª DatasetDict æŒ‰ splitï¼ˆæ•°æ®é›†åˆ’åˆ†ï¼‰åˆå¹¶ä¸ºä¸€ä¸ª DatasetDictã€‚

    è¯¥å‡½æ•°éå†æ‰€æœ‰è¾“å…¥çš„ DatasetDictï¼Œæ”¶é›†å…¶ä¸­æ‰€æœ‰çš„ split åç§°ï¼ˆå¦‚ 'train', 'validation', 'test'ï¼‰ï¼Œ
    ç„¶åå¯¹æ¯ä¸ª splitï¼Œå°†æ‰€æœ‰ DatasetDict ä¸­è¯¥ split å¯¹åº”çš„æ•°æ®é›†ä½¿ç”¨ concatenate_datasets è¿›è¡Œåˆå¹¶ã€‚

    Args:
        datasets (List[DatasetDict]): è¦åˆå¹¶çš„ DatasetDict åˆ—è¡¨ã€‚
            æ¯ä¸ª DatasetDict å¯èƒ½åŒ…å«ä¸åŒçš„ splitï¼Œå¦‚ {'train': dataset1, 'validation': dataset2}

    Returns:
        DatasetDict: åˆå¹¶åçš„ DatasetDictï¼ŒåŒ…å«æ‰€æœ‰è¾“å…¥ DatasetDict çš„æ‰€æœ‰ splitã€‚
            æ¯ä¸ª split çš„æ•°æ®æ˜¯å¯¹åº”è¾“å…¥ DatasetDict ä¸­è¯¥ split æ•°æ®çš„è¿æ¥ã€‚

    Example:
        >>> from datasets import Dataset, DatasetDict
        >>>
        >>> # åˆ›å»ºç¬¬ä¸€ä¸ª DatasetDict
        >>> ds1_data = {"text": ["hello", "world"]}
        >>> ds1 = Dataset.from_dict(ds1_data)
        >>> dict1 = DatasetDict({"train": ds1, "validation": ds1})
        >>>
        >>> # åˆ›å»ºç¬¬äºŒä¸ª DatasetDict
        >>> ds2_data = {"text": ["foo", "bar"]}
        >>> ds2 = Dataset.from_dict(ds2_data)
        >>> dict2 = DatasetDict({"train": ds2, "test": ds2})
        >>>
        >>> # åˆå¹¶ä¸¤ä¸ª DatasetDict
        >>> result = merge_dataset_dicts([dict1, dict2])
        >>>
        >>> # ç»“æœåŒ…å«æ‰€æœ‰ split: train, validation, test
        >>> # train split åŒ…å« 4 ä¸ªæ ·æœ¬: ["hello", "world", "foo", "bar"]
        >>> # validation split åŒ…å« 2 ä¸ªæ ·æœ¬: ["hello", "world"]
        >>> # test split åŒ…å« 2 ä¸ªæ ·æœ¬: ["foo", "bar"]
        >>> print(list(result.keys()))
        ['train', 'validation', 'test']
        >>> print(len(result['train']))
        4

    Input Example:
        [
            DatasetDict({
                'train': Dataset(samples: 1000),
                'validation': Dataset(samples: 200)
            }),
            DatasetDict({
                'train': Dataset(samples: 800),
                'test': Dataset(samples: 150)
            }),
            DatasetDict({
                'validation': Dataset(samples: 300),
                'test': Dataset(samples: 250)
            })
        ]

    Output Example:
        DatasetDict({
            'train': Dataset(samples: 1800),        # 1000 + 800
            'validation': Dataset(samples: 500),     # 200 + 300
            'test': Dataset(samples: 400)           # 150 + 250
        })
    """
    # åˆ›å»ºç©ºçš„ DatasetDict ç”¨äºå­˜æ”¾åˆå¹¶ç»“æœ
    merged = DatasetDict()

    # è·å–æ‰€æœ‰ DatasetDict ä¸­çš„æ‰€æœ‰ split åç§°çš„å¹¶é›†
    # ä¾‹å¦‚ï¼š[{'train', 'validation'}, {'train', 'test'}] -> {'train', 'validation', 'test'}
    all_splits = set().union(*[ds.keys() for ds in datasets])

    # éå†æ¯ä¸ª split åç§°
    for split in all_splits:
        # æ”¶é›†æ‰€æœ‰åŒ…å«è¯¥ split çš„ DatasetDict ä¸­å¯¹åº”çš„æ•°æ®é›†
        split_datasets = [
            ds[split] for ds in datasets if split in ds
        ]

        # å¦‚æœå­˜åœ¨è¯¥ split çš„æ•°æ®é›†ï¼Œåˆ™å°†å®ƒä»¬è¿æ¥èµ·æ¥
        if split_datasets:
            merged[split] = concatenate_datasets(split_datasets)

    return merged


# =========================================================
# HF Hub æ•°æ®åŠ è½½
# =========================================================

def load_from_hf_hub(args) -> Optional[DatasetDict]:
    """
    ä» HuggingFace Hub åŠ è½½ä¸€ä¸ªæˆ–å¤šä¸ªæ•°æ®é›†
    è‹¥æœªé…ç½®æˆ–åŠ è½½å¤±è´¥ï¼Œè¿”å› None
    """
    dataset_names = parse_comma_list(args.dataset_name)
    
    # å¦‚æœæ²¡æœ‰æŒ‡å®šæ•°æ®é›†åç§°ï¼Œç›´æ¥è¿”å› None
    if not dataset_names:
        return None
        
    # å¤„ç† dataset_config_name å¯èƒ½ä¸º None çš„æƒ…å†µ
    if args.dataset_config_name is None:
        dataset_configs = [None] * len(dataset_names)
    else:
        dataset_configs = [
            None if (c := config.strip()) in ("", "None", "none") else c
            for config in args.dataset_config_name.split(',')
        ]

    if len(dataset_names) != len(dataset_configs):
        raise ValueError(
            "dataset_name ä¸ dataset_config_name æ•°é‡ä¸ä¸€è‡´"
        )

    loaded = []

    for name, config in zip(dataset_names, dataset_configs):
        logger.info(f"ğŸ“¥ Loading HF dataset: {name}, config={config}")
        ds = load_dataset(
            name,
            config,
            cache_dir=args.cache_dir,
        )
        loaded.append(ds)

    return merge_dataset_dicts(loaded)


# =========================================================
# æœ¬åœ°æ–‡ä»¶æ•°æ®åŠ è½½
# =========================================================

def load_from_local_files(args) -> Optional[DatasetDict]:
    """
    ä»æœ¬åœ° JSON / JSONL æ–‡ä»¶åŠ è½½æ•°æ®é›†ï¼ˆæ”¯æŒé€’å½’æ‰«æï¼‰
    è‹¥æœªæ‰¾åˆ°æ–‡ä»¶ï¼Œè¿”å› None
    """
    data_files = {}

    if args.train_file_dir and os.path.exists(args.train_file_dir):
        train_files = glob(
            f"{args.train_file_dir}/**/*.json*", recursive=True
        )
        if train_files:
            data_files["train"] = train_files

    if args.validation_file_dir and os.path.exists(args.validation_file_dir):
        val_files = glob(
            f"{args.validation_file_dir}/**/*.json*", recursive=True
        )
        if val_files:
            data_files["validation"] = val_files

    if not data_files:
        return None

    logger.info(f"ğŸ“‚ Loading local files: {data_files}")

    return load_dataset(
        "json",
        data_files=data_files,
        cache_dir=args.cache_dir,
    )


# =========================================================
# åŸå§‹æ•°æ®é›†ç»Ÿä¸€å…¥å£
# =========================================================

def load_raw_datasets(args) -> DatasetDict:
    """
    åŠ è½½åŸå§‹æ•°æ®é›†ï¼Œæ”¯æŒï¼š
    1. HuggingFace Hub
    2. æœ¬åœ° JSON / JSONL æ–‡ä»¶

    ç‰¹æ€§ï¼š
    - ä¸¤ç±»æ•°æ®æºå¯åŒæ—¶ä½¿ç”¨
    - ä»»ä¸€ç±»æœ‰æ•°æ®å³å¯
    - è‡ªåŠ¨åˆ‡ validation
    """
    datasets = []

    # ------------------------------
    # HF Hub
    # ------------------------------
    hf_datasets = load_from_hf_hub(args)
    if hf_datasets is not None:
        datasets.append(hf_datasets)

    # ------------------------------
    # Local files
    # ------------------------------
    local_datasets = load_from_local_files(args)
    if local_datasets is not None:
        datasets.append(local_datasets)

    # ------------------------------
    # æ ¡éªŒ
    # ------------------------------
    if not datasets:
        raise ValueError(
            "æœªåŠ è½½åˆ°ä»»ä½•æ•°æ®é›†ï¼ˆHF Hub ä¸æœ¬åœ°æ–‡ä»¶å‡ä¸ºç©ºï¼‰"
        )

    raw_datasets = merge_dataset_dicts(datasets)

    # ------------------------------
    # è‡ªåŠ¨åˆ‡ validation
    # ------------------------------
    if "validation" not in raw_datasets:
        split_pct = args.validation_split_percentage
        train_len = len(raw_datasets["train"])

        if train_len == 0:
            raise ValueError("è®­ç»ƒé›†ä¸ºç©ºï¼Œæ— æ³•åˆ‡åˆ† validation")

        split_idx = int(train_len * split_pct / 100)

        logger.info(
            f"âœ‚ï¸ Auto split validation: {split_pct}% "
            f"({split_idx}/{train_len})"
        )

        raw_datasets["validation"] = raw_datasets["train"].select(
            range(split_idx)
        )
        raw_datasets["train"] = raw_datasets["train"].select(
            range(split_idx, train_len)
        )

    logger.info(f"âœ… Raw datasets loaded: {raw_datasets}")
    return raw_datasets




# =========================================================
# DPO æ•°æ®é›†æ„å»ºï¼ˆæ ¸å¿ƒï¼‰
# =========================================================

def build_dpo_datasets(args, raw_datasets, prompt_template):
    """
    å°†åŸå§‹åå¥½æ•°æ®æ„é€ æˆ DPOTrainer æ‰€éœ€æ ¼å¼ï¼š

    è¾“å‡ºå­—æ®µï¼š
    - prompt
    - chosen
    - rejected

    å¹¶è¿›è¡Œï¼š
    - prompt æ‹¼æ¥
    - é•¿åº¦è¿‡æ»¤
    - train / eval åˆ†åˆ«å¤„ç†

    Args:
        args: è®­ç»ƒå‚æ•°
        raw_datasets: åŸå§‹æ•°æ®
        prompt_template: å¯¹è¯æ¨¡æ¿

    Returns:
        train_dataset, eval_dataset
    """
    max_length = args.max_source_length + args.max_target_length

    def build_prompt_and_responses(examples) -> Dict[str, str]:
        """
        å°† system + history + question æ‹¼æ¥æˆæœ€ç»ˆ prompt
        """
        prompts = []
        for system, history, question in zip(
            examples["system"],
            examples["history"],
            examples["question"],
        ):
            system_prompt = system or ""
            history = history or []
            history_with_question = history + [[question, ""]]

            prompts.append(
                prompt_template.get_prompt(
                    messages=history_with_question,
                    system_prompt=system_prompt,
                )
            )

        return {
            "prompt": prompts,
            "chosen": examples["response_chosen"],
            "rejected": examples["response_rejected"],
        }

    def process_split(split_name, max_samples):
        """
        å¤„ç†å•ä¸ªæ•°æ®é›† splitï¼ˆtrain / validationï¼‰
        """
        ds = raw_datasets[split_name]

        if max_samples:
            ds = ds.select(range(min(len(ds), max_samples)))

        ds = ds.map(
            build_prompt_and_responses,
            batched=True,
            num_proc=args.preprocessing_num_workers,
            remove_columns=ds.column_names,
            load_from_cache_file=not args.overwrite_cache,
            desc=f"Processing {split_name} dataset",
        )

        # é•¿åº¦è¿‡æ»¤ï¼ˆé¿å… OOMï¼‰
        ds = ds.filter(
            lambda x: 0 < len(x["prompt"] + x["chosen"]) <= max_length
            and 0 < len(x["prompt"] + x["rejected"]) <= max_length
        )
        return ds

    train_dataset = (
        process_split("train", args.max_train_samples)
        if args.do_train
        else None
    )

    eval_dataset = (
        process_split("validation", args.max_eval_samples)
        if args.do_eval
        else None
    )

    return train_dataset, eval_dataset


# =========================================================
# æ¨¡å‹åŠ è½½ï¼ˆDDP / QLoRA / æ¢¯åº¦æ£€æŸ¥ç‚¹ï¼‰
# =========================================================

def load_model(args):
    """
    åŠ è½½æ¨¡å‹å¹¶å¤„ç†ï¼š
    - DDP device_map
    - DeepSpeedé›†æˆ
    - QLoRA / 4bit / 8bit
    - FP16 æ¢¯åº¦ç¨³å®šæ€§é—®é¢˜
    - Gradient Checkpointing

    Args:
        args: è®­ç»ƒå‚æ•°

    Returns:
        model (AutoModelForCausalLM)
    """
    # DeepSpeedé›†æˆ
    if args.deepspeed is not None:
        # DeepSpeedæ¨¡å¼ä¸‹ä¸è®¾ç½®device_mapï¼Œè®©DeepSpeedå¤„ç†
        device_map = None
        logger.info("ä½¿ç”¨DeepSpeedåˆ†å¸ƒå¼è®­ç»ƒï¼Œdevice_mapè®¾ä¸ºNone")
    else:
        # å•å¡æˆ–å¤šå¡DDPæ¨¡å¼
        world_size = int(os.environ.get("WORLD_SIZE", "1"))
        if world_size > 1:
            device_map = {"": int(os.environ.get("LOCAL_RANK", 0))}
        else:
            device_map = args.device_map

    dtype = (
        args.dtype
        if args.dtype in ["auto", None]
        else getattr(torch, args.dtype)
    )

    config = AutoConfig.from_pretrained(
        args.model_name_or_path,
        trust_remote_code=args.trust_remote_code,
        dtype=dtype,
        cache_dir=args.cache_dir,
    )

    # QLoRA é‡åŒ–é…ç½®
    quant_config = None
    if args.qlora:
        quant_config = BitsAndBytesConfig(
            load_in_4bit=args.load_in_4bit,
            load_in_8bit=args.load_in_8bit,
            bnb_4bit_use_double_quant=True,
            bnb_4bit_quant_type="nf4",
            bnb_4bit_compute_dtype=dtype,
        )

    model = AutoModelForCausalLM.from_pretrained(
        args.model_name_or_path,
        config=config,
        dtype=dtype,
        device_map=device_map,
        trust_remote_code=args.trust_remote_code,
        low_cpu_mem_usage=True,
        quantization_config=quant_config,
    )

    # ------------------------------
    # ä¿®å¤ DPO ä¸­å¸¸è§çš„ FP16 æ¢¯åº¦å¼‚å¸¸
    # ------------------------------
    for p in model.parameters():
        if p.requires_grad:
            p.data = p.data.float()

    # ------------------------------
    # Gradient Checkpointing
    # ------------------------------
    if args.gradient_checkpointing:
        model.gradient_checkpointing_enable()
        model.config.use_cache = False
    else:
        model.config.use_cache = True

    return model


# =========================================================
# DPOTrainer æ„å»º
# =========================================================

def build_dpo_trainer(args, model, tokenizer, train_dataset, eval_dataset):
    """
    æ„å»º DPOTrainerï¼ŒåŒ…æ‹¬ï¼š
    - DPOConfig
    - å¯é€‰ LoRA / PEFT
    - reference model å¤„ç†
    - DeepSpeedé›†æˆ

    Args:
        args: è®­ç»ƒå‚æ•°
        model: ä¸»æ¨¡å‹
        tokenizer: tokenizer
        train_dataset / eval_dataset

    Returns:
        trainer (DPOTrainer)
    """
    # DeepSpeedé›†æˆ
    deepspeed_config = None
    if args.deepspeed is not None:
        import json
        with open(args.deepspeed, 'r') as f:
            deepspeed_config = json.load(f)
        logger.info(f"åŠ è½½DeepSpeedé…ç½®: {args.deepspeed}")

    training_args = DPOConfig(
        # =========================
        # åºåˆ—é•¿åº¦ç›¸å…³
        # =========================
        # promptï¼ˆç”¨æˆ·è¾“å…¥ / æŒ‡ä»¤éƒ¨åˆ†ï¼‰çš„æœ€å¤§é•¿åº¦
        # è¶…è¿‡é•¿åº¦ä¼šè¢«æˆªæ–­ï¼Œç›´æ¥å½±å“ï¼š
        # - ä¸Šä¸‹æ–‡ä¿ç•™å®Œæ•´åº¦
        # - æ˜¾å­˜å ç”¨
        # - DPO ä¸­ preference å¯¹é½æ•ˆæœ
        max_prompt_length=args.max_source_length,

        # å•æ¡æ ·æœ¬çš„æ€»æœ€å¤§é•¿åº¦ = prompt + response
        # å¯¹ DPO æ¥è¯´ï¼Œchosen / rejected éƒ½ä¼šè¢« pad / truncate åˆ°è¿™ä¸ªé•¿åº¦
        # è®¾ç½®è¿‡å° â†’ ä¿¡æ¯ä¸¢å¤±
        # è®¾ç½®è¿‡å¤§ â†’ æ˜¾å­˜ & è®¡ç®—é‡çˆ†ç‚¸
        max_length=args.max_source_length + args.max_target_length,


        # =========================
        # Batch ç›¸å…³
        # =========================
        # å•å¡è®­ç»ƒ batch size
        # å®é™… effective batch size =
        #   per_device_train_batch_size Ã— gradient_accumulation_steps Ã— GPU æ•°
        per_device_train_batch_size=args.per_device_train_batch_size,

        # å•å¡è¯„ä¼° batch size
        # eval ä¸åä¼ ï¼Œä¸€èˆ¬å¯ä»¥æ¯” train å¤§ä¸€ç‚¹
        per_device_eval_batch_size=args.per_device_eval_batch_size,

        # æ¢¯åº¦ç´¯ç§¯æ­¥æ•°
        # ç”¨äºåœ¨æ˜¾å­˜å—é™æ—¶æ¨¡æ‹Ÿå¤§ batch
        # å¯¹ DPO æ¥è¯´ï¼Œbatch ç¨³å®šæ€§å¯¹ loss å¾ˆå…³é”®
        gradient_accumulation_steps=args.gradient_accumulation_steps,


        # =========================
        # ä¼˜åŒ–å™¨ & å­¦ä¹ ç‡
        # =========================
        # åŸºç¡€å­¦ä¹ ç‡
        # DPO é€šå¸¸æ¯” SFT å°ï¼ˆä¾‹å¦‚ 1e-6 ~ 5e-5ï¼‰
        # å¤ªå¤§å®¹æ˜“ preference å´©
        learning_rate=args.learning_rate,

        # å­¦ä¹ ç‡ warmup æ­¥æ•°
        # é˜²æ­¢ä¸€å¼€å§‹ loss çˆ†ç‚¸ï¼ŒDPO å°¤å…¶æ¨èå¼€
        warmup_steps=args.warmup_steps,

        # ä½¿ç”¨çš„ä¼˜åŒ–å™¨ç±»å‹
        # å¸¸è§ï¼šadamw_torch / adamw_bnb_8bitï¼ˆQLoRAï¼‰
        optim=args.optim,

        # æƒé‡è¡°å‡ç³»æ•°
        # å¿…é¡»ä¸DeepSpeedé…ç½®ä¿æŒä¸€è‡´
        weight_decay=args.weight_decay,

        # Adamä¼˜åŒ–å™¨çš„betaå‚æ•°
        # å¿…é¡»ä¸DeepSpeedé…ç½®ä¿æŒä¸€è‡´ [0.9, 0.95]
        adam_beta1=args.adam_beta1,
        adam_beta2=args.adam_beta2,


        # =========================
        # æ—¥å¿— & checkpoint
        # =========================
        # æ¯éš”å¤šå°‘ step æ‰“ä¸€æ¬¡æ—¥å¿—ï¼ˆloss / lr ç­‰ï¼‰
        logging_steps=args.logging_steps,

        # æ¯éš”å¤šå°‘ step ä¿å­˜ä¸€æ¬¡ checkpoint
        # DPO è®­ç»ƒå»ºè®®ä¸è¦å¤ªé¢‘ç¹ï¼ˆç£ç›˜ + IO å‹åŠ›å¤§ï¼‰
        save_steps=args.save_steps,


        # =========================
        # è¯„ä¼°ç›¸å…³
        # =========================
        # è¯„ä¼°ç­–ç•¥ï¼š
        # - "steps"ï¼šæŒ‰æ­¥æ•°è¯„ä¼°
        # - "epoch"ï¼šæ¯ä¸ª epoch è¯„ä¼°ä¸€æ¬¡
        # - "no"ï¼šä¸è¯„ä¼°
        eval_strategy=args.eval_strategy,

        # å½“ eval_strategy="steps" æ—¶ç”Ÿæ•ˆ
        # æŒ‡å®šå¤šå°‘ step è¿›è¡Œä¸€æ¬¡è¯„ä¼°
        eval_steps=args.eval_steps,


        # =========================
        # ç²¾åº¦ç›¸å…³
        # =========================
        # æ˜¯å¦ä½¿ç”¨ bfloat16
        # Ampere+ GPU æ¨èï¼Œæ•°å€¼ç¨³å®šæ€§ä¼˜äº fp16
        bf16=args.bf16,

        # æ˜¯å¦ä½¿ç”¨ fp16
        # ä¸ bf16 äºŒé€‰ä¸€ï¼Œè€ GPU æˆ–ä¸æ”¯æŒ bf16 æ—¶ä½¿ç”¨
        fp16=args.fp16,


        # =========================
        # è¾“å‡º & è¿è¡Œä¿¡æ¯
        # =========================
        # æ¨¡å‹ã€checkpointã€trainer çŠ¶æ€çš„ä¿å­˜ç›®å½•
        output_dir=args.output_dir,

        # æœ¬æ¬¡è®­ç»ƒ run çš„åç§°
        # ç”¨äºæ—¥å¿—ç³»ç»Ÿ / å®éªŒè¿½è¸ªï¼ˆå³ä½¿ report_to=None ä¹Ÿæœ‰ç”¨ï¼‰
        run_name="dpo_v1",

        # æ˜¯å¦ç§»é™¤ dataset ä¸­æ¨¡å‹ forward ç”¨ä¸åˆ°çš„å­—æ®µ
        # å¯¹è‡ªå®šä¹‰æ•°æ®ç»“æ„å¾ˆé‡è¦ï¼š
        # - Trueï¼šçœå†…å­˜ï¼Œä½†å­—æ®µåå¿…é¡»å®Œå…¨åŒ¹é…
        # - Falseï¼šæ›´å®‰å…¨ï¼Œæ¨èå¤æ‚ DPO æ•°æ®ç”¨ False
        remove_unused_columns=args.remove_unused_columns,

        # ç¦ç”¨ wandb / swanlab / tensorboard ç­‰è‡ªåŠ¨ä¸ŠæŠ¥
        # é¿å…åœ¨å†…ç½‘æˆ–æ— æƒé™ç¯å¢ƒä¸­å‡ºç°è¶…æ—¶æˆ–é˜»å¡
        report_to=None,

        # =========================
        # åˆ†å¸ƒå¼è®­ç»ƒç›¸å…³
        # =========================
        # æœ¬åœ°è¿›ç¨‹æ’åï¼Œç”¨äºåˆ†å¸ƒå¼è®­ç»ƒ
        local_rank=args.local_rank,

        # DeepSpeedé…ç½®æ–‡ä»¶è·¯å¾„
        deepspeed=args.deepspeed,

        # æ˜¯å¦åœ¨åˆ†å¸ƒå¼è®­ç»ƒä¸­ä»…ä¿å­˜ä¸»èŠ‚ç‚¹æ¨¡å‹
        ddp_find_unused_parameters=False,
        ddp_backend="nccl",
    )

    peft_config = None
    if args.use_peft:
        # å¤„ç† target_modules
        target_modules = args.target_modules.split(',') if args.target_modules else None
        if target_modules and 'all' in target_modules:
            target_modules = find_all_linear_names(model, int4=args.load_in_4bit, int8=args.load_in_8bit)
        logger.info(f"Peft target_modules: {target_modules}")
        
        peft_config = LoraConfig(
            task_type=TaskType.CAUSAL_LM,
            r=args.lora_rank,
            lora_alpha=args.lora_alpha,
            lora_dropout=args.lora_dropout,
            target_modules=target_modules,
            inference_mode=False,
        )

    trainer = DPOTrainer(
        model=model,
        ref_model=None if args.use_peft else deepcopy(model),
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=eval_dataset,
        processing_class=tokenizer,
        peft_config=peft_config,
    )

    print_trainable_parameters(trainer.model)
    return trainer


# =========================================================
# ä¸»å…¥å£
# =========================================================

def main():
    """
    DPO è®­ç»ƒä¸»å…¥å£ï¼ˆé‡æ„åï¼‰

    æ‰§è¡Œæµç¨‹ï¼š
    1. å‚æ•°è§£æ
    2. tokenizer & prompt æ¨¡æ¿åŠ è½½
    3. åŸå§‹æ•°æ®é›†åŠ è½½
    4. DPO æ•°æ®é›†æ„å»º
    5. æ¨¡å‹åŠ è½½
    6. DPOTrainer æ„å»º
    7. è®­ç»ƒ / è¯„ä¼° / ä¿å­˜
    """
    args = parse_args()

    tokenizer, prompt_template = load_tokenizer_and_template(args)
    raw_datasets = load_raw_datasets(args)

    train_dataset, eval_dataset = build_dpo_datasets(
        args, raw_datasets, prompt_template
    )

    model = load_model(args)

    trainer = build_dpo_trainer(
        args,
        model,
        tokenizer,
        train_dataset,
        eval_dataset,
    )

    if args.do_train:
        trainer.train()
        trainer.save_model(args.output_dir)
        tokenizer.save_pretrained(args.output_dir)

    if args.do_eval:
        trainer.evaluate()


if __name__ == "__main__":
    main()
