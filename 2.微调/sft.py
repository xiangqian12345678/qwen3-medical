import math
import os
import sys
from dataclasses import dataclass, field
from glob import glob
from types import MethodType
from typing import Literal, Optional, Tuple

import torch
import torch.utils.data
from datasets import load_dataset
from loguru import logger
from peft import LoraConfig, TaskType, get_peft_model, PeftModel, prepare_model_for_kbit_training
from transformers import (
    AutoConfig,
    AutoModelForCausalLM,
    AutoTokenizer,
    HfArgumentParser,
    Trainer,
    Seq2SeqTrainingArguments,
    set_seed,
    BitsAndBytesConfig,
    DataCollatorForSeq2Seq,
)
from transformers.trainer import TRAINING_ARGS_NAME
from transformers.trainer_pt_utils import LabelSmoother
from transformers.utils.versions import require_version
from transformers.integrations import is_deepspeed_zero3_enabled

is_flash_attn_2_available = False
try:
    from flash_attn import flash_attn_func, flash_attn_varlen_func
    from flash_attn.bert_padding import pad_input, unpad_input

    is_flash_attn_2_available = True
except ImportError:
    is_flash_attn_2_available = False

from template import get_conv_template


@dataclass
class ModelArguments:
    """
    ä¸æ¨¡å‹ / é…ç½® / tokenizer ç›¸å…³çš„å‚æ•°
    ç”¨äºæŒ‡å®šï¼šåŠ è½½å“ªä¸ªæ¨¡å‹ã€å¦‚ä½•åŠ è½½ã€ç”¨ä»€ä¹ˆç²¾åº¦ã€æ˜¯å¦å¯ç”¨é«˜çº§ç‰¹æ€§ç­‰
    """
    model_name_or_path: Optional[str] = field(
        default=None,
        metadata={
            "help": (
                "The model checkpoint for weights initialization. "
                "Don't set if you want to train a model from scratch."
            )
        },
    )
    load_in_8bit: bool = field(
        default=False,
        metadata={"help": "Whether to load the model in 8bit mode or not."}
    )
    load_in_4bit: bool = field(
        default=False,
        metadata={"help": "Whether to load the model in 4bit mode or not."}
    )
    tokenizer_name_or_path: Optional[str] = field(
        default=None,
        metadata={
            "help": (
                "The tokenizer for weights initialization. "
                "Don't set if you want to train a model from scratch."
            )
        },
    )
    cache_dir: Optional[str] = field(
        default=None,
        metadata={
            "help": "Where do you want to store the pretrained models downloaded from huggingface.co"
        },
    )
    model_revision: Optional[str] = field(
        default="main",
        metadata={
            "help": "The specific model version to use (can be a branch name, tag name or commit id)."
        },
    )
    hf_hub_token: Optional[str] = field(
        default=None,
        metadata={"help": "Auth token to log in with Hugging Face Hub."}
    )
    use_fast_tokenizer: bool = field(
        default=False,
        metadata={
            "help": "Whether to use one of the fast tokenizer (backed by the tokenizers library) or not."
        },
    )
    dtype: Optional[str] = field(
        default="float16",
        metadata={
            "help": (
                "Override the default `torch.dtype` and load the model under this dtype. "
                "If `auto` is passed, the dtype will be automatically derived from the model's weights."
            ),
            "choices": ["auto", "bfloat16", "float16", "float32"],
        },
    )
    device_map: Optional[str] = field(
        default="auto",
        metadata={
            "help": (
                "Device to map model to. "
                "If `auto` is passed, the device will be selected automatically."
            )
        },
    )
    trust_remote_code: bool = field(
        default=True,
        metadata={
            "help": "Whether to trust remote code when loading a model from a remote checkpoint."
        },
    )
    rope_scaling: Optional[Literal["linear", "dynamic"]] = field(
        default=None,
        metadata={"help": "Adopt scaled rotary positional embeddings."}
    )
    flash_attn: Optional[bool] = field(
        default=False,
        metadata={"help": "Enable FlashAttention-2 for faster training."}
    )
    shift_attn: Optional[bool] = field(
        default=False,
        metadata={
            "help": "Enable shifted sparse attention (S^2-Attn) proposed by LongLoRA."
        }
    )
    neft_alpha: Optional[float] = field(
        default=0,
        metadata={
            "help": "The alpha parameter to control the noise magnitude in NEFTune. value can be 5."
        }
    )

    def __post_init__(self):
        if self.model_name_or_path is None:
            raise ValueError("You must specify a valid model_name_or_path to run training.")


@dataclass
class DataArguments:
    """
    Arguments pertaining to what data we are going to input our model for training and eval.
    """
    dataset_name: Optional[str] = field(
        default=None, metadata={"help": "The name of the dataset to use (via the datasets library)."}
    )
    dataset_config_name: Optional[str] = field(
        default=None, metadata={"help": "The configuration name of the dataset to use (via the datasets library)."}
    )
    train_file_dir: Optional[str] = field(default=None, metadata={"help": "The train jsonl data file folder."})
    validation_file_dir: Optional[str] = field(default=None, metadata={"help": "The evaluation jsonl file folder."})
    max_train_samples: Optional[int] = field(
        default=None,
        metadata={
            "help": (
                "For debugging purposes or quicker training, truncate the number of training examples to this "
                "value if set."
            )
        },
    )
    max_eval_samples: Optional[int] = field(
        default=None,
        metadata={
            "help": (
                "For debugging purposes or quicker training, truncate the number of evaluation examples to this "
                "value if set."
            )
        },
    )
    ignore_pad_token_for_loss: bool = field(
        default=True,
        metadata={"help": "If only pad tokens should be ignored. This assumes that `config.pad_token_id` is defined."},
    )
    overwrite_cache: bool = field(
        default=False, metadata={"help": "Overwrite the cached training and evaluation sets"}
    )
    validation_split_percentage: Optional[int] = field(
        default=1,
        metadata={
            "help": "The percentage of the train set used as validation set in case there's no validation split"
        },
    )
    preprocessing_num_workers: Optional[int] = field(
        default=None,
        metadata={"help": "The number of processes to use for the preprocessing."},
    )

    def __post_init__(self):
        if self.max_train_samples is not None and 0 < self.max_train_samples <= 1000:
            logger.warning("You may set max_train_samples = -1 to run all samples in production.")


@dataclass
class ScriptArguments:
    use_peft: bool = field(
        default=True,
        metadata={"help": "Whether to use peft"}
    )
    train_on_inputs: bool = field(
        default=False,
        metadata={"help": "Whether to train on inputs"}
    )
    target_modules: Optional[str] = field(
        default="all"
    )
    lora_rank: Optional[int] = field(
        default=8
    )
    lora_dropout: Optional[float] = field(
        default=0.05
    )
    lora_alpha: Optional[float] = field(
        default=32.0
    )
    modules_to_save: Optional[str] = field(
        default=None
    )
    peft_path: Optional[str] = field(
        default=None,
        metadata={"help": "The path to the peft model"}
    )
    qlora: bool = field(
        default=False,
        metadata={"help": "Whether to use qlora"}
    )
    model_max_length: int = field(
        default=512,
        metadata={
            "help": (
                "Maximum model context length. "
                "suggest: 8192 * 4, 8192 * 2, 8192, 4096, 2048, 1024, 512"
            )
        }
    )
    template_name: Optional[str] = field(
        default="vicuna",
        metadata={"help": "The prompt template name."}
    )

    def __post_init__(self):
        if self.model_max_length < 60:
            raise ValueError(
                "You must specify a valid model_max_length >= 60 to run training"
            )


class SavePeftModelTrainer(Trainer):
    """
    Trainer for lora models
    """

    def save_model(self, output_dir=None, _internal_call=False):
        """Save the LoRA model."""
        os.makedirs(output_dir, exist_ok=True)
        torch.save(self.args, os.path.join(output_dir, TRAINING_ARGS_NAME))
        self.model.save_pretrained(output_dir)


def save_model(model, tokenizer, args):
    """Save the model and the tokenizer."""
    output_dir = args.output_dir
    os.makedirs(output_dir, exist_ok=True)
    model_to_save = model.module if hasattr(model, "module") else model
    model_to_save.save_pretrained(output_dir)
    tokenizer.save_pretrained(output_dir)


def save_model_zero3(model, tokenizer, args, trainer):
    """Save the model for deepspeed zero3."""
    output_dir = args.output_dir
    os.makedirs(output_dir, exist_ok=True)
    state_dict_zero3 = trainer.model_wrapped._zero3_consolidated_16bit_state_dict()
    model_to_save = model.module if hasattr(model, "module") else model
    model_to_save.save_pretrained(args.output_dir, state_dict=state_dict_zero3)
    tokenizer.save_pretrained(output_dir)


def print_trainable_parameters(model):
    """Prints the number of trainable parameters in the model."""
    trainable_params = 0
    all_param = 0
    for _, param in model.named_parameters():
        all_param += param.numel()
        if param.requires_grad:
            trainable_params += param.numel()
    if all_param > 0:
        print(
            f"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param:.2f}"
        )
    else:
        print("No parameters found in the model (possibly using DeepSpeed ZeRO optimization)")


def find_all_linear_names(peft_model, int4=False, int8=False):
    """æ‰¾å‡ºæ¨¡å‹ä¸­æ‰€æœ‰å¯ç”¨äº LoRA æ³¨å…¥çš„ Linear å±‚åç§°"""
    cls = torch.nn.Linear
    if int4 or int8:
        import bitsandbytes as bnb
        if int4:
            cls = bnb.nn.Linear4bit
        elif int8:
            cls = bnb.nn.Linear8bitLt

    lora_module_names = set()
    for name, module in peft_model.named_modules():
        if isinstance(module, cls):
            if 'lm_head' in name:
                continue
            if 'output_layer' in name:
                continue
            names = name.split('.')
            lora_module_names.add(names[0] if len(names) == 1 else names[-1])

    return sorted(lora_module_names)


def check_and_optimize_memory():
    """æ£€æŸ¥å¹¶ä¼˜åŒ– GPU æ˜¾å­˜ä½¿ç”¨æƒ…å†µ"""
    if not torch.cuda.is_available():
        return

    logger.info("ğŸ” æ£€æŸ¥GPUå†…å­˜çŠ¶æ€...")
    torch.cuda.empty_cache()
    num_gpus = torch.cuda.device_count()

    for i in range(num_gpus):
        props = torch.cuda.get_device_properties(i)
        total_memory = props.total_memory / 1024 ** 3
        allocated = torch.cuda.memory_allocated(i) / 1024 ** 3
        cached = torch.cuda.memory_reserved(i) / 1024 ** 3
        free = total_memory - allocated - cached

        logger.info(f"GPU {i} ({props.name}):")
        logger.info(f"  æ€»å†…å­˜: {total_memory:.1f}GB")
        logger.info(f"  å·²åˆ†é…: {allocated:.1f}GB")
        logger.info(f"  å·²ç¼“å­˜: {cached:.1f}GB")
        logger.info(f"  å¯ç”¨: {free:.1f}GB")

    if hasattr(torch.backends.cuda, 'enable_flash_sdp'):
        torch.backends.cuda.enable_flash_sdp(True)
        logger.info("âœ… å¯ç”¨Flash Attentionä¼˜åŒ–")

    if hasattr(torch.backends.cuda, 'enable_mem_efficient_sdp'):
        torch.backends.cuda.enable_mem_efficient_sdp(True)
        logger.info("âœ… å¯ç”¨å†…å­˜é«˜æ•ˆæ³¨æ„åŠ›æœºåˆ¶")


def get_dialog_from_examples(examples, prompt_template, roles):
    """
    ä»è®­ç»ƒæ ·æœ¬ä¸­æå–è§„èŒƒåŒ–åçš„å¯¹è¯æ–‡æœ¬ï¼Œå¹¶ç”Ÿæˆæœ€ç»ˆ prompt

    å‚æ•°è¯´æ˜ï¼š
    - examples: æ•°æ®é›†ä¸­çš„ä¸€ä¸ª batchï¼Œé€šå¸¸åŒ…å«ï¼š
        - examples["conversations"]: å¤šè½®å¯¹è¯åˆ—è¡¨
        - examples["system_prompt"]ï¼ˆå¯é€‰ï¼‰: ä¸æ ·æœ¬ä¸€ä¸€å¯¹åº”çš„ system prompt
    - prompt_template: prompt æ¨¡æ¿å¯¹è±¡ï¼Œè´Ÿè´£å°†å¯¹è¯å†å²æ‹¼è£…æˆæ¨¡å‹è¾“å…¥æ ¼å¼
    - roles: è§’è‰²é¡ºåºå®šä¹‰ï¼Œä¾‹å¦‚ ["user", "assistant"]

    äº§å‡ºï¼š
    - yield ç»è¿‡ prompt_template å¤„ç†åçš„å®Œæ•´å¯¹è¯å­—ç¬¦ä¸²ï¼ˆgeneratorï¼‰

    æ ·ä¾‹ï¼š
    - è¾“å…¥ï¼š
        examples = {
            "conversations": [
                [
                    {"from": "system", "value": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„åŒ»ç–—åŠ©æ‰‹ï¼Œè¯·æä¾›å‡†ç¡®çš„åŒ»ç–—å»ºè®®ã€‚"},
                    {"from": "human", "value": "æ²»ç–—é˜³ç—¿åƒä»€ä¹ˆè¯å‘¢ï¼Ÿ"},
                    {"from": "gpt", "value": "ç”·å­æ—©æ³„ã€æ—©æ³„ç—…ç—‡çš„å†æ¬¡å‘ç”Ÿï¼Œå¤šç”±æ£æƒ…çºµæ¬²..."}
                ],
                [
                    {"from": "human", "value": "ä¸¤åªè„šæ˜æ˜¾å¤§å°ä¸ä¸€æ ·ï¼Œè¯¥æ€ä¹ˆåŠï¼Ÿ"},
                    {"from": "gpt", "value": "ä¸èµ°è·¯å§¿åŠ¿æ²¡æœ‰å…³ç³»çš„ï¼Œäººçš„å™¨å®˜ï¼Œæ²¡æœ‰å®Œå…¨å¯¹ç§°çš„..."}
                ]
            ],
            "system_prompt": ["é»˜è®¤ç³»ç»Ÿæç¤º", "é»˜è®¤ç³»ç»Ÿæç¤º"]
        }

        roles = ["human", "gpt"]
    - è¾“å‡ºï¼š
        # ç¬¬ä¸€æ¡å¯¹è¯çš„è¾“å‡ºï¼ˆåŒ…å«system promptï¼‰
        "ç³»ç»Ÿï¼šä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„åŒ»ç–—åŠ©æ‰‹ï¼Œè¯·æä¾›å‡†ç¡®çš„åŒ»ç–—å»ºè®®ã€‚\nç”¨æˆ·ï¼šæ²»ç–—é˜³ç—¿åƒä»€ä¹ˆè¯å‘¢ï¼Ÿ\nåŠ©æ‰‹ï¼šç”·å­æ—©æ³„ã€æ—©æ³„ç—…ç—‡çš„å†æ¬¡å‘ç”Ÿï¼Œå¤šç”±æ£æƒ…çºµæ¬²..."

        # ç¬¬äºŒæ¡å¯¹è¯çš„è¾“å‡ºï¼ˆä½¿ç”¨batchçº§åˆ«çš„system promptï¼‰
        "ç³»ç»Ÿï¼šé»˜è®¤ç³»ç»Ÿæç¤º\nç”¨æˆ·ï¼šä¸¤åªè„šæ˜æ˜¾å¤§å°ä¸ä¸€æ ·ï¼Œè¯¥æ€ä¹ˆåŠï¼Ÿ\nåŠ©æ‰‹ï¼šä¸èµ°è·¯å§¿åŠ¿æ²¡æœ‰å…³ç³»çš„ï¼Œäººçš„å™¨å®˜ï¼Œæ²¡æœ‰å®Œå…¨å¯¹ç§°çš„..."
    """

    # å– batch çº§åˆ«çš„ system_promptï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    system_prompts = examples.get("system_prompt", "")

    # éå† batch ä¸­çš„æ¯æ¡å¯¹è¯æ ·æœ¬
    for i, source in enumerate(examples['conversations']):
        system_prompt = ""

        # è‡³å°‘éœ€è¦ä¸€é—®ä¸€ç­”ï¼Œé•¿åº¦ä¸è¶³ç›´æ¥è·³è¿‡
        if len(source) < 2:
            continue

        # è¯»å–ç¬¬ä¸€æ¡æ¶ˆæ¯çš„è§’è‰²
        data_role = source[0].get("from", "")

        # å¦‚æœç¬¬ä¸€æ¡æ˜¯ system è§’è‰²ï¼Œåˆ™å•ç‹¬æŠ½å– system_prompt
        if data_role == "system":
            system_prompt = source[0]["value"]
            source = source[1:]  # å»æ‰ system æ¶ˆæ¯
            data_role = source[0].get("from", "")

        # å¦‚æœé¦–æ¡æ¶ˆæ¯ä¸æ˜¯ roles[0]ï¼ˆå¦‚ userï¼‰ï¼Œåˆ™è·³è¿‡ç¬¬ä¸€æ¡
        # ç”¨äºä¿®å¤æ•°æ®ä¸­ä¸è§„èŒƒçš„å¯¹è¯èµ·å§‹
        if data_role not in roles or data_role != roles[0]:
            source = source[1:]

        # å†æ¬¡æ ¡éªŒï¼Œç¡®ä¿è‡³å°‘è¿˜æœ‰ä¸€é—®ä¸€ç­”
        if len(source) < 2:
            continue

        messages = []

        # éå†å‰©ä½™å¯¹è¯å†…å®¹
        for j, sentence in enumerate(source):
            data_role = sentence.get("from", "")

            # å‡ºç°æœªçŸ¥è§’è‰²ï¼Œç›´æ¥ä¸¢å¼ƒæ•´æ¡æ ·æœ¬
            if data_role not in roles:
                logger.warning(f"unknown role: {data_role}, {i}. (ignored)")
                break

            # æ ¡éªŒè§’è‰²æ˜¯å¦ç¬¦åˆ user/assistant äº¤æ›¿é¡ºåº
            if data_role == roles[j % 2]:
                messages.append(sentence["value"])

        # æ¶ˆæ¯æ•°å¿…é¡»ä¸ºå¶æ•°ï¼ˆuser/assistant æˆå¯¹ï¼‰
        if len(messages) % 2 != 0:
            continue

        # å°†æ¶ˆæ¯æ•´ç†æˆ [[user, assistant], ...] çš„å†å²å¯¹è¯ç»“æ„
        history_messages = [
            [messages[k], messages[k + 1]]
            for k in range(0, len(messages), 2)
        ]

        # å¦‚æœå¯¹è¯å†…æ²¡æœ‰ system_promptï¼Œåˆ™å°è¯•ä½¿ç”¨ batch çº§ system_prompt
        if not system_prompt:
            system_prompt = system_prompts[i] if system_prompts else ""

        # ä½¿ç”¨æ¨¡æ¿ç”Ÿæˆæœ€ç»ˆ promptï¼ˆé€šå¸¸æ˜¯æ¨¡å‹çš„è¾“å…¥æ–‡æœ¬ï¼‰
        yield prompt_template.get_dialog(
            history_messages,
            system_prompt=system_prompt
        )


def preprocess_dialogue_data(dialog, tokenizer, max_length, script_args, IGNORE_INDEX):
    """
    é¢„å¤„ç†å¯¹è¯æ•°æ®ï¼Œå°†å¯¹è¯æ ¼å¼è½¬æ¢ä¸ºæ¨¡å‹è®­ç»ƒæ‰€éœ€çš„input_idså’Œlabels
    
    å‚æ•°:
        dialog: list - å¯¹è¯æ•°æ®ï¼Œæ ¼å¼ä¸º[ç”¨æˆ·1, åŠ©æ‰‹1, ç”¨æˆ·2, åŠ©æ‰‹2, ...]
        tokenizer: object - åˆ†è¯å™¨å¯¹è±¡ï¼Œç”¨äºæ–‡æœ¬ç¼–ç 
        max_length: int - åºåˆ—æœ€å¤§é•¿åº¦é™åˆ¶
        script_args: object - è„šæœ¬å‚æ•°é…ç½®ï¼ŒåŒ…å«train_on_inputsç­‰è®­ç»ƒè®¾ç½®
        IGNORE_INDEX: int - å¿½ç•¥æ ‡è®°ï¼Œç”¨äºlabelsä¸­è¡¨ç¤ºä¸å‚ä¸lossè®¡ç®—çš„ä½ç½®
    
    è¿”å›:
        tuple: (input_ids, labels) - å¤„ç†åçš„è¾“å…¥IDåºåˆ—å’Œå¯¹åº”çš„æ ‡ç­¾åºåˆ—
    
    ä½¿ç”¨ç¤ºä¾‹:
    ```python
    dialog = [
        "æ²»ç–—é˜³ç—¿åƒä»€ä¹ˆè¯å‘¢ï¼Ÿæ€§ç”Ÿæ´»ä¸€ç›´å¾ˆæ­£å¸¸çš„ï¼Œä½†æ˜¯è¿™æ®µæ—¶é—´æ„Ÿè§‰æ€§æ¬²å˜ä½äº†ï¼Œæœ‰æ—¶å‹ƒèµ·éƒ½æ„Ÿè§‰å¾ˆå›°éš¾ã€‚",
        "ç”·å­æ—©æ³„ã€æ—©æ³„ç—…ç—‡çš„å†æ¬¡å‘ç”Ÿï¼Œå¤šç”±æ£æƒ…çºµæ¬²ï¼Œæˆ–é’å¹´è¯¯çŠ¯æ€§äº¤ï¼Œè‡³å‘½é—¨ç«è¡°ï¼Œç²¾æ°”è™šå¯’ï¼›æˆ–å› æ¹¿çƒ­ä¸‹æ³¨ï¼Œå®—ç­‹å¼›è€Œç—¿çš„ã€‚",
        "éœ€è¦åšä»€ä¹ˆæ£€æŸ¥ï¼Ÿ",
        "å»ºè®®åˆ°åŒ»é™¢åšç›¸å…³æ£€æŸ¥ï¼ŒåŒ…æ‹¬è¡€æ¶²æ£€æŸ¥ã€æ¿€ç´ æ°´å¹³æ£€æµ‹ç­‰ã€‚"
    ]
    
    tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen3-0.6B")
    max_length = 512
    script_args = argparse.Namespace()
    script_args.train_on_inputs = False
    IGNORE_INDEX = -100
    
    # è°ƒç”¨å‡½æ•°
    input_ids, labels = preprocess_dialogue_data(dialog, tokenizer, max_length, script_args, IGNORE_INDEX)
    
    # é¢„æœŸè¾“å‡ºï¼š
    # input_ids: [ç”¨æˆ·1çš„tokenåºåˆ—] + [åŠ©æ‰‹1çš„tokenåºåˆ—] + [eos_token] + [ç”¨æˆ·2çš„tokenåºåˆ—] + [åŠ©æ‰‹2çš„tokenåºåˆ—] + [eos_token]
    # labels: [-100, -100, -100, ..., åŠ©æ‰‹1çš„tokenåºåˆ—, eos_token, -100, -100, ..., åŠ©æ‰‹2çš„tokenåºåˆ—, eos_token]
    ```
    """
    input_ids, labels = [], []  # åˆå§‹åŒ–è¾“å…¥IDå’Œæ ‡ç­¾åˆ—è¡¨

    # éå†å¯¹è¯ï¼Œæ¯æ¬¡å¤„ç†ä¸€ç»„ç”¨æˆ·-åŠ©æ‰‹å¯¹è¯
    for i in range(len(dialog) // 2):
        # è·å–ç”¨æˆ·è¾“å…¥å’ŒåŠ©æ‰‹å›å¤çš„æ–‡æœ¬
        user_text = dialog[2 * i]  # ç”¨æˆ·è¾“å…¥
        assistant_text = dialog[2 * i + 1]  # åŠ©æ‰‹å›å¤

        # ä½¿ç”¨åˆ†è¯å™¨ç¼–ç æ–‡æœ¬
        # ç”¨æˆ·è¾“å…¥ï¼šç¬¬ä¸€ä¸ªå¯¹è¯æ—¶æ·»åŠ ç‰¹æ®Štokenï¼ˆå¦‚bos_tokenï¼‰ï¼Œåç»­ä¸æ·»åŠ 
        source_ids = tokenizer.encode(text=user_text, add_special_tokens=(i == 0))
        # åŠ©æ‰‹å›å¤ï¼šä¸æ·»åŠ ç‰¹æ®Štoken
        target_ids = tokenizer.encode(text=assistant_text, add_special_tokens=False)

        # è®¡ç®—æ€»é•¿åº¦ï¼Œç”¨äºæŒ‰æ¯”ä¾‹åˆ†é…æœ€å¤§é•¿åº¦
        total_len = len(source_ids) + len(target_ids)
        # æ ¹æ®æºæ–‡æœ¬é•¿åº¦å æ¯”è®¡ç®—æœ€å¤§æºæ–‡æœ¬é•¿åº¦
        max_source_len = int(max_length * (len(source_ids) / total_len))
        # æ ¹æ®ç›®æ ‡æ–‡æœ¬é•¿åº¦å æ¯”è®¡ç®—æœ€å¤§ç›®æ ‡æ–‡æœ¬é•¿åº¦
        max_target_len = int(max_length * (len(target_ids) / total_len))

        # æˆªæ–­è¿‡é•¿çš„åºåˆ—
        if len(source_ids) > max_source_len:
            source_ids = source_ids[:max_source_len]
        if len(target_ids) > max_target_len - 1:  # -1ä¸ºåé¢çš„eos_tokené¢„ç•™ç©ºé—´
            target_ids = target_ids[:max_target_len - 1]

        # å¤„ç†ç‰¹æ®Štokenï¼Œé¿å…é‡å¤
        # å¦‚æœæºæ–‡æœ¬ä»¥eos_tokenå¼€å¤´ï¼Œç§»é™¤å®ƒ
        if len(source_ids) > 0 and source_ids[0] == tokenizer.eos_token_id:
            source_ids = source_ids[1:]
        # å¦‚æœç›®æ ‡æ–‡æœ¬ä»¥eos_tokenç»“å°¾ï¼Œç§»é™¤å®ƒï¼ˆåé¢ä¼šç»Ÿä¸€æ·»åŠ ï¼‰
        if len(target_ids) > 0 and target_ids[-1] == tokenizer.eos_token_id:
            target_ids = target_ids[:-1]

        # æ£€æŸ¥æ˜¯å¦ä¼šè¶…å‡ºæœ€å¤§é•¿åº¦é™åˆ¶
        if len(input_ids) + len(source_ids) + len(target_ids) + 1 > max_length:
            break

        # æ„å»ºè¾“å…¥åºåˆ—ï¼šæºæ–‡æœ¬ + ç›®æ ‡æ–‡æœ¬ + eos_token
        input_ids += source_ids + target_ids + [tokenizer.eos_token_id]

        # æ„å»ºæ ‡ç­¾åºåˆ—
        if script_args.train_on_inputs:
            # å¦‚æœè®­ç»ƒæ—¶åŒ…å«è¾“å…¥ï¼Œåˆ™å…¨éƒ¨tokenéƒ½å‚ä¸lossè®¡ç®—
            labels += source_ids + target_ids + [tokenizer.eos_token_id]
        else:
            # å¦‚æœä¸è®­ç»ƒè¾“å…¥ï¼Œåˆ™æºæ–‡æœ¬éƒ¨åˆ†è®¾ä¸ºIGNORE_INDEXï¼Œåªè®¡ç®—ç›®æ ‡éƒ¨åˆ†çš„loss
            labels += [IGNORE_INDEX] * len(source_ids) + target_ids + [tokenizer.eos_token_id]

    return input_ids, labels


def preprocess_function(examples, tokenizer, max_length, script_args, IGNORE_INDEX, prompt_template):
    """
    æ•°æ®é¢„å¤„ç†å‡½æ•° - å°†åŸå§‹å¯¹è¯æ•°æ®è½¬æ¢ä¸ºæ¨¡å‹è®­ç»ƒæ‰€éœ€çš„æ ¼å¼

    åŠŸèƒ½è¯´æ˜ï¼š
    - æ‰¹é‡å¤„ç†å¯¹è¯æ ·æœ¬ï¼Œå°†å…¶è½¬æ¢ä¸ºæ¨¡å‹è¾“å…¥æ ¼å¼
    - ç”Ÿæˆinput_idsï¼ˆtokenåºåˆ—ï¼‰ã€attention_maskï¼ˆæ³¨æ„åŠ›æ©ç ï¼‰ã€labelsï¼ˆæ ‡ç­¾ï¼‰ä¸‰ä¸ªå…³é”®å­—æ®µ
    - æ”¯æŒå¤šè½®å¯¹è¯å¤„ç†ï¼Œè‡ªåŠ¨å¤„ç†human/gptè§’è‰²äº¤æ›¿

    å‚æ•°è¯´æ˜ï¼š
    - examples: åŸå§‹å¯¹è¯æ•°æ®æ ·æœ¬åˆ—è¡¨
    - tokenizer: åˆ†è¯å™¨å¯¹è±¡ï¼Œç”¨äºæ–‡æœ¬tokenization
    - max_length: åºåˆ—æœ€å¤§é•¿åº¦é™åˆ¶
    - script_args: è„šæœ¬å‚æ•°é…ç½®å­—å…¸
    - IGNORE_INDEX: å¿½ç•¥ç´¢å¼•å€¼ï¼Œç”¨äºmaskä¸éœ€è¦è®¡ç®—lossçš„token
    - prompt_template: æç¤ºè¯æ¨¡æ¿ï¼Œç”¨äºæ ¼å¼åŒ–å¯¹è¯

    è¾“å…¥æ ·ä¾‹ï¼š
    examples = [
        {
            "conversations": [
                {"from": "human", "value": "ä½ å¥½ï¼Œè¯·é—®å¤´ç–¼æ€ä¹ˆåŠï¼Ÿ"},
                {"from": "gpt", "value": "å¤´ç–¼å¯èƒ½æ˜¯ç”±å¤šç§åŸå› å¼•èµ·çš„ï¼Œå»ºè®®æ‚¨å…ˆä¼‘æ¯ï¼Œå¦‚æœæŒç»­ä¸ç¼“è§£è¯·åŠæ—¶å°±åŒ»ã€‚"}
            ]
        }
    ]
    tokenizer = AutoTokenizer.from_pretrained("qwen-model")
    max_length = 512
    script_args = {"padding": "max_length", "truncation": True}
    IGNORE_INDEX = -100
    prompt_template = "{role}: {content}\n"

    è¾“å‡ºæ ·ä¾‹ï¼š
    {
        "input_ids": [[1, 234, 567, 890, 123, 456, 789, 2]],  # tokenåºåˆ—
        "attention_mask": [[1, 1, 1, 1, 1, 1, 1, 1]],           # æ³¨æ„åŠ›æ©ç 
        "labels": [[-100, -100, 567, 890, 123, 456, 789, 2]]  # æ ‡ç­¾ï¼ˆhumanéƒ¨åˆ†ä¸º-100ï¼‰
    }
    """
    input_ids_list = []
    attention_mask_list = []
    targets_list = []
    roles = ["human", "gpt"]

    for dialog in get_dialog_from_examples(examples, prompt_template, roles):
        input_ids, labels = preprocess_dialogue_data(
            dialog, tokenizer, max_length, script_args, IGNORE_INDEX
        )
        input_ids_list.append(input_ids)
        attention_mask_list.append([1] * len(input_ids))
        targets_list.append(labels)

    return dict(
        input_ids=input_ids_list,
        attention_mask=attention_mask_list,
        labels=targets_list,
    )


def filter_empty_labels(example):
    """
    Remove empty labels dataset.
    # ç¤ºä¾‹æ•°æ®
        dataset = [
            {"text": "Hello", "labels": [1, 2, 3]},        # æœ‰æ•ˆæ ·æœ¬
            {"text": "World", "labels": [-100, -100, -100]}, # æ— æ•ˆæ ·æœ¬ï¼ˆå…¨paddingï¼‰
            {"text": "Test", "labels": [5, -100, 7]},       # æœ‰æ•ˆæ ·æœ¬ï¼ˆéƒ¨åˆ†æœ‰æ•ˆï¼‰
        ]
    filtered_dataset = filter(filter_empty_labels, dataset)
    # ç»“æœï¼š{"text": "Hello", "labels": [1, 2, 3]} å’Œ {"text": "Test", "labels": [5, -100, 7]}
    """
    return not all(label == -100 for label in example["labels"])


def setup_tokenizer(model_args, script_args):
    """
    å‚æ•°è®¾ç½®ï¼šæ„å»ºtokenizeråˆå§‹åŒ–æ‰€éœ€çš„å‚æ•°å­—å…¸
    è·¯å¾„ç¡®å®šï¼šæ™ºèƒ½é€‰æ‹©tokenizerè·¯å¾„ï¼Œä¼˜å…ˆä½¿ç”¨æŒ‡å®šè·¯å¾„ï¼Œå¦åˆ™ä½¿ç”¨æ¨¡å‹è·¯å¾„
    åŠ è½½tokenizerï¼šä»é¢„è®­ç»ƒæ¨¡å‹åŠ è½½tokenizer
    è·å–å¯¹è¯æ¨¡æ¿ï¼šç”¨äºæ ¼å¼åŒ–è¾“å…¥æ–‡æœ¬
    ç‰¹æ®Štokenè®¾ç½®ï¼š
    EOSï¼ˆç»“æŸç¬¦ï¼‰ï¼šå¦‚æœä¸å­˜åœ¨åˆ™ä½¿ç”¨æ¨¡æ¿çš„åœæ­¢å­—ç¬¦ä¸²
    BOSï¼ˆå¼€å§‹ç¬¦ï¼‰ï¼šå¦‚æœä¸å­˜åœ¨åˆ™ä½¿ç”¨ç»“æŸç¬¦
    PADï¼ˆå¡«å……ç¬¦ï¼‰ï¼šä¼˜å…ˆä½¿ç”¨æœªçŸ¥tokenï¼Œå¦åˆ™ä½¿ç”¨ç»“æŸç¬¦
    è°ƒè¯•è¾“å‡ºï¼šè®°å½•tokenizerä¿¡æ¯å¹¶è¿”å›
    """
    # æ„å»ºtokenizerçš„åˆå§‹åŒ–å‚æ•°å­—å…¸
    tokenizer_kwargs = {
        "cache_dir": model_args.cache_dir,  # ç¼“å­˜ç›®å½•ï¼Œé¿å…é‡å¤ä¸‹è½½æ¨¡å‹
        "use_fast": model_args.use_fast_tokenizer,  # æ˜¯å¦ä½¿ç”¨å¿«é€Ÿtokenizer
        "trust_remote_code": model_args.trust_remote_code,  # æ˜¯å¦ä¿¡ä»»è¿œç¨‹ä»£ç 
    }

    # ç¡®å®štokenizerçš„è·¯å¾„ï¼šä¼˜å…ˆä½¿ç”¨æŒ‡å®šçš„tokenizerè·¯å¾„ï¼Œå¦åˆ™ä½¿ç”¨æ¨¡å‹è·¯å¾„
    tokenizer_name_or_path = model_args.tokenizer_name_or_path
    if not tokenizer_name_or_path:
        tokenizer_name_or_path = model_args.model_name_or_path

    # ä»é¢„è®­ç»ƒæ¨¡å‹åŠ è½½tokenizer
    tokenizer = AutoTokenizer.from_pretrained(
        tokenizer_name_or_path,
        **tokenizer_kwargs
    )

    # è·å–å¯¹è¯æ¨¡æ¿ï¼Œç”¨äºæ ¼å¼åŒ–è¾“å…¥æ–‡æœ¬
    prompt_template = get_conv_template(script_args.template_name)

    '''
    tokenizer.eos_token å’Œ tokenizer.eos_token_id æ˜¯ä¸¤ä¸ªç›¸å…³ä½†ä¸åŒçš„å±æ€§ï¼š

    1.tokenizer.eos_token
        ç±»å‹ï¼šå­—ç¬¦ä¸² (str)
        å†…å®¹ï¼šç»“æŸç¬¦çš„å®é™…æ–‡æœ¬ï¼Œä¾‹å¦‚ "<|endoftext|>" ã€ "</s>" æˆ– "\n"
        ç”¨é€”ï¼š
            åœ¨æ–‡æœ¬å¤„ç†å’Œå­—ç¬¦ä¸²æ“ä½œä¸­ä½¿ç”¨
            ç”¨äºç”Ÿæˆå¯¹è¯æ—¶çš„æ–‡æœ¬æ ¼å¼åŒ–
            åœ¨æ•°æ®é¢„å¤„ç†æ—¶æ‹¼æ¥æ–‡æœ¬
    2.tokenizer.eos_token_id
        ç±»å‹ï¼šæ•´æ•° (int)
        å†…å®¹ï¼šç»“æŸç¬¦åœ¨è¯æ±‡è¡¨ä¸­çš„ç´¢å¼•ç¼–å·ï¼Œä¾‹å¦‚ 2 ã€ 151643 ç­‰
        ç”¨é€”ï¼š
            åœ¨æ¨¡å‹è¾“å…¥çš„tokenåºåˆ—ä¸­ä½¿ç”¨
            ç”¨äºæ¨¡å‹çš„å¼ é‡è®¡ç®—å’Œæ¨ç†
            åœ¨è®­ç»ƒæ—¶æ ‡è¯†åºåˆ—ç»“æŸä½ç½®
    '''
    # æ£€æŸ¥å¹¶è®¾ç½®ç»“æŸç¬¦ï¼ˆeos_tokenï¼‰
    if tokenizer.eos_token_id is None:
        # å¦‚æœæ²¡æœ‰ç»“æŸç¬¦ï¼Œä½¿ç”¨æ¨¡æ¿çš„åœæ­¢å­—ç¬¦ä¸²ä½œä¸ºç»“æŸç¬¦
        tokenizer.eos_token = prompt_template.stop_str
        tokenizer.add_special_tokens({"eos_token": tokenizer.eos_token})
        logger.info(
            f"Add eos_token: {tokenizer.eos_token}, "
            f"eos_token_id: {tokenizer.eos_token_id}"
        )

    # æ£€æŸ¥å¹¶è®¾ç½®å¼€å§‹ç¬¦ï¼ˆbos_tokenï¼‰
    if tokenizer.bos_token_id is None:
        # å¦‚æœæ²¡æœ‰å¼€å§‹ç¬¦ï¼Œä½¿ç”¨ç»“æŸç¬¦ä½œä¸ºå¼€å§‹ç¬¦
        tokenizer.add_special_tokens({"bos_token": tokenizer.eos_token})
        tokenizer.bos_token_id = tokenizer.eos_token_id
        logger.info(
            f"Add bos_token: {tokenizer.bos_token}, "
            f"bos_token_id: {tokenizer.bos_token_id}"
        )

    # æ£€æŸ¥å¹¶è®¾ç½®å¡«å……ç¬¦ï¼ˆpad_tokenï¼‰
    if tokenizer.pad_token_id is None:
        if tokenizer.unk_token_id is not None:
            # ä¼˜å…ˆä½¿ç”¨æœªçŸ¥tokenä½œä¸ºå¡«å……ç¬¦
            tokenizer.pad_token = tokenizer.unk_token
        else:
            # å¦åˆ™ä½¿ç”¨ç»“æŸç¬¦ä½œä¸ºå¡«å……ç¬¦
            tokenizer.pad_token = tokenizer.eos_token
        logger.info(
            f"Add pad_token: {tokenizer.pad_token}, "
            f"pad_token_id: {tokenizer.pad_token_id}"
        )

    # è¾“å‡ºè°ƒè¯•ä¿¡æ¯
    logger.debug(f"Tokenizer: {tokenizer}")
    return tokenizer, prompt_template

def load_hf_datasets(data_args, model_args):
    """
    ä» HuggingFace æ•°æ®é›†åŠ è½½æ•°æ®ï¼Œå¹¶å¤„ç†éªŒè¯é›†
    """
    from datasets import load_dataset

    raw_datasets = {}
    if not data_args.dataset_name:
        return raw_datasets, False

    has_data_source = False

    dataset_names = [name.strip() for name in data_args.dataset_name.split(',') if name.strip()]
    dataset_configs = []
    if data_args.dataset_config_name:
        dataset_configs = [
            None if (c := config.strip()) in ("", "None", "none") else c
            for config in data_args.dataset_config_name.split(',')
        ]

    # è¡¥å…¨é…ç½®åˆ—è¡¨é•¿åº¦
    while len(dataset_configs) < len(dataset_names):
        dataset_configs.append(None)
    dataset_configs = dataset_configs[:len(dataset_names)]

    for i, dataset_name in enumerate(dataset_names):
        dataset_config = dataset_configs[i]
        try:
            logger.info(f"åŠ è½½ HuggingFace æ•°æ®é›† '{dataset_name}' (é…ç½®: {dataset_config})")
            named_datasets = load_dataset(dataset_name, dataset_config, cache_dir=model_args.cache_dir)
            if not named_datasets:
                logger.warning(f"æ•°æ®é›† '{dataset_name}' åŠ è½½æˆåŠŸä½†ä¸ºç©º")
                continue
            has_data_source = True

            # å¤„ç†éªŒè¯é›†
            if "validation" not in named_datasets:
                if "train" in named_datasets and len(named_datasets["train"]) > 0:
                    shuffled_train = named_datasets["train"].shuffle(seed=42)
                    split = shuffled_train.train_test_split(
                        test_size=data_args.validation_split_percentage / 100, seed=42
                    )
                    named_datasets["train"] = split["train"]
                    named_datasets["validation"] = split["test"]
                else:
                    logger.warning(f"æ•°æ®é›† '{dataset_name}' æ²¡æœ‰è®­ç»ƒæ•°æ®ï¼Œæ— æ³•åˆ†å‰²éªŒè¯é›†")

            # åˆå¹¶æ•°æ®é›†
            for key in named_datasets:
                if key in named_datasets and len(named_datasets[key]) > 0:
                    if key in raw_datasets:
                        raw_datasets[key] = raw_datasets[key].concatenate(named_datasets[key])
                    else:
                        raw_datasets[key] = named_datasets[key]

        except Exception as e:
            logger.error(f"åŠ è½½ HuggingFace æ•°æ®é›† '{dataset_name}' å¤±è´¥: {str(e)}")
            logger.warning(f"è·³è¿‡æ•°æ®é›† '{dataset_name}'ï¼Œç»§ç»­åŠ è½½å…¶ä»–æ•°æ®é›†")

    return raw_datasets, has_data_source


def load_local_datasets(data_args, model_args):
    """
    ä»æœ¬åœ° JSON/JSONL æ–‡ä»¶åŠ è½½æ•°æ®ï¼Œå¹¶å¤„ç†éªŒè¯é›†
    """
    from datasets import load_dataset
    import os
    from glob import glob

    raw_datasets = {}
    has_data_source = False
    data_files = {}

    # è®­ç»ƒæ–‡ä»¶
    train_files = []
    if data_args.train_file_dir and os.path.exists(data_args.train_file_dir):
        train_files = glob(f'{data_args.train_file_dir}/**/*.json', recursive=True) + \
                      glob(f'{data_args.train_file_dir}/**/*.jsonl', recursive=True)
        if train_files:
            has_data_source = True
    elif data_args.train_file_dir:
        logger.warning(f"è®­ç»ƒæ–‡ä»¶ç›®å½•ä¸å­˜åœ¨: {data_args.train_file_dir}")

    # éªŒè¯æ–‡ä»¶
    val_files = []
    if data_args.validation_file_dir and os.path.exists(data_args.validation_file_dir):
        val_files = glob(f'{data_args.validation_file_dir}/**/*.json', recursive=True) + \
                    glob(f'{data_args.validation_file_dir}/**/*.jsonl', recursive=True)
        if val_files:
            has_data_source = True
    elif data_args.validation_file_dir:
        logger.warning(f"éªŒè¯æ–‡ä»¶ç›®å½•ä¸å­˜åœ¨: {data_args.validation_file_dir}")

    if train_files or val_files:
        try:
            if train_files:
                data_files["train"] = train_files
            if val_files:
                data_files["validation"] = val_files

            logger.info("åŠ è½½æœ¬åœ°æ–‡ä»¶æ•°æ®é›†...")
            file_datasets = load_dataset('json', data_files=data_files, cache_dir=model_args.cache_dir)
            if not file_datasets:
                logger.warning("æœ¬åœ°æ–‡ä»¶åŠ è½½æˆåŠŸä½†æ•°æ®é›†ä¸ºç©º")
            else:
                has_data_source = True

            # ä»…è®­ç»ƒæ–‡ä»¶æ—¶ï¼Œåˆ†å‰²éªŒè¯é›†
            if train_files and not val_files and "validation" not in file_datasets:
                if "train" in file_datasets and len(file_datasets["train"]) > 0:
                    shuffled_train = file_datasets["train"].shuffle(seed=42)
                    split = shuffled_train.train_test_split(
                        test_size=float(data_args.validation_split_percentage / 100), seed=42
                    )
                    file_datasets["train"] = split["train"]
                    file_datasets["validation"] = split["test"]
                else:
                    logger.warning("è®­ç»ƒæ–‡ä»¶ä¸ºç©ºï¼Œæ— æ³•åˆ†å‰²éªŒè¯é›†")

            # åˆå¹¶æ•°æ®é›†
            for key in file_datasets:
                if key in file_datasets and len(file_datasets[key]) > 0:
                    if key in raw_datasets:
                        raw_datasets[key] = raw_datasets[key].concatenate(file_datasets[key])
                    else:
                        raw_datasets[key] = file_datasets[key]

        except Exception as e:
            logger.error(f"åŠ è½½æœ¬åœ°æ–‡ä»¶æ•°æ®é›†å¤±è´¥: {str(e)}")

    return raw_datasets, has_data_source


def load_datasets(data_args, model_args):
    """
    åŠ è½½æ•°æ®é›†ï¼ˆHuggingFace + æœ¬åœ°æ–‡ä»¶ï¼‰ï¼Œå¹¶åˆå¹¶
    """
    raw_datasets = {}
    overall_has_data = False

    # HuggingFace æ•°æ®é›†
    hf_datasets, hf_has_data = load_hf_datasets(data_args, model_args)
    if hf_has_data:
        overall_has_data = True
        for k, v in hf_datasets.items():
            raw_datasets[k] = v

    # æœ¬åœ°æ–‡ä»¶
    local_datasets, local_has_data = load_local_datasets(data_args, model_args)
    if local_has_data:
        overall_has_data = True
        for k, v in local_datasets.items():
            if k in raw_datasets:
                raw_datasets[k] = raw_datasets[k].concatenate(v)
            else:
                raw_datasets[k] = v

    # éªŒè¯æœ€ç»ˆæ•°æ®é›†
    if not overall_has_data or "train" not in raw_datasets or len(raw_datasets["train"]) == 0:
        raise ValueError("æœªèƒ½åŠ è½½æœ‰æ•ˆè®­ç»ƒæ•°æ®é›†ï¼Œè¯·æ£€æŸ¥é…ç½®æˆ–æ•°æ®æº")

    # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
    logger.info("=" * 50)
    logger.info("æ•°æ®é›†åŠ è½½å®Œæˆï¼Œç»Ÿè®¡ä¿¡æ¯:")
    for key, dataset in raw_datasets.items():
        logger.info(f"  {key}: {len(dataset)} æ¡æ•°æ®")
    logger.info("=" * 50)

    return raw_datasets



def process_train_dataset(train_dataset, data_args, training_args, is_main_process, tokenizer, script_args,
                          IGNORE_INDEX, prompt_template):
    """å¤„ç†è®­ç»ƒæ•°æ®é›†"""
    # è·å–è®­ç»ƒæ•°æ®é›†çš„æ€»æ ·æœ¬æ•°
    max_train_samples = len(train_dataset)

    # å¦‚æœç”¨æˆ·è®¾ç½®äº†æœ€å¤§è®­ç»ƒæ ·æœ¬æ•°é™åˆ¶ï¼Œåˆ™æˆªå–æ•°æ®é›†
    if data_args.max_train_samples is not None and data_args.max_train_samples > 0:
        # å–æ•°æ®é›†é•¿åº¦å’Œè®¾ç½®çš„æœ€å¤§å€¼ä¸­çš„è¾ƒå°å€¼
        max_train_samples = min(len(train_dataset), data_args.max_train_samples)
        # åªé€‰æ‹©å‰max_train_samplesä¸ªæ ·æœ¬
        train_dataset = train_dataset.select(range(max_train_samples))

    # å¦‚æœæ˜¯ä¸»è¿›ç¨‹ï¼Œæ‰“å°ç¬¬ä¸€ä¸ªè®­ç»ƒæ ·æœ¬çš„ç¤ºä¾‹ï¼ˆç”¨äºè°ƒè¯•ï¼‰
    if is_main_process:
        logger.debug(f"Example train_dataset[0]: {train_dataset[0]}")

    # ä½¿ç”¨ä¸»è¿›ç¨‹ä¼˜å…ˆçš„ä¸Šä¸‹æ–‡ç®¡ç†å™¨è¿›è¡Œæ•°æ®é›†tokenization
    # è¿™ç¡®ä¿åœ¨åˆ†å¸ƒå¼è®­ç»ƒä¸­ï¼Œåªæœ‰ä¸»è¿›ç¨‹æ‰§è¡Œé¢„å¤„ç†ï¼Œå…¶ä»–è¿›ç¨‹ç­‰å¾…
    with training_args.main_process_first(desc="Train dataset tokenization"):
        # å¯¹è®­ç»ƒæ•°æ®é›†è¿›è¡Œæ˜ å°„å¤„ç†ï¼Œå°†æ–‡æœ¬è½¬æ¢ä¸ºtoken IDs
        tokenized_dataset = train_dataset.map(
            # lambdaå‡½æ•°ï¼šå¯¹æ¯ä¸ªæ ·æœ¬åº”ç”¨é¢„å¤„ç†å‡½æ•°
            lambda examples: preprocess_function(examples, tokenizer, script_args.model_max_length, script_args,
                                                 IGNORE_INDEX, prompt_template),
            batched=True,  # æ‰¹é‡å¤„ç†ä»¥æé«˜æ•ˆç‡
            num_proc=1,  # ä½¿ç”¨1ä¸ªè¿›ç¨‹è¿›è¡Œå¤„ç†
            remove_columns=train_dataset.column_names,  # ç§»é™¤åŸå§‹åˆ—ï¼Œåªä¿ç•™tokenizationåçš„ç»“æœ
            load_from_cache_file=not data_args.overwrite_cache,  # æ ¹æ®é…ç½®å†³å®šæ˜¯å¦åŠ è½½ç¼“å­˜
            desc="Running tokenizer on dataset" if is_main_process else None,  # è¿›åº¦æè¿°ï¼ˆä»…ä¸»è¿›ç¨‹æ˜¾ç¤ºï¼‰
        )

        # è¿‡æ»¤æ‰æ ‡ç­¾ä¸ºç©ºçš„æ ·æœ¬
        train_dataset = tokenized_dataset.filter(filter_empty_labels, num_proc=1)

        # å¦‚æœæ˜¯ä¸»è¿›ç¨‹ï¼Œè¾“å‡ºè°ƒè¯•ä¿¡æ¯
        if is_main_process:
            # æ‰“å°æœ€ç»ˆçš„è®­ç»ƒæ ·æœ¬æ•°é‡
            logger.debug(f"Num train_samples: {len(train_dataset)}")
            logger.debug("Tokenized training example:")

            # è§£ç å¹¶æ‰“å°ç¬¬ä¸€ä¸ªæ ·æœ¬çš„input_idsï¼ˆè¾“å…¥åºåˆ—ï¼‰
            logger.debug(f"Decode input_ids[0]:\n{tokenizer.decode(train_dataset[0]['input_ids'])}")

            # å¤„ç†labelsåºåˆ—ï¼šå°†IGNORE_INDEXæ›¿æ¢ä¸ºpad_token_idä»¥ä¾¿è§£ç 
            replaced_labels = [label if label != IGNORE_INDEX else tokenizer.pad_token_id
                               for label in list(train_dataset[0]['labels'])]
            # è§£ç å¹¶æ‰“å°ç¬¬ä¸€ä¸ªæ ·æœ¬çš„labelsï¼ˆæ ‡ç­¾åºåˆ—ï¼‰
            logger.debug(f"Decode labels[0]:\n{tokenizer.decode(replaced_labels)}")

    # è¿”å›å¤„ç†åçš„è®­ç»ƒæ•°æ®é›†å’Œå®é™…ä½¿ç”¨çš„æ ·æœ¬æ•°é‡
    return train_dataset, max_train_samples


def process_eval_dataset(eval_dataset, data_args, training_args, tokenizer, script_args, IGNORE_INDEX, prompt_template):
    """å¤„ç†è¯„ä¼°æ•°æ®é›†"""
    # è·å–åŸå§‹è¯„ä¼°æ•°æ®é›†çš„æ€»æ ·æœ¬æ•°
    max_eval_samples = len(eval_dataset)

    # å¦‚æœè®¾ç½®äº†æœ€å¤§è¯„ä¼°æ ·æœ¬æ•°é™åˆ¶ï¼Œåˆ™æˆªå–æ•°æ®é›†
    if data_args.max_eval_samples is not None and data_args.max_eval_samples > 0:
        max_eval_samples = min(len(eval_dataset), data_args.max_eval_samples)
        eval_dataset = eval_dataset.select(range(max_eval_samples))

    # è·å–å¤„ç†åçš„è¯„ä¼°æ•°æ®é›†å¤§å°
    eval_size = len(eval_dataset)

    # è®°å½•è¯„ä¼°æ ·æœ¬æ•°é‡ä¿¡æ¯
    logger.debug(f"Num eval_samples: {eval_size}")

    # å¦‚æœè¯„ä¼°æ ·æœ¬è¿‡å¤šï¼ˆè¶…è¿‡500ä¸ªï¼‰ï¼Œå‘å‡ºè­¦å‘Šæç¤ºç”¨æˆ·è€ƒè™‘å‡å°‘æ ·æœ¬æ•°ä»¥æé«˜è®­ç»ƒé€Ÿåº¦
    if eval_size > 500:
        logger.warning(f"Num eval_samples is large: {eval_size}, "
                       f"training slow, consider reduce it by `--max_eval_samples=50`")

    # è¾“å‡ºç¬¬ä¸€ä¸ªè¯„ä¼°æ ·æœ¬çš„åŸå§‹æ•°æ®æ ¼å¼ï¼Œç”¨äºè°ƒè¯•
    logger.debug(f"Example eval_dataset[0]: {eval_dataset[0]}")

    # å¯¹è¯„ä¼°æ•°æ®é›†è¿›è¡Œé¢„å¤„ç†
    eval_dataset = eval_dataset.map(
        # é¢„å¤„ç†å‡½æ•°ï¼šå¯¹æ¯ä¸ªæ ·æœ¬è¿›è¡Œtokenizationï¼Œæ ¼å¼åŒ–ä¸ºæ¨¡å‹è¾“å…¥æ ¼å¼
        lambda examples: preprocess_function(examples, tokenizer, script_args.model_max_length, script_args,
                                             IGNORE_INDEX, prompt_template),
        batched=True,  # æ‰¹é‡å¤„ç†æé«˜æ•ˆç‡
        num_proc=data_args.preprocessing_num_workers,  # å¤šè¿›ç¨‹å¤„ç†
        remove_columns=eval_dataset.column_names,  # ç§»é™¤åŸå§‹åˆ—ï¼Œåªä¿ç•™å¤„ç†åçš„tokenizedæ•°æ®
        load_from_cache_file=not data_args.overwrite_cache,  # æ˜¯å¦ä»ç¼“å­˜åŠ è½½
        desc="Running tokenizer on validation dataset",  # è¿›åº¦æè¿°
    )

    # è¿‡æ»¤æ‰ç©ºæ ‡ç­¾çš„æ ·æœ¬ï¼ˆå¯èƒ½ç”±äºé¢„å¤„ç†å¯¼è‡´æ ‡ç­¾ä¸ºç©ºï¼‰
    eval_dataset = eval_dataset.filter(filter_empty_labels, num_proc=data_args.preprocessing_num_workers)

    # è®°å½•è¿‡æ»¤åçš„æœ€ç»ˆè¯„ä¼°æ ·æœ¬æ•°é‡
    logger.debug(f"Num eval_samples: {len(eval_dataset)}")

    # è¾“å‡ºç¬¬ä¸€ä¸ªtokenizedåçš„æ ·æœ¬è§£ç ç»“æœï¼Œç”¨äºéªŒè¯é¢„å¤„ç†æ˜¯å¦æ­£ç¡®
    logger.debug("Tokenized eval example:")
    logger.debug(tokenizer.decode(eval_dataset[0]['input_ids']))

    # è¿”å›å¤„ç†åçš„è¯„ä¼°æ•°æ®é›†å’Œæœ€å¤§æ ·æœ¬æ•°
    return eval_dataset, max_eval_samples


def setup_quantization_config(model_args, script_args, dtype, training_args):
    """è®¾ç½®é‡åŒ–é…ç½®

    é‡åŒ–æ˜¯ä¸€ç§æ¨¡å‹å‹ç¼©æŠ€æœ¯ï¼Œé€šè¿‡é™ä½æ¨¡å‹å‚æ•°çš„ç²¾åº¦æ¥å‡å°‘å†…å­˜å ç”¨å’Œè®¡ç®—å¼€é”€ã€‚
    æ”¯æŒ4ä½é‡åŒ–ï¼ˆQLoRAï¼‰å’Œ8ä½é‡åŒ–ä¸¤ç§æ¨¡å¼ã€‚

    Args:
        model_args: æ¨¡å‹å‚æ•°å¯¹è±¡ï¼ŒåŒ…å«é‡åŒ–ç›¸å…³çš„é…ç½®é€‰é¡¹
        script_args: è„šæœ¬å‚æ•°å¯¹è±¡ï¼ŒåŒ…å«QLoRAç­‰é«˜çº§ç‰¹æ€§å¼€å…³
        dtype: PyTorchæ•°æ®ç±»å‹ï¼ˆå¦‚torch.float16æˆ–torch.bfloat16ï¼‰
        training_args: è®­ç»ƒå‚æ•°å¯¹è±¡ï¼ˆå½“å‰æœªä½¿ç”¨ä½†ä¿ç•™æ¥å£ï¼‰

    Returns:
        tuple: (quantization_config, load_in_4bit, load_in_8bit)
            - quantization_config: BitsAndBytesConfigå¯¹è±¡æˆ–None
            - load_in_4bit: æ˜¯å¦å¯ç”¨4ä½é‡åŒ–çš„å¸ƒå°”å€¼
            - load_in_8bit: æ˜¯å¦å¯ç”¨8ä½é‡åŒ–çš„å¸ƒå°”å€¼

    Raises:
        ValueError: å½“4ä½å’Œ8ä½é‡åŒ–åŒæ—¶å¯ç”¨ï¼Œæˆ–ä¸DeepSpeed ZeRO-3å†²çªæ—¶
    """
    # ä»æ¨¡å‹å‚æ•°ä¸­æå–é‡åŒ–é…ç½®é€‰é¡¹
    load_in_4bit = model_args.load_in_4bit  # 4ä½é‡åŒ–æ ‡å¿—
    load_in_8bit = model_args.load_in_8bit  # 8ä½é‡åŒ–æ ‡å¿—
    quantization_config = None  # åˆå§‹åŒ–é‡åŒ–é…ç½®ä¸ºNone

    # æ£€æŸ¥äº’æ–¥æ€§ï¼š4ä½å’Œ8ä½é‡åŒ–ä¸èƒ½åŒæ—¶å¯ç”¨
    if load_in_4bit and load_in_8bit:
        raise ValueError("Error, load_in_4bit and load_in_8bit cannot be set at the same time")
    # å¦‚æœå¯ç”¨äº†ä»»æ„ä¸€ç§é‡åŒ–æ¨¡å¼
    elif load_in_8bit or load_in_4bit:
        # è®°å½•é‡åŒ–é…ç½®ä¿¡æ¯
        logger.info(f"Quantizing model, load_in_4bit: {load_in_4bit}, load_in_8bit: {load_in_8bit}")

        # æ£€æŸ¥ä¸DeepSpeed ZeRO-3çš„å…¼å®¹æ€§
        if is_deepspeed_zero3_enabled():
            raise ValueError("DeepSpeed ZeRO-3 is incompatible with quantization.")

        # 8ä½é‡åŒ–é…ç½®
        if load_in_8bit:
            quantization_config = BitsAndBytesConfig(load_in_8bit=True)
        # 4ä½é‡åŒ–é…ç½®
        elif load_in_4bit:
            if script_args.qlora:
                # QLoRAæ¨¡å¼çš„4ä½é‡åŒ–ï¼šå¯ç”¨æ‰€æœ‰ä¼˜åŒ–ç‰¹æ€§
                quantization_config = BitsAndBytesConfig(
                    load_in_4bit=True,  # å¯ç”¨4ä½é‡åŒ–
                    bnb_4bit_compute_dtype=dtype,  # è®¡ç®—æ—¶ä½¿ç”¨çš„æ•°æ®ç±»å‹
                    bnb_4bit_use_double_quant=True,  # å¯ç”¨åŒé‡é‡åŒ–ï¼ˆè¿›ä¸€æ­¥å‹ç¼©ï¼‰
                    bnb_4bit_quant_type="nf4"  # ä½¿ç”¨NF4é‡åŒ–ç±»å‹ï¼ˆ4ä½å½’ä¸€åŒ–æµ®ç‚¹ï¼‰
                )
            else:
                # æ ‡å‡†4ä½é‡åŒ–æ¨¡å¼
                quantization_config = BitsAndBytesConfig(
                    load_in_4bit=True,  # å¯ç”¨4ä½é‡åŒ–
                    bnb_4bit_compute_dtype=dtype,  # è®¡ç®—æ—¶ä½¿ç”¨çš„æ•°æ®ç±»å‹
                )

    return quantization_config, load_in_4bit, load_in_8bit


def setup_model_kwargs(model_args, config, config_kwargs, dtype, quantization_config, training_args=None):
    """è®¾ç½®æ¨¡å‹åŠ è½½å‚æ•°

    Args:
        model_args: æ¨¡å‹å‚æ•°å¯¹è±¡ï¼ŒåŒ…å«è®¾å¤‡æ˜ å°„ã€ä¿¡ä»»è¿œç¨‹ä»£ç ç­‰é…ç½®
        config: æ¨¡å‹é…ç½®å¯¹è±¡
        config_kwargs: é…ç½®å…³é”®å­—å‚æ•°
        dtype: PyTorchæ•°æ®ç±»å‹ï¼ˆå¦‚torch.float16, torch.bfloat16ç­‰ï¼‰
        quantization_config: é‡åŒ–é…ç½®å¯¹è±¡

    Returns:
        dict: åŒ…å«æ‰€æœ‰æ¨¡å‹åŠ è½½å‚æ•°çš„å­—å…¸
    """
    # è·å–å¯ç”¨GPUæ•°é‡
    num_gpus = torch.cuda.device_count()

    # åŸºç¡€æ¨¡å‹å‚æ•°é…ç½®ï¼ˆä¸åŒ…å«device_mapï¼Œç¨åè®¾ç½®ï¼‰
    model_kwargs = {
        "config": config,  # æ¨¡å‹é…ç½®å¯¹è±¡
        "dtype": dtype,  # æŒ‡å®šæ¨¡å‹çš„æ•°æ®ç±»å‹ï¼Œå½±å“ç²¾åº¦å’Œå†…å­˜ä½¿ç”¨
        "trust_remote_code": model_args.trust_remote_code,  # æ˜¯å¦ä¿¡ä»»è¿œç¨‹ä»£ç ï¼ˆç”¨äºåŠ è½½è‡ªå®šä¹‰æ¨¡å‹ï¼‰
        "quantization_config": quantization_config,  # é‡åŒ–é…ç½®ï¼Œç”¨äºå‡å°‘å†…å­˜å ç”¨
        "low_cpu_mem_usage": True,  # å¯ç”¨ä½CPUå†…å­˜ä½¿ç”¨æ¨¡å¼
    }

    # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨DeepSpeed ZeRO-3
    using_deepspeed_zero3 = False
    if training_args and training_args.deepspeed is not None:
        # å¯¼å…¥å¹¶æ£€æŸ¥DeepSpeedé…ç½®
        try:
            from transformers.integrations import is_deepspeed_zero3_enabled
            using_deepspeed_zero3 = is_deepspeed_zero3_enabled()
        except ImportError:
            pass

    # è®¾ç½®è®¾å¤‡æ˜ å°„ç­–ç•¥
    if using_deepspeed_zero3:
        # DeepSpeed ZeRO-3ä¸æ”¯æŒdevice_mapï¼Œç”±DeepSpeedè‡ªåŠ¨ç®¡ç†
        logger.info("ğŸ”§ æ£€æµ‹åˆ°DeepSpeed ZeRO-3ï¼Œå°†è®©DeepSpeedè‡ªåŠ¨ç®¡ç†è®¾å¤‡æ˜ å°„")
        model_kwargs["device_map"] = None
    elif model_args.device_map == 'auto':
        if num_gpus > 1:
            # ä¿æŒè‡ªåŠ¨è®¾å¤‡æ˜ å°„
            model_kwargs["device_map"] = "auto"

            # ä¸ºæ¯ä¸ªGPUè®¾ç½®æœ€å¤§å†…å­˜é™åˆ¶
            max_memory = {}
            for i in range(num_gpus):
                # è·å–GPUå±æ€§ä¿¡æ¯
                gpu_props = torch.cuda.get_device_properties(i)
                total_mem = gpu_props.total_memory  # GPUæ€»å†…å­˜

                # è®¾ç½®å¯ç”¨å†…å­˜ä¸ºæ€»å†…å­˜çš„80%ï¼Œé¢„ç•™20%ä½œä¸ºç¼“å†²
                usable_mem = int(total_mem * 0.8)

                # å°†å†…å­˜å¤§å°è½¬æ¢ä¸ºGiBå•ä½å¹¶æ·»åŠ åˆ°å­—å…¸ä¸­
                max_memory[i] = f"{usable_mem // (1024 ** 3)}GiB"

            # å°†æœ€å¤§å†…å­˜é…ç½®æ·»åŠ åˆ°æ¨¡å‹å‚æ•°ä¸­
            model_kwargs["max_memory"] = max_memory
        else:
            # å•GPUæƒ…å†µï¼Œä¸è®¾ç½®device_mapè®©å…¶è‡ªåŠ¨ä½¿ç”¨GPU:0
            model_kwargs["device_map"] = None
    else:
        # ä½¿ç”¨ç”¨æˆ·æŒ‡å®šçš„device_map
        model_kwargs["device_map"] = model_args.device_map

    return model_kwargs


def log_model_info(model):
    """è®°å½•æ¨¡å‹ä¿¡æ¯"""
    logger.info("ğŸ“Š æ¨¡å‹åˆ†å¸ƒæƒ…å†µ:")

    # æ£€æŸ¥æ¨¡å‹æ˜¯å¦ä½¿ç”¨HuggingFaceçš„è®¾å¤‡æ˜ å°„ï¼ˆé€šå¸¸ç”¨äºå¤šGPUåˆ†å¸ƒå¼è®­ç»ƒï¼‰
    if hasattr(model, 'hf_device_map') and model.hf_device_map:
        logger.info("ğŸ”§ ä½¿ç”¨HuggingFaceè®¾å¤‡æ˜ å°„:")

        # éå†å¹¶è®°å½•æ¯ä¸ªæ¨¡å‹æ¨¡å—æ‰€åœ¨çš„è®¾å¤‡
        for module_name, device in model.hf_device_map.items():
            logger.info(f"  {module_name}: {device}")

        # ç»Ÿè®¡æ¯ä¸ªè®¾å¤‡ä¸Šåˆ†é…çš„æ¨¡å—æ•°é‡
        device_count = {}
        for device in model.hf_device_map.values():
            device_str = str(device)
            device_count[device_str] = device_count.get(device_str, 0) + 1

        logger.info("ğŸ“ˆ è®¾å¤‡ä½¿ç”¨ç»Ÿè®¡:")
        for device, count in device_count.items():
            logger.info(f"  {device}: {count} ä¸ªæ¨¡å—")
    else:
        # å¦‚æœæ²¡æœ‰è®¾å¤‡æ˜ å°„ï¼Œåˆ™æ‰‹åŠ¨ç»Ÿè®¡å‚æ•°åœ¨å„è®¾å¤‡ä¸Šçš„åˆ†å¸ƒ
        device_params = {}
        total_params = 0

        # éå†æ¨¡å‹çš„æ‰€æœ‰å‘½åå‚æ•°
        for name, param in model.named_parameters():
            device = str(param.device)
            if device not in device_params:
                device_params[device] = {'count': 0, 'size': 0}
            device_params[device]['count'] += 1  # å‚æ•°ç»„æ•°é‡
            device_params[device]['size'] += param.numel()  # å‚æ•°æ€»æ•°
            total_params += param.numel()

        logger.info("ğŸ“ˆ å‚æ•°è®¾å¤‡åˆ†å¸ƒ:")
        if total_params > 0:
            for device, info in device_params.items():
                # è®¡ç®—å‚æ•°å¤§å°ï¼ˆå‡è®¾float32ï¼Œæ¯ä¸ªå‚æ•°4å­—èŠ‚ï¼‰
                param_size_gb = info['size'] * 4 / 1024 ** 3
                percentage = info['size'] / total_params * 100
                logger.info(f"  {device}: {info['count']} ä¸ªå‚æ•°ç»„, {param_size_gb:.2f}GB ({percentage:.1f}%)")
        else:
            logger.info("  æœªæ£€æµ‹åˆ°æ¨¡å‹å‚æ•°ï¼ˆå¯èƒ½ä½¿ç”¨äº†DeepSpeed ZeROç­‰ä¼˜åŒ–æŠ€æœ¯ï¼‰")

    # å¦‚æœCUDAå¯ç”¨ï¼Œæ˜¾ç¤ºGPUå†…å­˜ä½¿ç”¨æƒ…å†µ
    if torch.cuda.is_available():
        logger.info("ğŸ’¾ GPUå†…å­˜ä½¿ç”¨æƒ…å†µ:")
        for i in range(torch.cuda.device_count()):
            allocated = torch.cuda.memory_allocated(i) / 1024 ** 3  # å·²åˆ†é…å†…å­˜
            cached = torch.cuda.memory_reserved(i) / 1024 ** 3  # ç¼“å­˜å†…å­˜
            total = torch.cuda.get_device_properties(i).total_memory / 1024 ** 3  # GPUæ€»å†…å­˜
            logger.info(f"  GPU {i}: å·²åˆ†é…={allocated:.1f}GB, ç¼“å­˜={cached:.1f}GB, æ€»è®¡={total:.1f}GB")


def setup_neftune(model, model_args):
    """
    è®¾ç½®NEFTune (Noisy Embedding Instruction Fine-Tuning)

    NEFTuneçš„æ•°å­¦åŸç†ï¼š
    ==================

    1. æ ¸å¿ƒæ€æƒ³ï¼š
    NEFTuneé€šè¿‡åœ¨è¾“å…¥åµŒå…¥å‘é‡ä¸­æ·»åŠ å™ªå£°æ¥å¢å¼ºæ¨¡å‹å¯¹æŒ‡ä»¤çš„é²æ£’æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚
    è¿™ç§æ–¹æ³•åŸºäºä»¥ä¸‹å‡è®¾ï¼šåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å¼•å…¥é€‚å½“çš„å™ªå£°å¯ä»¥é˜²æ­¢æ¨¡å‹è¿‡æ‹Ÿåˆï¼Œ
    å¹¶æé«˜å…¶å¯¹è¾“å…¥æ‰°åŠ¨çš„å®¹å¿åº¦ã€‚

    2. æ•°å­¦å…¬å¼ï¼š
    ç»™å®šåŸå§‹åµŒå…¥å‘é‡ E âˆˆ R^(dÃ—V)ï¼ˆå…¶ä¸­dæ˜¯åµŒå…¥ç»´åº¦ï¼ŒVæ˜¯è¯æ±‡è¡¨å¤§å°ï¼‰ï¼Œ
    å¯¹äºè¾“å…¥tokenç´¢å¼•iï¼Œæ·»åŠ å™ªå£°åçš„åµŒå…¥å‘é‡ï¼š

    áº¼_i = E_i + Îµ_i

    å…¶ä¸­å™ªå£° Îµ_i ~ Uniform(-Î±/âˆš(dÃ—V), Î±/âˆš(dÃ—V))

    Î± æ˜¯æ§åˆ¶å™ªå£°å¼ºåº¦çš„è¶…å‚æ•°ï¼ˆneft_alphaï¼‰
    âˆš(dÃ—V) æ˜¯å½’ä¸€åŒ–å› å­ï¼Œç¡®ä¿å™ªå£°å¹…åº¦ä¸åµŒå…¥å±‚å¤§å°æˆåæ¯”

    3. ç†è®ºä¾æ®ï¼š
    - å™ªå£°æ³¨å…¥ç­‰ä»·äºL2æ­£åˆ™åŒ–çš„ä¸€ç§å½¢å¼ï¼Œæœ‰åŠ©äºå¹³æ»‘æŸå¤±å‡½æ•°
    - ç±»ä¼¼äºæ•°æ®å¢å¼ºï¼Œåœ¨è¡¨ç¤ºç©ºé—´åˆ›å»ºæ›´å¤šæ ·åŒ–çš„è®­ç»ƒæ ·æœ¬
    - é¼“åŠ±æ¨¡å‹å­¦ä¹ å¯¹è¾“å…¥æ‰°åŠ¨ä¸æ•æ„Ÿçš„é²æ£’ç‰¹å¾è¡¨ç¤º

    4. å®éªŒå‘ç°ï¼š
    åŸè®ºæ–‡è¡¨æ˜ï¼Œåœ¨æŒ‡ä»¤å¾®è°ƒé˜¶æ®µæ·»åŠ å™ªå£°å¯ä»¥æ˜¾è‘—æå‡æ¨¡å‹åœ¨
    unseenæŒ‡ä»¤ä¸Šçš„è¡¨ç°ï¼Œç‰¹åˆ«æ˜¯åœ¨å°æ ·æœ¬å­¦ä¹ åœºæ™¯ä¸­ã€‚

    å‚æ•°è¯´æ˜ï¼š
    - model: è¦è¿›è¡ŒNEFTuneçš„æ¨¡å‹
    - model_args: åŒ…å«neft_alphaå‚æ•°çš„æ¨¡å‹å‚æ•°å¯¹è±¡

    Returns:
        None (ç›´æ¥ä¿®æ”¹æ¨¡å‹çš„å‰å‘ä¼ æ’­æ–¹æ³•)
    """
    # æ£€æŸ¥æ˜¯å¦å¯ç”¨äº†NEFTuneï¼ˆneft_alpha > 0ï¼‰
    if model_args.neft_alpha > 0:
        # è·å–æ¨¡å‹çš„è¾“å…¥åµŒå…¥å±‚
        input_embed = model.get_input_embeddings()

        # ç¡®ä¿è¾“å…¥åµŒå…¥å±‚æ˜¯æ ‡å‡†çš„nn.Embeddingç±»å‹
        if isinstance(input_embed, torch.nn.Embedding):
            def noisy_forward(self: torch.nn.Embedding, x: torch.Tensor) -> torch.Tensor:
                """
                å¸¦å™ªå£°çš„åµŒå…¥å‰å‘ä¼ æ’­å‡½æ•°

                Args:
                    self: åµŒå…¥å±‚å®ä¾‹
                    x: è¾“å…¥tokenç´¢å¼•å¼ é‡ [batch_size, seq_len]

                Returns:
                    æ·»åŠ äº†å‡åŒ€åˆ†å¸ƒå™ªå£°çš„åµŒå…¥å‘é‡ [batch_size, seq_len, embedding_dim]
                """
                # é¦–å…ˆè®¡ç®—åŸå§‹åµŒå…¥å‘é‡
                embeddings = input_embed.__class__.forward(self, x)

                # è®¡ç®—åµŒå…¥å±‚çš„æ€»ç»´åº¦æ•°ï¼ˆè¯æ±‡è¡¨å¤§å° Ã— åµŒå…¥ç»´åº¦ï¼‰
                # è¿™ä¸ªå€¼ç”¨ä½œå™ªå£°å¹…åº¦çš„å½’ä¸€åŒ–å› å­
                dims = self.num_embeddings * self.embedding_dim

                # è®¡ç®—å™ªå£°å¹…åº¦ï¼šÎ± / âˆš(è¯æ±‡è¡¨å¤§å° Ã— åµŒå…¥ç»´åº¦)
                # è¿™æ ·å¯ä»¥ç¡®ä¿å™ªå£°å¹…åº¦ä¸åµŒå…¥å±‚è§„æ¨¡æˆåæ¯”ï¼Œé¿å…åœ¨å¤§æ¨¡å‹ä¸­å™ªå£°è¿‡å¤§
                mag_norm = model_args.neft_alpha / (dims ** 0.5)

                # ç”Ÿæˆä¸åµŒå…¥å‘é‡åŒå½¢çŠ¶çš„å‡åŒ€åˆ†å¸ƒå™ªå£°å¹¶æ·»åŠ åˆ°åŸå§‹åµŒå…¥ä¸­
                # å™ªå£°èŒƒå›´ï¼š[-mag_norm, mag_norm]
                embeddings += torch.zeros_like(embeddings).uniform_(-mag_norm, mag_norm)

                return embeddings

            # ä½¿ç”¨MethodTypeå°†å™ªå£°å‰å‘ä¼ æ’­å‡½æ•°ç»‘å®šåˆ°åµŒå…¥å±‚å®ä¾‹
            # è¿™æ ·å°±æ›¿æ¢äº†åŸå§‹çš„forwardæ–¹æ³•ï¼Œåœ¨æ¯æ¬¡å‰å‘ä¼ æ’­æ—¶éƒ½ä¼šæ·»åŠ å™ªå£°
            input_embed.forward = MethodType(noisy_forward, input_embed)

            # è®°å½•NEFTuneå·²å¯ç”¨ï¼Œæ˜¾ç¤ºå™ªå£°å¼ºåº¦å‚æ•°
            logger.info("Using noisy embedding with alpha={:.2f}".format(model_args.neft_alpha))
        else:
            # å¦‚æœåµŒå…¥å±‚ä¸æ˜¯æ ‡å‡†nn.Embeddingï¼Œå‘å‡ºè­¦å‘Š
            # æŸäº›æ¨¡å‹å¯èƒ½ä½¿ç”¨è‡ªå®šä¹‰çš„åµŒå…¥å±‚å®ç°
            logger.warning("Input embeddings are not normal nn.Embedding, cannot transform into noisy embedding.")


def setup_model_patches(model, config, training_args):
    """è®¾ç½®æ¨¡å‹è¡¥ä¸ - å¤„ç†ä¸åŒæ¨¡å‹ç±»å‹çš„ç‰¹æ®Šé…ç½®å’Œè®­ç»ƒä¼˜åŒ–"""

    # 1. å¤„ç† ChatGLM å’Œ InternLM2 æ¨¡å‹çš„è¾“å‡ºå±‚æ˜ å°„
    if getattr(config, "model_type", None) == "chatglm" or getattr(config, "model_type", None) == "internlm2":
        # å°† lm_head æ˜ å°„åˆ° transformer.output_layer
        setattr(model, "lm_head", model.transformer.output_layer)
        # ä¿å­˜æ—¶å¿½ç•¥ lm_head.weightï¼Œé¿å…å†—ä½™
        setattr(model, "_keys_to_ignore_on_save", ["lm_head.weight"])

    # 2. å¤„ç† Mixtral æ¨¡å‹çš„ DeepSpeed ZeRO-3 ä¼˜åŒ–
    if getattr(config, "model_type", None) == "mixtral" and is_deepspeed_zero3_enabled():
        # æ£€æŸ¥ DeepSpeed ç‰ˆæœ¬è¦æ±‚
        require_version("deepspeed>=0.13.0", "To fix: pip install deepspeed>=0.13.0")
        from deepspeed.utils import set_z3_leaf_modules
        from transformers.models.mixtral.modeling_mixtral import MixtralSparseMoeBlock
        # è®¾ç½® MoE å—ä¸ºå¶å­æ¨¡å—ï¼Œä¼˜åŒ–å†…å­˜ä½¿ç”¨
        set_z3_leaf_modules(model, [MixtralSparseMoeBlock])

    # 3. å¤„ç† DeepSeek V3 æ¨¡å‹çš„ DeepSpeed ZeRO-3 ä¼˜åŒ–
    if getattr(config, "model_type", None) == "deepseek_v3" and is_deepspeed_zero3_enabled():
        require_version("deepspeed>=0.13.0", "To fix: pip install deepspeed>=0.13.0")
        # æ‰‹åŠ¨è®¾ç½®æ¯ä¸ª MoE å±‚ä¸ºå¶å­æ¨¡å—
        for layer in model.model.layers:
            if 'DeepseekV3MoE' in str(type(layer.mlp)):
                layer.mlp._z3_leaf = True

    # 4. é…ç½®æ¢¯åº¦æ£€æŸ¥ç‚¹ (Gradient Checkpointing)
    if training_args.gradient_checkpointing and getattr(model, "supports_gradient_checkpointing", False):
        # å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹ä»¥èŠ‚çœæ˜¾å­˜
        model.gradient_checkpointing_enable()
        # ç¦ç”¨ç¼“å­˜ä»¥é¿å…æ¢¯åº¦æ£€æŸ¥ç‚¹å†²çª
        model.config.use_cache = False
        logger.info("Gradient checkpointing enabled.")
    else:
        # ä¸å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹æ—¶ä¿æŒç¼“å­˜å¼€å¯
        model.config.use_cache = True
        logger.info("Gradient checkpointing disabled.")

    # 5. å¯ç”¨è¾“å…¥æ¢¯åº¦è®¡ç®—
    # ç¡®ä¿æ¨¡å‹å‚æ•°å¯ä»¥æ­£ç¡®è®¡ç®—æ¢¯åº¦
    model.enable_input_require_grads()


def setup_peft_model(model, script_args, training_args, load_in_8bit, load_in_4bit):
    """è®¾ç½®PEFTæ¨¡å‹"""
    # è®°å½•æ—¥å¿—ï¼šæ˜¾ç¤ºå½“å‰ä½¿ç”¨çš„æ˜¯LoRA(PEFT)å¾®è°ƒæ–¹æ³•
    logger.info("Fine-tuning method: LoRA(PEFT)")

    # è·å–æ¨¡å‹çš„è¾“å‡ºå±‚ï¼ˆé€šå¸¸æ˜¯lm_headï¼‰
    output_layer = getattr(model, "lm_head")
    # æ£€æŸ¥è¾“å‡ºå±‚æ˜¯å¦ä¸ºçº¿æ€§å±‚ä¸”æƒé‡æ•°æ®ç±»å‹ä¸æ˜¯float32
    if isinstance(output_layer, torch.nn.Linear) and output_layer.weight.dtype != torch.float32:
        # å®šä¹‰ä¸€ä¸ªåå‘é’©å­å‡½æ•°ï¼Œå°†è¾“å‡ºè½¬æ¢ä¸ºfloat32ç²¾åº¦
        def fp32_forward_post_hook(module: torch.nn.Module, args: Tuple[torch.Tensor], output: torch.Tensor):
            return output.to(torch.float32)

        # ä¸ºè¾“å‡ºå±‚æ³¨å†Œå‰å‘ä¼ æ’­é’©å­ï¼Œç¡®ä¿è¾“å‡ºä¸ºfloat32ç²¾åº¦
        output_layer.register_forward_hook(fp32_forward_post_hook)

    # æ£€æŸ¥æ˜¯å¦æŒ‡å®šäº†é¢„è®­ç»ƒçš„PEFTæ¨¡å‹è·¯å¾„
    if script_args.peft_path is not None:
        # ä»é¢„è®­ç»ƒçš„PEFTæ¨¡å‹åŠ è½½æƒé‡
        logger.info(f"Peft from pre-trained model: {script_args.peft_path}")
        # åˆ›å»ºPeftModelå®ä¾‹ï¼Œè®¾ç½®ä¸ºå¯è®­ç»ƒæ¨¡å¼
        model = PeftModel.from_pretrained(model, script_args.peft_path, is_trainable=True)
    else:
        # æ²¡æœ‰é¢„è®­ç»ƒPEFTæ¨¡å‹ï¼Œéœ€è¦åˆå§‹åŒ–æ–°çš„PEFTæ¨¡å‹
        logger.info("Init new peft model")

        # å¦‚æœä½¿ç”¨äº†8ä½æˆ–4ä½é‡åŒ–ï¼Œéœ€è¦ä¸ºé‡åŒ–è®­ç»ƒå‡†å¤‡æ¨¡å‹
        if load_in_8bit or load_in_4bit:
            # å‡†å¤‡æ¨¡å‹ç”¨äºk-bitè®­ç»ƒï¼ŒåŒ…æ‹¬æ¢¯åº¦æ£€æŸ¥ç‚¹è®¾ç½®
            model = prepare_model_for_kbit_training(model, training_args.gradient_checkpointing)

        # è§£æç›®æ ‡æ¨¡å—å‚æ•°ï¼šå¦‚æœæŒ‡å®šäº†target_modulesï¼Œåˆ™æŒ‰é€—å·åˆ†å‰²
        target_modules = script_args.target_modules.split(',') if script_args.target_modules else None

        # å¦‚æœç›®æ ‡æ¨¡å—åŒ…å«'all'ï¼Œåˆ™è‡ªåŠ¨æŸ¥æ‰¾æ‰€æœ‰çº¿æ€§å±‚
        if target_modules and 'all' in target_modules:
            # æ ¹æ®é‡åŒ–ç±»å‹æŸ¥æ‰¾æ‰€æœ‰çº¿æ€§å±‚åç§°
            target_modules = find_all_linear_names(model, int4=load_in_4bit, int8=load_in_8bit)

        # è§£æéœ€è¦ä¿å­˜çš„æ¨¡å—å‚æ•°ï¼šå¦‚æœæŒ‡å®šäº†modules_to_saveï¼Œåˆ™æŒ‰é€—å·åˆ†å‰²
        modules_to_save = script_args.modules_to_save
        if modules_to_save is not None:
            modules_to_save = modules_to_save.split(',')

        # è®°å½•PEFTé…ç½®ä¿¡æ¯
        logger.info(f"Peft target_modules: {target_modules}")
        logger.info(f"Peft lora_rank: {script_args.lora_rank}")

        # åˆ›å»ºLoRAé…ç½®å¯¹è±¡
        peft_config = LoraConfig(
            task_type=TaskType.CAUSAL_LM,  # ä»»åŠ¡ç±»å‹ï¼šå› æœè¯­è¨€æ¨¡å‹
            target_modules=target_modules,  # ç›®æ ‡æ¨¡å—ï¼šåº”ç”¨LoRAçš„æ¨¡å—
            inference_mode=False,  # éæ¨ç†æ¨¡å¼ï¼Œå¯ç”¨è®­ç»ƒ
            r=script_args.lora_rank,  # LoRA rankï¼šä½ç§©çŸ©é˜µçš„ç§©
            lora_alpha=script_args.lora_alpha,  # LoRA alphaï¼šç¼©æ”¾å› å­
            lora_dropout=script_args.lora_dropout,  # LoRA dropoutï¼šé˜²æ­¢è¿‡æ‹Ÿåˆ
            modules_to_save=modules_to_save)  # éœ€è¦å®Œæ•´ä¿å­˜çš„æ¨¡å—

        # ä½¿ç”¨PEFTé…ç½®åŒ…è£…æ¨¡å‹ï¼Œè¿”å›å¯è®­ç»ƒçš„PEFTæ¨¡å‹
        model = get_peft_model(model, peft_config)

    # å°†æ‰€æœ‰éœ€è¦æ¢¯åº¦çš„å‚æ•°è½¬æ¢ä¸ºfloat32ç²¾åº¦ï¼Œç¡®ä¿è®­ç»ƒç¨³å®šæ€§
    for param in filter(lambda p: p.requires_grad, model.parameters()):
        param.data = param.data.to(torch.float32)

    # æ‰“å°å¯è®­ç»ƒå‚æ•°ä¿¡æ¯ï¼Œæ˜¾ç¤ºå‚æ•°æ•°é‡ç»Ÿè®¡
    model.print_trainable_parameters()

    # è¿”å›é…ç½®å¥½çš„PEFTæ¨¡å‹
    return model


def train_model(trainer, training_args, max_train_samples):
    """
    è®­ç»ƒæ¨¡å‹
    
    Args:
        trainer: è®­ç»ƒå™¨å¯¹è±¡ï¼Œè´Ÿè´£æ¨¡å‹è®­ç»ƒçš„æ‰§è¡Œ
        training_args: è®­ç»ƒå‚æ•°é…ç½®å¯¹è±¡ï¼ŒåŒ…å«å„ç§è®­ç»ƒç›¸å…³çš„é…ç½®
        max_train_samples: æœ€å¤§è®­ç»ƒæ ·æœ¬æ•°é‡
    
    Returns:
        dict: è®­ç»ƒç»“æœæŒ‡æ ‡å­—å…¸ï¼ŒåŒ…å«æŸå¤±ã€è®­ç»ƒæ ·æœ¬æ•°ç­‰è®­ç»ƒç»Ÿè®¡ä¿¡æ¯
    """
    # ä»…åœ¨ä¸»è¿›ç¨‹(è¿›ç¨‹IDä¸º0)ä¸­æ‰“å°è®­ç»ƒå¼€å§‹ä¿¡æ¯
    if trainer.is_world_process_zero():
        logger.info("*** å¼€å§‹æ¨¡å‹è®­ç»ƒ ***")

        # è·å–è®­ç»ƒæ•°æ®åŠ è½½å™¨ä¸­çš„ä¸€ä¸ªæ ·æœ¬ç”¨äºè°ƒè¯•
        sample = next(iter(trainer.get_train_dataloader()))
        logger.debug(f"è®­ç»ƒæ•°æ®æ ·æœ¬ç¤ºä¾‹: {sample}")

        # æ‰“å°æ ·æœ¬ä¸­çš„input_idså’Œlabelsçš„å‰3ä¸ªå…ƒç´ ï¼Œç”¨äºæ£€æŸ¥æ•°æ®æ ¼å¼
        logger.debug(f"è¾“å…¥IDåºåˆ—:\n{list(sample['input_ids'])[:3]}, \næ ‡ç­¾åºåˆ—:\n{list(sample['labels'])[:3]}")

    # åˆå§‹åŒ–æ£€æŸ¥ç‚¹è·¯å¾„ï¼Œç”¨äºæ–­ç‚¹ç»­è®­
    checkpoint = None
    if training_args.resume_from_checkpoint is not None:
        checkpoint = training_args.resume_from_checkpoint

    # æ‰§è¡Œæ¨¡å‹è®­ç»ƒï¼Œæ”¯æŒä»æ£€æŸ¥ç‚¹æ¢å¤è®­ç»ƒ
    train_result = trainer.train(resume_from_checkpoint=checkpoint)

    # è·å–è®­ç»ƒç»“æœæŒ‡æ ‡
    metrics = train_result.metrics

    # æ·»åŠ è®­ç»ƒæ ·æœ¬æ•°é‡åˆ°æŒ‡æ ‡ä¸­
    metrics["train_samples"] = max_train_samples

    # è®°å½•è®­ç»ƒæŒ‡æ ‡åˆ°æ—¥å¿—
    trainer.log_metrics("train", metrics)

    # ä¿å­˜è®­ç»ƒæŒ‡æ ‡åˆ°æ–‡ä»¶
    trainer.save_metrics("train", metrics)

    # ä¿å­˜è®­ç»ƒå™¨çŠ¶æ€ï¼ŒåŒ…æ‹¬ä¼˜åŒ–å™¨çŠ¶æ€ã€è°ƒåº¦å™¨çŠ¶æ€ç­‰
    trainer.save_state()

    # è¿”å›è®­ç»ƒæŒ‡æ ‡
    return metrics


def evaluate_model(trainer, max_eval_samples):
    """è¯„ä¼°æ¨¡å‹

    Args:
        trainer: è®­ç»ƒå™¨å¯¹è±¡ï¼Œè´Ÿè´£æ¨¡å‹çš„è®­ç»ƒå’Œè¯„ä¼°æ“ä½œ
        max_eval_samples: æœ€å¤§è¯„ä¼°æ ·æœ¬æ•°é‡ï¼Œç”¨äºæŒ‡å®šè¯„ä¼°æ—¶ä½¿ç”¨çš„æ•°æ®é‡

    Returns:
        dict: åŒ…å«è¯„ä¼°æŒ‡æ ‡çš„å­—å…¸ï¼ŒåŒ…æ‹¬lossã€perplexityç­‰æŒ‡æ ‡
    """
    # æ£€æŸ¥å½“å‰è¿›ç¨‹æ˜¯å¦ä¸ºä¸»è¿›ç¨‹ï¼ˆrank 0ï¼‰
    # åªæœ‰ä¸»è¿›ç¨‹æ‰ä¼šæ‰“å°æ—¥å¿—ä¿¡æ¯ï¼Œé¿å…å¤šè¿›ç¨‹é‡å¤è¾“å‡º
    if trainer.is_world_process_zero():
        logger.info("*** Evaluate ***")

    # æ‰§è¡Œæ¨¡å‹è¯„ä¼°ï¼Œè·å–è¯„ä¼°æŒ‡æ ‡
    # metric_key_prefix="eval" ä¸ºæŒ‡æ ‡é”®åæ·»åŠ å‰ç¼€ï¼Œå¦‚"eval_loss"
    metrics = trainer.evaluate(metric_key_prefix="eval")

    # æ·»åŠ è¯„ä¼°æ ·æœ¬æ•°é‡åˆ°æŒ‡æ ‡å­—å…¸ä¸­
    metrics["eval_samples"] = max_eval_samples

    # è®¡ç®—å›°æƒ‘åº¦ï¼ˆPerplexityï¼‰
    # å›°æƒ‘åº¦æ˜¯è¯„ä¼°è¯­è¨€æ¨¡å‹æ€§èƒ½çš„é‡è¦æŒ‡æ ‡ï¼Œå€¼è¶Šå°è¡¨ç¤ºæ¨¡å‹é¢„æµ‹è¶Šå‡†ç¡®
    # ä½¿ç”¨æ•°å­¦å…¬å¼ï¼šperplexity = exp(loss)
    try:
        perplexity = math.exp(metrics["eval_loss"])
    except OverflowError:
        # å½“lossè¿‡å¤§æ—¶ï¼ŒæŒ‡æ•°è¿ç®—å¯èƒ½å¯¼è‡´æ•°å€¼æº¢å‡º
        # æ­¤æ—¶å°†å›°æƒ‘åº¦è®¾ä¸ºæ— ç©·å¤§ï¼Œè¡¨ç¤ºæ¨¡å‹æ€§èƒ½æå·®
        perplexity = float("inf")

    # å°†å›°æƒ‘åº¦æ·»åŠ åˆ°æŒ‡æ ‡å­—å…¸ä¸­
    metrics["perplexity"] = perplexity

    # è®°å½•è¯„ä¼°æŒ‡æ ‡åˆ°æ—¥å¿—ç³»ç»Ÿ
    # è¿™äº›æŒ‡æ ‡ä¼šè¢«ä¿å­˜åˆ°è®­ç»ƒæ—¥å¿—ä¸­ï¼Œä¾¿äºåç»­åˆ†æ
    trainer.log_metrics("eval", metrics)

    # ä¿å­˜è¯„ä¼°æŒ‡æ ‡åˆ°æ–‡ä»¶
    # é€šå¸¸ä¼šä¿å­˜åˆ°output_dir/eval_results.jsonç­‰æ–‡ä»¶ä¸­
    trainer.save_metrics("eval", metrics)

    # è¿”å›åŒ…å«æ‰€æœ‰è¯„ä¼°æŒ‡æ ‡çš„å­—å…¸
    return metrics


def setup_model_config(model_args, script_args):
    dtype = (
        model_args.dtype
        if model_args.dtype in ["auto", None]
        else getattr(torch, model_args.dtype)
    )

    config_kwargs = {
        "trust_remote_code": model_args.trust_remote_code,
        "cache_dir": model_args.cache_dir,
        "revision": model_args.model_revision,
        "token": model_args.hf_hub_token,
    }
    config = AutoConfig.from_pretrained(model_args.model_name_or_path, **config_kwargs)

    return config, config_kwargs, dtype


def main():
    # 1. åˆ›å»ºå‚æ•°è§£æå™¨ï¼Œè§£æå››ç§ç±»å‹çš„å‚æ•°
    # - ModelArguments: æ¨¡å‹ç›¸å…³å‚æ•°ï¼ˆæ¨¡å‹è·¯å¾„ã€é‡åŒ–è®¾ç½®ç­‰ï¼‰
    # - DataArguments: æ•°æ®ç›¸å…³å‚æ•°ï¼ˆæ•°æ®é›†è·¯å¾„ã€æ ·æœ¬æ•°é‡é™åˆ¶ç­‰ï¼‰
    # - Seq2SeqTrainingArguments: è®­ç»ƒå‚æ•°ï¼ˆå­¦ä¹ ç‡ã€æ‰¹æ¬¡å¤§å°ã€è®­ç»ƒæ­¥æ•°ç­‰ï¼‰
    # - ScriptArguments: è„šæœ¬ç‰¹å®šå‚æ•°ï¼ˆLoRAè®¾ç½®ã€æ¨¡æ¿é€‰æ‹©ç­‰ï¼‰
    parser = HfArgumentParser((ModelArguments, DataArguments, Seq2SeqTrainingArguments, ScriptArguments))

    # 2. è§£æå‘½ä»¤è¡Œå‚æ•°
    if len(sys.argv) == 2 and sys.argv[1].endswith(".json"):
        # å¦‚æœæä¾›äº†JSONé…ç½®æ–‡ä»¶ï¼Œä»æ–‡ä»¶è§£æå‚æ•°
        model_args, data_args, training_args, script_args = parser.parse_json_file(
            json_file=os.path.abspath(sys.argv[1]))
    else:
        # å¦åˆ™ä»å‘½ä»¤è¡Œå‚æ•°è§£æ
        model_args, data_args, training_args, script_args = parser.parse_args_into_dataclasses(look_for_args_file=False)

    # 3. å¤„ç†DeepSpeedé…ç½®
    if training_args.deepspeed is not None:
        # æ¸…ç©ºåˆ†å¸ƒå¼çŠ¶æ€ä¸­çš„DeepSpeedæ’ä»¶é…ç½®ï¼Œé¿å…å†²çª
        training_args.distributed_state.deepspeed_plugin = None

    # 4. ç¡®å®šæ˜¯å¦ä¸ºä¸»è¿›ç¨‹
    # local_rankä¸º-1è¡¨ç¤ºå•GPUï¼Œ0è¡¨ç¤ºå¤šGPUä¸­çš„ä¸»è¿›ç¨‹
    is_main_process = training_args.local_rank in [-1, 0]

    # 5. ä»…åœ¨ä¸»è¿›ç¨‹ä¸Šæ‰“å°é…ç½®ä¿¡æ¯
    if is_main_process:
        logger.info(f"Model args: {model_args}")
        logger.info(f"Data args: {data_args}")
        logger.info(f"Training args: {training_args}")
        logger.info(f"Script args: {script_args}")
        # æ‰“å°åˆ†å¸ƒå¼è®­ç»ƒå’Œç²¾åº¦è®¾ç½®ä¿¡æ¯
        logger.info(
            f"Process rank: {training_args.local_rank}, device: {training_args.device}, n_gpu: {training_args.n_gpu}"
            + f" distributed training: {bool(training_args.local_rank != -1)}, 16-bits training: {training_args.fp16}"
        )

    # 6. è®¾ç½®éšæœºç§å­ï¼Œç¡®ä¿å®éªŒå¯å¤ç°
    set_seed(training_args.seed)

    # 7. æ£€æŸ¥å¹¶ä¼˜åŒ–GPUæ˜¾å­˜ä½¿ç”¨
    check_and_optimize_memory()

    # 8. è®¾ç½®tokenizerå’Œæç¤ºè¯æ¨¡æ¿
    tokenizer, prompt_template = setup_tokenizer(model_args, script_args)

    # 9. è®¾ç½®æŸå¤±è®¡ç®—æ—¶çš„å¿½ç•¥ç´¢å¼•
    # å¦‚æœé…ç½®ä¸ºå¿½ç•¥padding tokençš„æŸå¤±ï¼Œä½¿ç”¨-100ï¼ˆLabelSmoother.ignore_indexï¼‰
    # å¦åˆ™ä½¿ç”¨tokenizerçš„pad_token_id
    IGNORE_INDEX = (
        LabelSmoother.ignore_index
        if data_args.ignore_pad_token_for_loss
        else tokenizer.pad_token_id
    )

    # 10. åŠ è½½æ•°æ®é›†
    raw_datasets = load_datasets(data_args, model_args)

    # 11. å¤„ç†è®­ç»ƒæ•°æ®é›†
    train_dataset = None
    max_train_samples = 0
    if training_args.do_train:
        # æ£€æŸ¥æ˜¯å¦æœ‰è®­ç»ƒæ•°æ®
        if "train" not in raw_datasets:
            raise ValueError("--do_train requires a train dataset")

        # æ‰“ä¹±è®­ç»ƒæ•°æ®å¹¶è·å–æ•°æ®é›†
        train_dataset = raw_datasets['train'].shuffle(seed=42)

        # å¯¹è®­ç»ƒæ•°æ®è¿›è¡Œé¢„å¤„ç†ï¼ˆtokenizationã€æ ¼å¼è½¬æ¢ç­‰ï¼‰
        train_dataset, max_train_samples = process_train_dataset(
            train_dataset, data_args, training_args, is_main_process, tokenizer, script_args, IGNORE_INDEX,
            prompt_template
        )

    # 12. å¤„ç†éªŒè¯æ•°æ®é›†
    eval_dataset = None
    max_eval_samples = 0
    if training_args.do_eval:
        # æ£€æŸ¥æ˜¯å¦æœ‰éªŒè¯æ•°æ®
        if "validation" not in raw_datasets:
            raise ValueError("--do_eval requires a validation dataset")

        # æ‰“ä¹±éªŒè¯æ•°æ®å¹¶è·å–æ•°æ®é›†
        eval_dataset = raw_datasets['validation'].shuffle(seed=42)

        # å¯¹éªŒè¯æ•°æ®è¿›è¡Œé¢„å¤„ç†
        eval_dataset, max_eval_samples = process_eval_dataset(
            eval_dataset, data_args, training_args, tokenizer, script_args, IGNORE_INDEX, prompt_template
        )

    # 13. åŠ è½½æ¨¡å‹
    if model_args.model_name_or_path:
        # è®¾ç½®æ¨¡å‹é…ç½®
        config, config_kwargs, dtype = setup_model_config(model_args, script_args)

        # è®¾ç½®é‡åŒ–é…ç½®ï¼ˆ4bit/8bité‡åŒ–ï¼‰
        quantization_config, load_in_4bit, load_in_8bit \
            = setup_quantization_config(model_args, script_args, dtype, training_args)

        # è®¾ç½®æ¨¡å‹åŠ è½½å‚æ•°
        model_kwargs = setup_model_kwargs(model_args, config, config_kwargs, dtype, quantization_config, training_args)

        # 14. å¤„ç†åˆ†å¸ƒå¼è®­ç»ƒè®¾ç½®
        world_size = int(os.environ.get("WORLD_SIZE", "1"))
        ddp = world_size != 1  # æ˜¯å¦ä¸ºåˆ†å¸ƒå¼æ•°æ®å¹¶è¡Œ
        if ddp:
            # è®¾ç½®è®¾å¤‡æ˜ å°„ï¼Œæ¯ä¸ªè¿›ç¨‹ä½¿ç”¨å¯¹åº”çš„GPU
            model_args.device_map = {"": int(os.environ.get("LOCAL_RANK", "0"))}
            # è°ƒæ•´æ¢¯åº¦ç´¯ç§¯æ­¥æ•°ä»¥é€‚åº”å¤šè¿›ç¨‹
            training_args.gradient_accumulation_steps = training_args.gradient_accumulation_steps // world_size or 1

        # 15. æ£€æŸ¥QLoRAä¸ZeRO-3çš„å…¼å®¹æ€§
        if script_args.qlora and (len(training_args.fsdp) > 0 or is_deepspeed_zero3_enabled()):
            logger.warning("FSDP and DeepSpeed ZeRO-3 are both currently incompatible with QLoRA.")

        # æ‰“å°æ¨¡å‹åŠ è½½é…ç½®
        logger.info(f"ğŸ”§ å¤§æ¨¡å‹è®­ç»ƒé…ç½®:")
        logger.info(f"  model_kwargs: {model_kwargs}")

        # 16. ä»é¢„è®­ç»ƒæ¨¡å‹åŠ è½½
        model = AutoModelForCausalLM.from_pretrained(
            model_args.model_name_or_path,
            **model_kwargs
        )

        logger.info("âœ… æ¨¡å‹åŠ è½½å®Œæˆ")

        # æ‰“å°æ¨¡å‹ä¿¡æ¯ï¼ˆå‚æ•°æ•°é‡ã€è®¾å¤‡åˆ†å¸ƒç­‰ï¼‰
        log_model_info(model)

        # è®¾ç½®NEFTuneå™ªå£°æ³¨å…¥ï¼ˆæé«˜æ³›åŒ–èƒ½åŠ›ï¼‰
        setup_neftune(model, model_args)

        # åº”ç”¨æ¨¡å‹ç‰¹å®šè¡¥ä¸ï¼ˆå¦‚ChatGLMã€Mixtralç­‰ï¼‰
        setup_model_patches(model, config, training_args)

        # 17. å¤šGPUå¹¶è¡Œè®¾ç½®
        if not ddp and torch.cuda.device_count() > 1:
            model.is_parallelizable = True
            model.model_parallel = True

        # 18. åº”ç”¨PEFTï¼ˆå‚æ•°é«˜æ•ˆå¾®è°ƒï¼‰
        if script_args.use_peft:
            model = setup_peft_model(model, script_args, training_args, load_in_8bit, load_in_4bit)
        else:
            # å…¨å‚æ•°å¾®è°ƒæ¨¡å¼
            logger.info("Fine-tuning method: Full parameters training")
            model = model.float()
            print_trainable_parameters(model)
    else:
        # å¿…é¡»æŒ‡å®šé¢„è®­ç»ƒæ¨¡å‹è·¯å¾„
        raise ValueError(f"Error, model_name_or_path is None, SFT must be loaded from a pre-trained model")

    # 19. åˆ›å»ºæ•°æ®æ•´ç†å™¨
    # è´Ÿè´£å°†batchä¸­çš„æ•°æ®æ•´ç†æˆæ¨¡å‹è¾“å…¥æ ¼å¼
    data_collator = DataCollatorForSeq2Seq(
        tokenizer=tokenizer,
        model=model,
        label_pad_token_id=IGNORE_INDEX,
        pad_to_multiple_of=4 if tokenizer.padding_side == "right" else None,
    )

    # 20. åˆ›å»ºè®­ç»ƒå™¨
    # ä½¿ç”¨è‡ªå®šä¹‰çš„SavePeftModelTraineræ¥æ”¯æŒPEFTæ¨¡å‹çš„ä¿å­˜
    trainer = SavePeftModelTrainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset if training_args.do_train else None,
        eval_dataset=eval_dataset if training_args.do_eval else None,
        tokenizer=tokenizer,
        data_collator=data_collator,
    )

    # 21. æ‰§è¡Œè®­ç»ƒ
    if training_args.do_train:
        # å¼€å§‹è®­ç»ƒå¹¶è·å–è®­ç»ƒæŒ‡æ ‡
        metrics = train_model(trainer, training_args, max_train_samples)

        # è®­ç»ƒå®Œæˆåé‡æ–°å¯ç”¨ç¼“å­˜ä»¥æé«˜æ¨ç†æ•ˆç‡
        model.config.use_cache = True
        tokenizer.padding_side = "left"
        tokenizer.init_kwargs["padding_side"] = "left"

        # ä»…åœ¨ä¸»è¿›ç¨‹ä¸Šä¿å­˜æ¨¡å‹
        if trainer.is_world_process_zero():
            logger.debug(f"Training metrics: {metrics}")
            logger.info(f"Saving model checkpoint to {training_args.output_dir}")

            # æ ¹æ®æ˜¯å¦ä½¿ç”¨DeepSpeed ZeRO-3é€‰æ‹©ä¸åŒçš„ä¿å­˜æ–¹å¼
            if is_deepspeed_zero3_enabled():
                save_model_zero3(model, tokenizer, training_args, trainer)
            else:
                save_model(model, tokenizer, training_args)

    # 22. æ‰§è¡Œè¯„ä¼°
    if training_args.do_eval:
        # åœ¨éªŒè¯é›†ä¸Šè¯„ä¼°æ¨¡å‹æ€§èƒ½
        metrics = evaluate_model(trainer, max_eval_samples)

        # ä»…åœ¨ä¸»è¿›ç¨‹ä¸Šæ‰“å°è¯„ä¼°ç»“æœ
        if trainer.is_world_process_zero():
            logger.debug(f"Eval metrics: {metrics}")


if __name__ == "__main__":
    main()
