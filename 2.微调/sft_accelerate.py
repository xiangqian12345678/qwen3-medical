import math
import os
import sys
from dataclasses import dataclass, field
from glob import glob
from typing import Literal, Optional, Tuple

import torch
import torch.utils.data
from datasets import load_dataset
from loguru import logger
from peft import LoraConfig, TaskType, get_peft_model, PeftModel, prepare_model_for_kbit_training
from transformers import (
    AutoConfig,
    AutoModelForCausalLM,
    AutoTokenizer,
    HfArgumentParser,
    Seq2SeqTrainingArguments,
    set_seed,
    BitsAndBytesConfig,
    DataCollatorForSeq2Seq,
    get_linear_schedule_with_warmup,
)
from transformers.trainer_pt_utils import LabelSmoother
from tqdm.auto import tqdm

from accelerate import Accelerator
from accelerate.utils import set_seed as accelerate_set_seed

is_flash_attn_2_available = False
try:
    from flash_attn import flash_attn_func, flash_attn_varlen_func
    from flash_attn.bert_padding import pad_input, unpad_input

    is_flash_attn_2_available = True
except ImportError:
    is_flash_attn_2_available = False
from template import get_conv_template


@dataclass
class ModelArguments:
    """ä¸è¦åŠ è½½ / å¾®è°ƒçš„æ¨¡å‹ã€é…ç½®å’Œ tokenizer ç›¸å…³çš„å‚æ•°"""

    # é¢„è®­ç»ƒæ¨¡å‹åç§°æˆ–æœ¬åœ°è·¯å¾„
    # ä¾‹å¦‚: "meta-llama/Llama-2-7b-hf" æˆ– "./checkpoints/llama"
    model_name_or_path: Optional[str] = field(default=None)

    # æ˜¯å¦ä»¥ 8bit é‡åŒ–æ–¹å¼åŠ è½½æ¨¡å‹ï¼ˆbitsandbytesï¼‰
    # ä¼˜ç‚¹ï¼šæ˜¾å­˜å ç”¨æ˜¾è‘—é™ä½
    # ç¼ºç‚¹ï¼šæ¨ç† / è®­ç»ƒç²¾åº¦ç•¥æœ‰æŸå¤±
    load_in_8bit: bool = field(default=False)

    # æ˜¯å¦ä»¥ 4bit é‡åŒ–æ–¹å¼åŠ è½½æ¨¡å‹ï¼ˆQLoRA åœºæ™¯å¸¸ç”¨ï¼‰
    # é€šå¸¸ä¸ LoRA ä¸€èµ·ä½¿ç”¨ï¼Œæå¤§é™ä½æ˜¾å­˜å ç”¨
    load_in_4bit: bool = field(default=False)

    # tokenizer åç§°æˆ–è·¯å¾„
    # é»˜è®¤ä¸ model_name_or_path ç›¸åŒ
    tokenizer_name_or_path: Optional[str] = field(default=None)

    # HuggingFace ç¼“å­˜ç›®å½•
    # ç”¨äºå­˜æ”¾ä¸‹è½½çš„æ¨¡å‹æƒé‡ã€é…ç½®ã€tokenizer ç­‰
    cache_dir: Optional[str] = field(default=None)

    # æ¨¡å‹ç‰ˆæœ¬ï¼ˆgit revision / branch / tag / commitï¼‰
    # å¸¸è§å–å€¼ï¼š"main"ã€"v1.0"ã€å…·ä½“ commit hash
    model_revision: Optional[str] = field(default="main")

    # HuggingFace Hub è®¿é—® token
    # ç”¨äºåŠ è½½ç§æœ‰æ¨¡å‹æˆ–é¿å…é¢‘ç¹ rate limit
    hf_hub_token: Optional[str] = field(default=None)

    # æ˜¯å¦ä½¿ç”¨ fast tokenizerï¼ˆRust å®ç°ï¼‰
    # ä¼˜ç‚¹ï¼šé€Ÿåº¦å¿«
    # ç¼ºç‚¹ï¼šä¸ªåˆ«æ¨¡å‹ï¼ˆå°¤å…¶æ˜¯è‡ªå®šä¹‰ tokenizerï¼‰å¯èƒ½ä¸å…¼å®¹
    use_fast_tokenizer: bool = field(default=False)

    # æ¨¡å‹æƒé‡çš„è®¡ç®—ç²¾åº¦
    # å¸¸è§å–å€¼ï¼š"float16"ã€"bfloat16"ã€"float32"
    # é€šå¸¸ä¸ mixed precision è®­ç»ƒç›¸å…³
    dtype: Optional[str] = field(default="float16")

    # è®¾å¤‡æ˜ å°„ç­–ç•¥
    # "auto"ï¼šç”± accelerate è‡ªåŠ¨åˆ†é…åˆ°å¤šå¡ / CPU / GPU
    # ä¹Ÿå¯æ‰‹åŠ¨æŒ‡å®šï¼Œå¦‚ {"": 0}
    device_map: Optional[str] = field(default="auto")

    # æ˜¯å¦ä¿¡ä»»è¿œç¨‹ä»“åº“ä¸­çš„è‡ªå®šä¹‰ä»£ç 
    # å¾ˆå¤šå›½äº§æ¨¡å‹ / å®šåˆ¶æ¨¡å‹å¿…é¡»è®¾ä¸º True
    trust_remote_code: bool = field(default=True)

    # RoPEï¼ˆæ—‹è½¬ä½ç½®ç¼–ç ï¼‰ç¼©æ”¾ç­–ç•¥
    # linearï¼šçº¿æ€§ç¼©æ”¾
    # dynamicï¼šåŠ¨æ€ NTK ç¼©æ”¾
    # å¸¸ç”¨äºæ‰©å±•ä¸Šä¸‹æ–‡é•¿åº¦ï¼ˆå¦‚ 4k -> 32kï¼‰
    rope_scaling: Optional[Literal["linear", "dynamic"]] = field(default=None)

    # æ˜¯å¦å¯ç”¨ FlashAttention-2
    # æ˜¾è‘—æå‡é•¿åºåˆ—è®­ç»ƒ / æ¨ç†é€Ÿåº¦ï¼Œå¹¶é™ä½æ˜¾å­˜
    # éœ€è¦ï¼š
    # 1. æ”¯æŒçš„ GPUï¼ˆå¦‚ A100 / H100 / éƒ¨åˆ† RTXï¼‰
    # 2. å¯¹åº”ç‰ˆæœ¬çš„ torch / flash-attn
    flash_attn: Optional[bool] = field(
        default=False,
        metadata={"help": "Enable FlashAttention-2 for faster training."}
    )


@dataclass
class DataArguments:
    # ä½¿ç”¨ datasets åº“åŠ è½½çš„ã€Œæ•°æ®é›†åç§°ã€
    # ä¾‹å¦‚ï¼šwikitext, c4, openwebtext
    # æ”¯æŒå¤šä¸ªæ•°æ®é›†ï¼Œç”¨è‹±æ–‡é€—å·åˆ†éš”ï¼š
    # --dataset_name wikitext,c4
    dataset_name: Optional[str] = field(
        default=None,
        metadata={
            "help": "The name of the dataset to use (via the datasets library). "
                    "Support multiple datasets separated by commas."
        }
    )

    # æ•°æ®é›†çš„ã€Œé…ç½®åç§°ã€
    # å¸¸è§äºåŒä¸€ä¸ª dataset ä¸‹çš„ä¸åŒå­ç‰ˆæœ¬
    # ä¾‹å¦‚ï¼šwikitext-2-raw-v1, wikitext-103-v1
    # ä¸ dataset_name ä¸€ä¸€å¯¹åº”ï¼Œä¹Ÿæ”¯æŒé€—å·åˆ†éš”
    dataset_config_name: Optional[str] = field(
        default=None,
        metadata={
            "help": "The configuration name of the dataset to use (via the datasets library). "
                    "Support multiple configs separated by commas."
        }
    )

    # æœ¬åœ°è®­ç»ƒæ•°æ®ç›®å½•
    # å½“ä¸ä½¿ç”¨ HuggingFace datasetsï¼Œè€Œæ˜¯è‡ªæœ‰æ•°æ®æ—¶ä½¿ç”¨
    # é€šå¸¸ç›®å½•ä¸‹æ˜¯ json/jsonl/txt ç­‰æ–‡ä»¶
    train_file_dir: str = field(
        default=None,
        metadata={"help": "Path to the training data."}
    )

    # æœ¬åœ°éªŒè¯æ•°æ®ç›®å½•
    # å¦‚æœä¸æä¾›ï¼Œå¸¸è§åšæ³•æ˜¯ä»è®­ç»ƒé›†é‡Œåˆ‡åˆ†ä¸€éƒ¨åˆ†ä½œä¸ºéªŒè¯é›†
    validation_file_dir: str = field(
        default=None,
        metadata={"help": "Path to the validation data."}
    )

    # æœ€å¤§è®­ç»ƒæ ·æœ¬æ•°ï¼ˆè°ƒè¯• / å°è§„æ¨¡å®éªŒéå¸¸æœ‰ç”¨ï¼‰
    # ä¾‹å¦‚ï¼š--max_train_samples 10000
    # None è¡¨ç¤ºä½¿ç”¨å…¨éƒ¨è®­ç»ƒæ•°æ®
    max_train_samples: Optional[int] = field(default=None)

    # æœ€å¤§éªŒè¯æ ·æœ¬æ•°
    # ç”¨äºå¿«é€ŸéªŒè¯æˆ–é™ä½è¯„æµ‹æˆæœ¬
    max_eval_samples: Optional[int] = field(default=None)

    # æ˜¯å¦è¦†ç›– datasets çš„æœ¬åœ°ç¼“å­˜
    # å½“ä½ ä¿®æ”¹äº†æ•°æ®å¤„ç†é€»è¾‘ï¼ˆtokenize / mapï¼‰ä½†ç¼“å­˜æ²¡å˜æ—¶ï¼Œéœ€è¦è®¾ä¸º True
    overwrite_cache: bool = field(
        default=False,
        metadata={"help": "Overwrite the cached training and evaluation sets"}
    )

    # å½“æœªæ˜¾å¼æä¾› validation_file_dir æ—¶ï¼Œ
    # ä»è®­ç»ƒé›†ä¸­åˆ‡åˆ†å‡ºå¤šå°‘ç™¾åˆ†æ¯”ä½œä¸ºéªŒè¯é›†
    # é»˜è®¤ 1 è¡¨ç¤º 1%
    validation_split_percentage: Optional[int] = field(default=1)

    # æ•°æ®é¢„å¤„ç†ï¼ˆtokenize / mapï¼‰ä½¿ç”¨çš„è¿›ç¨‹æ•°
    # ä¸€èˆ¬è®¾ä¸º CPU æ ¸æ•°ï¼Œèƒ½æ˜¾è‘—åŠ å¿«æ•°æ®å¤„ç†
    # ä¾‹å¦‚ï¼š--preprocessing_num_workers 8
    preprocessing_num_workers: Optional[int] = field(default=None)

    # æ˜¯å¦åœ¨è®¡ç®— loss æ—¶å¿½ç•¥ padding token
    # å¯¹äº causal LM / seq2seq è®­ç»ƒå‡ ä¹å¿…é¡»ä¸º True
    # å¦åˆ™ pad token ä¼šå¹²æ‰° loss
    ignore_pad_token_for_loss: bool = field(default=True)


@dataclass
class ScriptArguments:
    # æ˜¯å¦å¯ç”¨ PEFTï¼ˆParameter-Efficient Fine-Tuningï¼‰
    # Trueï¼šä½¿ç”¨ LoRA / QLoRA ç­‰è½»é‡å¾®è°ƒæ–¹å¼
    # Falseï¼šå¯¹æ¨¡å‹å…¨å‚æ•°è¿›è¡Œå¾®è°ƒï¼ˆæ˜¾å­˜ã€ç®—åŠ›æ¶ˆè€—å·¨å¤§ï¼‰
    use_peft: bool = field(default=True)

    # æ˜¯å¦åœ¨è®­ç»ƒæ—¶å¯¹ promptï¼ˆè¾“å…¥éƒ¨åˆ†ï¼‰è®¡ç®— loss
    # Falseï¼šåªå¯¹ assistant çš„å›ç­”éƒ¨åˆ†è®¡ç®— lossï¼ˆæ¨èï¼Œç¬¦åˆå¯¹è¯æ¨¡å‹è®­ç»ƒèŒƒå¼ï¼‰
    # Trueï¼šè¾“å…¥ + è¾“å‡ºéƒ½ä¼šå‚ä¸ lossï¼ˆæ›´åƒè¯­è¨€æ¨¡å‹ç»­å†™è®­ç»ƒï¼‰
    train_on_inputs: bool = field(default=False)

    # LoRA æ³¨å…¥çš„ç›®æ ‡æ¨¡å—
    # "all"ï¼šè‡ªåŠ¨æŸ¥æ‰¾æ‰€æœ‰ Linear å±‚å¹¶æ³¨å…¥ï¼ˆå¸¸è§é»˜è®¤ï¼‰
    # ä¹Ÿå¯ä»¥æ˜¯å­—ç¬¦ä¸²å½¢å¼çš„æ¨¡å—ååˆ—è¡¨ï¼Œå¦‚ "q_proj,k_proj,v_proj,o_proj"
    target_modules: Optional[str] = field(default="all")

    # LoRA çš„ç§©ï¼ˆrankï¼‰
    # å†³å®š LoRA çš„å‚æ•°é‡ä¸è¡¨è¾¾èƒ½åŠ›
    # å¸¸è§å–å€¼ï¼š4 / 8 / 16
    # rank è¶Šå¤§ï¼Œæ•ˆæœä¸Šé™è¶Šé«˜ï¼Œä½†æ˜¾å­˜å’Œè®¡ç®—é‡ä¹Ÿä¼šå¢åŠ 
    lora_rank: Optional[int] = field(default=8)

    # LoRA çš„ dropout æ¦‚ç‡
    # ç”¨äºé˜²æ­¢è¿‡æ‹Ÿåˆï¼Œé€šå¸¸åœ¨å°æ•°æ®é›†åœºæ™¯ä¸‹å¾ˆæœ‰ç”¨
    # å¸¸è§å–å€¼ï¼š0.0 ~ 0.1
    lora_dropout: Optional[float] = field(default=0.05)

    # LoRA çš„ç¼©æ”¾ç³»æ•°ï¼ˆalphaï¼‰
    # å®é™…ç”Ÿæ•ˆæƒé‡ç¼©æ”¾ä¸ºï¼šalpha / rank
    # alpha è¶Šå¤§ï¼ŒLoRA æ›´æ–°å¯¹åŸæ¨¡å‹çš„å½±å“è¶Šå¼º
    lora_alpha: Optional[float] = field(default=32.0)

    # é™¤ LoRA å¤–ï¼Œè¿˜éœ€è¦"å®Œæ•´ä¿å­˜"çš„æ¨¡å—
    # å¸¸ç”¨äº embedding / lm_head / router ç­‰ä¸é€‚åˆ LoRA çš„æ¨¡å—
    # ä¾‹å¦‚ï¼š"embed_tokens,lm_head"
    modules_to_save: Optional[str] = field(default=None)

    # å·²è®­ç»ƒå¥½çš„ PEFT æƒé‡è·¯å¾„
    # ç”¨äºï¼š
    # 1ï¼‰ç»§ç»­è®­ç»ƒï¼ˆresumeï¼‰
    # 2ï¼‰åŠ è½½å·²æœ‰ LoRA æƒé‡è¿›è¡Œæ¨ç†
    peft_path: Optional[str] = field(default=None)

    # æ˜¯å¦ä½¿ç”¨ QLoRAï¼ˆ4bit é‡åŒ– + LoRAï¼‰
    # Trueï¼šæ˜¾å­˜å ç”¨æä½ï¼Œé€‚åˆå•å¡è®­ç»ƒå¤§æ¨¡å‹ï¼ˆ7B/13Bï¼‰
    # Falseï¼šæ™®é€š LoRAï¼ˆfp16 / bf16ï¼‰
    qlora: bool = field(default=False)

    # æ¨¡å‹æœ€å¤§æ”¯æŒçš„åºåˆ—é•¿åº¦ï¼ˆtoken æ•°ï¼‰
    # ä¼šå½±å“ï¼š
    # - tokenizer æˆªæ–­
    # - position embedding
    # - æ˜¾å­˜å ç”¨
    model_max_length: int = field(default=2048)

    # å¯¹è¯æ¨¡æ¿åç§°
    # å†³å®š prompt çš„æ ¼å¼ï¼ˆsystem / user / assistant çš„ç»„ç»‡æ–¹å¼ï¼‰
    # å¸¸è§ï¼švicunaã€chatmlã€alpacaã€llama2 ç­‰
    template_name: Optional[str] = field(default="vicuna")

    # æ˜¯å¦å¯ç”¨å¼ é‡å¹¶è¡Œï¼ˆTensor Parallelismï¼‰
    # Trueï¼šå°†å•å±‚å‚æ•°æŒ‰ç»´åº¦åˆ‡åˆ†åˆ°å¤šå¼  GPU ä¸Šï¼ˆé€‚åˆè¶…å¤§æ¨¡å‹ï¼‰
    # Falseï¼šå•å¡æˆ–æ™®é€š DDP è®­ç»ƒ
    use_tensor_parallel: bool = field(
    default = False,
    metadata = {"help": "Whether to use tensor parallelism for large models"}

)

def find_all_linear_names(model, int4=False, int8=False):
    """æŸ¥æ‰¾æ¨¡å‹ä¸­æ‰€æœ‰çš„çº¿æ€§å±‚åç§°"""
    cls = torch.nn.Linear
    if int4 or int8:
        import bitsandbytes as bnb
        if int4:
            cls = bnb.nn.Linear4bit
        elif int8:
            cls = bnb.nn.Linear8bit
    lora_module_names = set()
    for name, module in model.named_modules():
        if isinstance(module, cls):
            # last layer is not add to lora_module_names
            if 'lm_head' in name:
                continue
            if 'output_layer' in name:
                continue
            names = name.split('.')
            lora_module_names.add(names[0] if len(names) == 1 else names[-1])
    return sorted(lora_module_names)


def save_model(model, tokenizer, output_dir):
    """Save the model and the tokenizer."""
    os.makedirs(output_dir, exist_ok=True)

    # Take care of distributed/parallel training
    model_to_save = model.module if hasattr(model, "module") else model
    model_to_save.save_pretrained(output_dir)
    tokenizer.save_pretrained(output_dir)


def print_trainable_parameters(model):
    """
    Prints the number of trainable parameters in the model.
    """
    trainable_params = 0
    all_param = 0
    for _, param in model.named_parameters():
        all_param += param.numel()
        if param.requires_grad:
            trainable_params += param.numel()
    if all_param > 0:
        print(
            f"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param:.2f}"
        )
    else:
        print("No parameters found in the model (possibly using DeepSpeed ZeRO optimization)")

from datasets import load_dataset, concatenate_datasets, Dataset
import os
from glob import glob

def load_hf_datasets(data_args, model_args):
    """
    åŠ è½½ HuggingFace Hub æ•°æ®é›†ï¼ˆæ”¯æŒå¤šä¸ªæ•°æ®é›†åŠé…ç½®ï¼Œå¹¶åˆå¹¶è®­ç»ƒ/éªŒè¯é›†ï¼‰
    """
    hf_train_datasets = []
    hf_validation_datasets = []

    if not data_args.dataset_name:
        return None, None

    dataset_names = [name.strip() for name in data_args.dataset_name.split(',') if name.strip()]

    config_names = []
    if data_args.dataset_config_name:
        config_names = [c.strip() for c in data_args.dataset_config_name.split(',') if c.strip()]

    # å¯¹é½æ•°æ®é›†ä¸é…ç½®æ•°é‡
    if not config_names:
        config_names = [None] * len(dataset_names)
    elif len(config_names) < len(dataset_names):
        config_names.extend([config_names[-1]] * (len(dataset_names) - len(config_names)))
    elif len(config_names) > len(dataset_names):
        config_names = config_names[:len(dataset_names)]

    for i, dataset_name in enumerate(dataset_names):
        config_name = config_names[i]
        try:
            logger.info(f"Loading HF dataset {dataset_name} (config={config_name})")
            dataset = load_dataset(dataset_name, config_name, cache_dir=model_args.cache_dir)

            if "train" in dataset:
                hf_train_datasets.append(dataset["train"])
            if "validation" in dataset:
                hf_validation_datasets.append(dataset["validation"])
            elif "test" in dataset:
                hf_validation_datasets.append(dataset["test"])

        except Exception as e:
            logger.warning(f"Failed to load HF dataset {dataset_name}: {e}")

    # åˆå¹¶è®­ç»ƒé›†å’ŒéªŒè¯é›†
    hf_train_dataset = concatenate_datasets(hf_train_datasets) if hf_train_datasets else None
    hf_validation_dataset = concatenate_datasets(hf_validation_datasets) if hf_validation_datasets else None

    return hf_train_dataset, hf_validation_dataset


def load_local_datasets(data_args, model_args):
    """
    åŠ è½½æœ¬åœ° JSON/JSONL æ•°æ®é›†ï¼ˆé€’å½’ç›®å½•æœç´¢ï¼‰ï¼Œè¿”å› DatasetDict
    """
    data_files = {}

    # è®­ç»ƒæ–‡ä»¶
    if data_args.train_file_dir and os.path.exists(data_args.train_file_dir):
        train_files = glob(f"{data_args.train_file_dir}/**/*.json", recursive=True) + \
                      glob(f"{data_args.train_file_dir}/**/*.jsonl", recursive=True)
        if train_files:
            data_files["train"] = train_files

    # éªŒè¯æ–‡ä»¶
    if data_args.validation_file_dir and os.path.exists(data_args.validation_file_dir):
        val_files = glob(f"{data_args.validation_file_dir}/**/*.json", recursive=True) + \
                    glob(f"{data_args.validation_file_dir}/**/*.jsonl", recursive=True)
        if val_files:
            data_files["validation"] = val_files

    if not data_files:
        return {}

    try:
        local_datasets = load_dataset("json", data_files=data_files, cache_dir=model_args.cache_dir)
        return local_datasets
    except Exception as e:
        logger.warning(f"Failed to load local datasets: {e}")
        return {}


def load_datasets(data_args, model_args):
    """
    ä¸»å‡½æ•°ï¼šåˆ†åˆ«åŠ è½½ HF æ•°æ®é›†å’Œæœ¬åœ°æ•°æ®é›†ï¼Œç„¶åèåˆï¼Œå¿…è¦æ—¶ä»è®­ç»ƒé›†åˆ‡åˆ†éªŒè¯é›†
    """
    # åˆ†åˆ«åŠ è½½
    hf_train_dataset, hf_validation_dataset = load_hf_datasets(data_args, model_args)
    local_datasets = load_local_datasets(data_args, model_args)

    if not hf_train_dataset and not hf_validation_dataset and not local_datasets:
        raise ValueError("No valid datasets found from either HF Hub or local files.")

    # åˆå¹¶è®­ç»ƒé›†
    train_datasets = []
    if hf_train_dataset: train_datasets.append(hf_train_dataset)
    if "train" in local_datasets: train_datasets.append(local_datasets["train"])

    merged_datasets = {}
    if train_datasets:
        merged_datasets["train"] = train_datasets[0] if len(train_datasets) == 1 else concatenate_datasets(train_datasets)

    # åˆå¹¶éªŒè¯é›†
    val_datasets = []
    if hf_validation_dataset: val_datasets.append(hf_validation_dataset)
    if "validation" in local_datasets: val_datasets.append(local_datasets["validation"])

    if val_datasets:
        merged_datasets["validation"] = val_datasets[0] if len(val_datasets) == 1 else concatenate_datasets(val_datasets)

    # å¦‚æœæ²¡æœ‰éªŒè¯é›†ï¼Œä»è®­ç»ƒé›†åˆ‡åˆ†
    if "validation" not in merged_datasets:
        shuffled_train = merged_datasets["train"].shuffle(seed=42)
        split = shuffled_train.train_test_split(test_size=data_args.validation_split_percentage / 100, seed=42)
        merged_datasets["train"] = split["train"]
        merged_datasets["validation"] = split["test"]

    logger.info(f"Final datasets: train={len(merged_datasets['train'])}, validation={len(merged_datasets['validation'])}")
    return merged_datasets



def create_preprocess_function(tokenizer, prompt_template, script_args, IGNORE_INDEX):
    """
    æ„å»ºä¸€ä¸ªç”¨äº HuggingFace datasets.map çš„é¢„å¤„ç†å‡½æ•°

    å‚æ•°è¯´æ˜ï¼š
    - tokenizer: tokenizerï¼Œç”¨äºæŠŠæ–‡æœ¬è½¬æˆ token ids
    - prompt_template: å¯¹è¯æ¨¡æ¿ï¼Œè´Ÿè´£æŠŠå¤šè½®å¯¹è¯æ‹¼æˆæ¨¡å‹å¯ç”¨çš„ prompt
    - script_args: è®­ç»ƒå‚æ•°ï¼ˆå¦‚ max_lengthã€æ˜¯å¦å¯¹ input è®¡ç®— lossï¼‰
    - IGNORE_INDEX: label ä¸­è¢«å¿½ç•¥ä½ç½®çš„å¡«å……å€¼ï¼ˆé€šå¸¸æ˜¯ -100ï¼‰
    """
    max_length = script_args.model_max_length

    def preprocess_function(examples):
        """
        å¯¹ä¸€ä¸ª batch çš„æ ·æœ¬è¿›è¡Œé¢„å¤„ç†
        examples é€šå¸¸æ˜¯ datasets ä¼ å…¥çš„ä¸€ä¸ª dictï¼Œkey -> list
        """
        input_ids_list = []
        attention_mask_list = []
        targets_list = []

        # åªæ”¯æŒ human -> gpt çš„å¯¹è¯é¡ºåº
        roles = ["human", "gpt"]

        def get_dialog(examples):
            """
            ä»åŸå§‹ examples ä¸­è§£æå‡ºæ ‡å‡†åŒ–çš„å¯¹è¯æ ¼å¼
            æœ€ç»ˆ yield çš„æ˜¯ï¼šprompt_template å¤„ç†åçš„ dialogï¼ˆlist[str]ï¼‰
            """
            # system_prompt å¯èƒ½æ˜¯ä¸€ä¸ª listï¼ˆbatch çº§åˆ«ï¼‰
            system_prompts = examples.get("system_prompt", "")

            for i, source in enumerate(examples['conversations']):
                system_prompt = ""

                # è‡³å°‘è¦æœ‰ä¸€é—®ä¸€ç­”
                if len(source) < 2:
                    continue

                data_role = source[0].get("from", "")

                # å¦‚æœç¬¬ä¸€æ¡æ˜¯ system æ¶ˆæ¯ï¼Œå…ˆå–å‡ºæ¥
                if data_role == "system":
                    system_prompt = source[0]["value"]
                    source = source[1:]
                    data_role = source[0].get("from", "")

                # å¦‚æœç¬¬ä¸€æ¡ä¸æ˜¯ humanï¼Œè·³è¿‡ä¸€æ¡
                # ä¿è¯ä» human å¼€å§‹
                if data_role not in roles or data_role != roles[0]:
                    source = source[1:]

                if len(source) < 2:
                    continue

                messages = []

                # æŒ‰ human / gpt äº¤æ›¿æ”¶é›†æ¶ˆæ¯
                for j, sentence in enumerate(source):
                    data_role = sentence.get("from", "")
                    if data_role not in roles:
                        logger.warning(f"unknown role: {data_role}, {i}. (ignored)")
                        break

                    # roles[j % 2] ç¡®ä¿è§’è‰²é¡ºåºæ­£ç¡®
                    if data_role == roles[j % 2]:
                        messages.append(sentence["value"])

                # å¿…é¡»æ˜¯å¶æ•°æ¡ï¼ˆhuman, gpt æˆå¯¹ï¼‰
                if len(messages) % 2 != 0:
                    continue

                # è½¬æˆ [[human, gpt], [human, gpt], ...] çš„å½¢å¼
                history_messages = [
                    [messages[k], messages[k + 1]]
                    for k in range(0, len(messages), 2)
                ]

                # å¦‚æœå½“å‰å¯¹è¯æ²¡ system_promptï¼Œç”¨ batch çº§çš„
                if not system_prompt:
                    system_prompt = system_prompts[i] if system_prompts else ""

                # é€šè¿‡æ¨¡æ¿ç”Ÿæˆæœ€ç»ˆ dialog
                yield prompt_template.get_dialog(
                    history_messages,
                    system_prompt=system_prompt
                )

        # å¯¹æ¯ä¸€ä¸ªæ ‡å‡†åŒ–åçš„ dialog è¿›è¡Œ token åŒ–
        for dialog in get_dialog(examples):
            input_ids = []
            labels = []

            # dialog ç»“æ„é€šå¸¸æ˜¯ï¼š
            # [prompt_0, answer_0, prompt_1, answer_1, ...]
            for i in range(len(dialog) // 2):
                # prompt éƒ¨åˆ†ï¼ˆhumanï¼‰
                source_ids = tokenizer.encode(
                    text=dialog[2 * i],
                    add_special_tokens=(i == 0)  # åªåœ¨ç¬¬ä¸€è½®åŠ  BOS ç­‰
                )

                # answer éƒ¨åˆ†ï¼ˆgptï¼‰
                target_ids = tokenizer.encode(
                    text=dialog[2 * i + 1],
                    add_special_tokens=False
                )

                # æŒ‰ source / target æ¯”ä¾‹åŠ¨æ€åˆ†é… max_length
                total_len = len(source_ids) + len(target_ids)
                max_source_len = int(max_length * (len(source_ids) / total_len))
                max_target_len = int(max_length * (len(target_ids) / total_len))

                # æˆªæ–­ source
                if len(source_ids) > max_source_len:
                    source_ids = source_ids[:max_source_len]

                # æˆªæ–­ targetï¼Œé¢„ç•™ eos
                if len(target_ids) > max_target_len - 1:
                    target_ids = target_ids[:max_target_len - 1]

                # é¿å… source ä»¥ eos å¼€å¤´
                if len(source_ids) > 0 and source_ids[0] == tokenizer.eos_token_id:
                    source_ids = source_ids[1:]

                # é¿å… target ä»¥ eos ç»“å°¾ï¼ˆåé¢ä¼šæ‰‹åŠ¨åŠ ï¼‰
                if len(target_ids) > 0 and target_ids[-1] == tokenizer.eos_token_id:
                    target_ids = target_ids[:-1]

                # å¦‚æœå†åŠ ä¸€è½®å°±è¶…é•¿ï¼Œç›´æ¥åœæ­¢
                if len(input_ids) + len(source_ids) + len(target_ids) + 1 > max_length:
                    break

                # æ‹¼æ¥ input_ids
                input_ids += source_ids + target_ids + [tokenizer.eos_token_id]

                # æ„å»º labels
                if script_args.train_on_inputs:
                    # å¯¹ prompt + answer éƒ½è®¡ç®— loss
                    labels += source_ids + target_ids + [tokenizer.eos_token_id]
                else:
                    # prompt éƒ¨åˆ†ç”¨ IGNORE_INDEX mask æ‰
                    labels += (
                            [IGNORE_INDEX] * len(source_ids)
                            + target_ids
                            + [tokenizer.eos_token_id]
                    )

            input_ids_list.append(input_ids)
            attention_mask_list.append([1] * len(input_ids))
            targets_list.append(labels)

        return dict(
            input_ids=input_ids_list,
            attention_mask=attention_mask_list,
            labels=targets_list,
        )

    return preprocess_function


def filter_empty_labels(example, IGNORE_INDEX):
    """Remove empty labels dataset."""
    return not all(label == IGNORE_INDEX for label in example["labels"])


def check_and_optimize_memory():
    """æ£€æŸ¥å¹¶ä¼˜åŒ–GPUå†…å­˜ä½¿ç”¨"""
    if not torch.cuda.is_available():
        return

    logger.info("ğŸ” æ£€æŸ¥GPUå†…å­˜çŠ¶æ€...")

    # æ¸…ç†ç¼“å­˜
    torch.cuda.empty_cache()

    # æ£€æŸ¥æ¯ä¸ªGPUçš„å†…å­˜çŠ¶æ€
    num_gpus = torch.cuda.device_count()
    for i in range(num_gpus):
        props = torch.cuda.get_device_properties(i)
        total_memory = props.total_memory / 1024 ** 3
        allocated = torch.cuda.memory_allocated(i) / 1024 ** 3
        cached = torch.cuda.memory_reserved(i) / 1024 ** 3
        free = total_memory - allocated - cached

        logger.info(f"GPU {i} ({props.name}):")
        logger.info(f"  æ€»å†…å­˜: {total_memory:.1f}GB")
        logger.info(f"  å·²åˆ†é…: {allocated:.1f}GB")
        logger.info(f"  å·²ç¼“å­˜: {cached:.1f}GB")
        logger.info(f"  å¯ç”¨: {free:.1f}GB")

        if free < 2.0:  # å¦‚æœå¯ç”¨å†…å­˜å°‘äº2GB
            logger.warning(f"âš ï¸ GPU {i} å¯ç”¨å†…å­˜ä¸è¶³ ({free:.1f}GB)ï¼Œå»ºè®®:")
            logger.warning("  1. ä½¿ç”¨ --load_in_4bit å¯ç”¨4bité‡åŒ–")
            logger.warning("  2. å‡å° --per_device_train_batch_size")
            logger.warning("  3. å¢åŠ  --gradient_accumulation_steps")
            logger.warning("  4. å‡å° --model_max_length")

    # è®¾ç½®å†…å­˜ä¼˜åŒ–é€‰é¡¹
    if hasattr(torch.backends.cuda, 'enable_flash_sdp'):
        torch.backends.cuda.enable_flash_sdp(True)
        logger.info("âœ… å¯ç”¨Flash Attentionä¼˜åŒ–")

    # å¯ç”¨å†…å­˜é«˜æ•ˆçš„æ³¨æ„åŠ›æœºåˆ¶
    if hasattr(torch.backends.cuda, 'enable_mem_efficient_sdp'):
        torch.backends.cuda.enable_mem_efficient_sdp(True)
        logger.info("âœ… å¯ç”¨å†…å­˜é«˜æ•ˆæ³¨æ„åŠ›æœºåˆ¶")


def get_unwrapped_model(model):
    """è·å–æœªåŒ…è£…çš„åŸå§‹æ¨¡å‹ï¼Œæ— è®ºå®ƒæ˜¯å¦è¢«DDPåŒ…è£…"""
    if hasattr(model, "module"):
        return model.module
    return model


def parse_arguments():
    """è§£æå‘½ä»¤è¡Œå‚æ•°å’Œé…ç½®æ–‡ä»¶"""
    os.environ.setdefault("PYTORCH_CUDA_ALLOC_CONF", "expandable_segments:True")
    parser = HfArgumentParser((ModelArguments, DataArguments, Seq2SeqTrainingArguments, ScriptArguments))

    if len(sys.argv) == 2 and sys.argv[1].endswith(".json"):
        model_args, data_args, training_args, script_args = parser.parse_json_file(
            json_file=os.path.abspath(sys.argv[1]))
    else:
        model_args, data_args, training_args, script_args = parser.parse_args_into_dataclasses(look_for_args_file=False)

    return model_args, data_args, training_args, script_args


def setup_accelerator():
    """åˆå§‹åŒ–Acceleratorå¹¶è®¾ç½®æ—¥å¿—"""
    logger.info(f"ğŸš€ ä½¿ç”¨Accelerateåº“è¿›è¡Œå¤šGPUè®­ç»ƒ")
    logger.info("ğŸš€ å¼€å§‹åˆå§‹åŒ–Accelerator...")
    accelerator = Accelerator()
    logger.info("âœ… Acceleratoråˆå§‹åŒ–å®Œæˆ")

    try:
        logger.info(f"è®¾å¤‡: {accelerator.device}")
        logger.info(f"æ£€æµ‹åˆ° {accelerator.num_processes} ä¸ªè¿›ç¨‹")
        logger.info(f"å½“å‰è¿›ç¨‹: {accelerator.process_index}")
        logger.info(f"åˆ†å¸ƒå¼ç±»å‹: {accelerator.distributed_type}")
    except:
        logger.warning("æ— æ³•è·å–å®Œæ•´çš„Acceleratorä¿¡æ¯ï¼Œä½†è¿™ä¸å½±å“è®­ç»ƒ")

    return accelerator


def setup_tokenizer(model_args, script_args):
    """
    é…ç½®å’ŒåŠ è½½tokenizer
    
    Args:
        model_args: æ¨¡å‹ç›¸å…³å‚æ•°ï¼ŒåŒ…å«æ¨¡å‹è·¯å¾„ã€ç¼“å­˜ç›®å½•ç­‰é…ç½®
        script_args: è„šæœ¬å‚æ•°ï¼ŒåŒ…å«æ¨¡æ¿åç§°ç­‰é…ç½®
        
    Returns:
        tuple: (tokenizer, prompt_template) - é…ç½®å¥½çš„tokenizerå’Œå¯¹è¯æ¨¡æ¿
    """
    # æ„å»ºtokenizerçš„åˆå§‹åŒ–å‚æ•°
    tokenizer_kwargs = {
        "cache_dir": model_args.cache_dir,  # ç¼“å­˜ç›®å½•ï¼Œç”¨äºå­˜å‚¨ä¸‹è½½çš„æ¨¡å‹æ–‡ä»¶
        "use_fast": model_args.use_fast_tokenizer,  # æ˜¯å¦ä½¿ç”¨fast tokenizerï¼ˆRustå®ç°ï¼Œé€Ÿåº¦æ›´å¿«ï¼‰
        "trust_remote_code": model_args.trust_remote_code,  # æ˜¯å¦ä¿¡ä»»è¿œç¨‹ä»£ç ï¼ˆå›½äº§æ¨¡å‹é€šå¸¸éœ€è¦ï¼‰
    }

    # ç¡®å®štokenizerçš„è·¯å¾„ï¼šä¼˜å…ˆä½¿ç”¨æŒ‡å®šçš„tokenizerè·¯å¾„ï¼Œå¦åˆ™ä½¿ç”¨æ¨¡å‹è·¯å¾„
    tokenizer_name_or_path = model_args.tokenizer_name_or_path or model_args.model_name_or_path

    # ä»é¢„è®­ç»ƒæ¨¡å‹åŠ è½½tokenizer
    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name_or_path, **tokenizer_kwargs)

    # è·å–å¯¹è¯æ¨¡æ¿ï¼Œç”¨äºå®šä¹‰å¯¹è¯æ ¼å¼å’Œåœæ­¢ç¬¦
    prompt_template = get_conv_template(script_args.template_name)

    # é…ç½®ç‰¹æ®Štokenï¼šeos_tokenï¼ˆç»“æŸç¬¦ï¼‰
    # å¦‚æœtokenizeræ²¡æœ‰è®¾ç½®ç»“æŸç¬¦ï¼Œä½¿ç”¨å¯¹è¯æ¨¡æ¿çš„åœæ­¢å­—ç¬¦ä¸²ä½œä¸ºç»“æŸç¬¦
    if tokenizer.eos_token_id is None:
        tokenizer.eos_token = prompt_template.stop_str
        # tokenizer.eos_token_id ä¼šè¢«è‡ªåŠ¨è®¾ç½®ä¸ºæ–°å¢ token çš„ ID
        tokenizer.add_special_tokens({"eos_token": tokenizer.eos_token})
        logger.info(f"Add eos_token: {tokenizer.eos_token}")

    '''
    # å‡è®¾ eos_token = "<|im_end|>"ï¼Œå…¶ token_id = 151643
    tokenizer.eos_token = "<|im_end|>"
    tokenizer.eos_token_id = 151643
    
    # å°† bos_token è®¾ç½®ä¸ºç›¸åŒçš„å€¼
    tokenizer.bos_token = "<|im_end|>"  # ä¸ eos_token ç›¸åŒ
    tokenizer.bos_token_id = 151643     # æ˜¾å¼åŒæ­¥ ID
    
    å°† bos_token è®¾ç½®ä¸ºä¸ eos_token ç›¸åŒä¸»è¦æ˜¯åŸºäºä»¥ä¸‹å‡ ä¸ªå®ç”¨æ€§çš„è€ƒè™‘ï¼š

    1. ç®€åŒ–æ¨¡å‹è®­ç»ƒ
        å¯¹äºå¯¹è¯å¼è¯­è¨€æ¨¡å‹ï¼Œå¼€å§‹å’Œç»“æŸè¾¹ç•Œçš„é‡è¦æ€§ç›¸å¯¹è¾ƒä½ï¼š
            æ¨¡å‹ä¸»è¦é€šè¿‡ä¸Šä¸‹æ–‡ç†è§£å¯¹è¯çš„å¼€å§‹å’Œç»“æŸ
            è€Œä¸æ˜¯ä¾èµ–ç‰¹æ®Šçš„å¼€å§‹/ç»“æŸæ ‡è®°
            è¿™æ ·å¯ä»¥å‡å°‘æ¨¡å‹éœ€è¦å­¦ä¹ çš„ç‰¹æ®Š token æ•°é‡
    2. é¿å…ç©ºå€¼å¯¼è‡´çš„é”™è¯¯
        å¾ˆå¤š tokenizerï¼ˆç‰¹åˆ«æ˜¯è‡ªå®šä¹‰çš„å¯¹è¯æ¨¡å‹ tokenizerï¼‰å¯èƒ½æ²¡æœ‰æ˜ç¡®å®šä¹‰ bos_token ï¼š
    3. å®é™…è®­ç»ƒä¸­çš„è€ƒè™‘
        åœ¨ SFTï¼ˆSupervised Fine-Tuningï¼‰è®­ç»ƒä¸­ï¼š
            è¾“å…¥æ•°æ®å·²ç»æœ‰æ˜ç¡®çš„æ ¼å¼ï¼ˆå¦‚æ¨¡æ¿åŒ–çš„å¯¹è¯ï¼‰
            æ¨¡å‹å­¦ä¹ çš„æ˜¯å¯¹è¯æ¨¡å¼ï¼Œè€Œä¸æ˜¯ä¾èµ–è¾¹ç•Œ token
            ç›¸åŒçš„å¼€å§‹å’Œç»“æŸ token ä¸ä¼šå½±å“æ¨¡å‹æ€§èƒ½
    '''
    # é…ç½®ç‰¹æ®Štokenï¼šbos_tokenï¼ˆå¼€å§‹ç¬¦ï¼‰
    # å¦‚æœæ²¡æœ‰å¼€å§‹ç¬¦ï¼Œä½¿ç”¨ç»“æŸç¬¦ä½œä¸ºå¼€å§‹ç¬¦ï¼ˆå¾ˆå¤šæ¨¡å‹è¿™æ ·åšï¼‰
    if tokenizer.bos_token_id is None:
        tokenizer.add_special_tokens({"bos_token": tokenizer.eos_token})
        tokenizer.bos_token_id = tokenizer.eos_token_id
        logger.info(f"Add bos_token: {tokenizer.bos_token}")

    # é…ç½®ç‰¹æ®Štokenï¼špad_tokenï¼ˆå¡«å……ç¬¦ï¼‰
    # ç”¨äºæ‰¹å¤„ç†æ—¶å°†ä¸åŒé•¿åº¦çš„åºåˆ—å¡«å……åˆ°ç›¸åŒé•¿åº¦
    if tokenizer.pad_token_id is None:
        if tokenizer.unk_token_id is not None:
            # ä¼˜å…ˆä½¿ç”¨æœªçŸ¥ç¬¦ä½œä¸ºå¡«å……ç¬¦
            tokenizer.pad_token = tokenizer.unk_token
        else:
            # å¦‚æœæ²¡æœ‰æœªçŸ¥ç¬¦ï¼Œä½¿ç”¨ç»“æŸç¬¦ä½œä¸ºå¡«å……ç¬¦
            tokenizer.pad_token = tokenizer.eos_token
        logger.info(f"Add pad_token: {tokenizer.pad_token}")

    logger.info("âœ… Tokenizeré…ç½®å®Œæˆ")
    return tokenizer, prompt_template


def estimate_model_size(model_args, config):
    """ä¼°ç®—æ¨¡å‹å¤§å°ï¼ˆGBï¼‰"""
    if hasattr(config, 'num_parameters'):
        return config.num_parameters * 2 / 1024 ** 3  # å‡è®¾fp16
    else:
        model_name_lower = model_args.model_name_or_path.lower()
        if '70b' in model_name_lower or '72b' in model_name_lower:
            return 140  # 70Bæ¨¡å‹å¤§çº¦140GB
        elif '32b' in model_name_lower or '34b' in model_name_lower:
            return 64  # 32Bæ¨¡å‹å¤§çº¦64GB
        elif '13b' in model_name_lower or '14b' in model_name_lower:
            return 26  # 13Bæ¨¡å‹å¤§çº¦26GB
        elif '7b' in model_name_lower or '8b' in model_name_lower:
            return 14  # 7Bæ¨¡å‹å¤§çº¦14GB
        elif '3b' in model_name_lower:
            return 6  # 3Bæ¨¡å‹å¤§çº¦6GB
        else:
            return 10  # é»˜è®¤ä¼°ç®—


def load_and_configure_model(model_args, script_args, accelerator):
    """
    åŠ è½½å¹¶é…ç½®å¤§æ¨¡å‹çš„ç»Ÿä¸€å…¥å£å‡½æ•°

    æ ¸å¿ƒç›®æ ‡ï¼š
    1. æ ¹æ®ç”¨æˆ·é…ç½®å†³å®šæ˜¯å¦ä½¿ç”¨ 4bit / 8bit é‡åŒ–
    2. æ ¹æ® GPU æ•°é‡ã€æ˜¾å­˜è§„æ¨¡ã€æ¨¡å‹å¤§å°ï¼Œè‡ªåŠ¨é€‰æ‹©ï¼š
       - Tensor Parallelï¼ˆæƒé‡åˆ‡åˆ†ï¼‰
       - æˆ– DDPï¼ˆæ•°æ®å¹¶è¡Œï¼‰
    3. å¯é€‰å¯ç”¨ FlashAttention-2
    4. åœ¨ Tensor Parallel å¤±è´¥æ—¶è‡ªåŠ¨å›é€€åˆ° DDP
    """

    logger.info("ğŸ”„ å¼€å§‹åŠ è½½æ¨¡å‹...")

    # ============================================================
    # 1. ç¡®å®šæ¨¡å‹è®¡ç®—ç²¾åº¦ï¼ˆfp16 / bf16 / fp32ï¼‰
    # ============================================================
    # dtype ä¼šå½±å“ï¼š
    # - æ¨¡å‹å‚æ•° dtype
    # - attention / matmul çš„è®¡ç®—ç²¾åº¦
    # - æ˜¾å­˜å ç”¨
    dtype = model_args.dtype

    # ============================================================
    # 2. æ„å»ºé‡åŒ–é…ç½®ï¼ˆBitsAndBytesï¼‰
    # ============================================================
    # é»˜è®¤ä¸ä½¿ç”¨é‡åŒ–
    quantization_config = None

    # ---- 4bit é‡åŒ–ï¼ˆQLoRA æ¨èé…ç½®ï¼‰----
    if model_args.load_in_4bit:
        quantization_config = BitsAndBytesConfig(
            load_in_4bit=True,  # å¯ç”¨ 4bit æƒé‡é‡åŒ–
            bnb_4bit_compute_dtype=dtype,  # å®é™…è®¡ç®—ä»ä½¿ç”¨ fp16 / bf16
            bnb_4bit_use_double_quant=True,  # å¯¹é‡åŒ–å‚æ•°å†æ¬¡é‡åŒ–ï¼Œè¿›ä¸€æ­¥çœæ˜¾å­˜
            bnb_4bit_quant_type="nf4"  # NF4ï¼šQLoRA è®ºæ–‡æ¨èçš„é‡åŒ–æ–¹å¼
        )

    # ---- 8bit é‡åŒ– ----
    elif model_args.load_in_8bit:
        quantization_config = BitsAndBytesConfig(
            load_in_8bit=True
        )

    # ============================================================
    # 3. æ„å»ºæ¨¡å‹é…ç½®åŠ è½½å‚æ•°ï¼ˆä¸åŠ è½½æƒé‡ï¼‰
    # ============================================================
    config_kwargs = {
        "trust_remote_code": model_args.trust_remote_code,  # æ˜¯å¦ä¿¡ä»»æ¨¡å‹ä»“åº“ä¸­çš„è‡ªå®šä¹‰ä»£ç 
        "cache_dir": model_args.cache_dir,  # æœ¬åœ°ç¼“å­˜ç›®å½•
        "revision": model_args.model_revision,  # æ¨¡å‹ç‰ˆæœ¬ï¼ˆcommit / tagï¼‰
        "hf_hub_token": model_args.hf_hub_token,  # HuggingFace ç§æœ‰æ¨¡å‹ token
    }

    # ---- FlashAttention-2 å¼€å…³ ----
    # FlashAttention å¿…é¡»åœ¨ config é˜¶æ®µå¯ç”¨
    if model_args.flash_attn:
        if is_flash_attn_2_available:
            config_kwargs["use_flash_attention_2"] = True
            logger.info("Using FlashAttention-2 for faster training and inference.")
        else:
            logger.warning("FlashAttention-2 is not installed.")

    # ============================================================
    # 4. åŠ è½½æ¨¡å‹é…ç½®ï¼ˆä»… configï¼Œä¸å  GPU æ˜¾å­˜ï¼‰
    # ============================================================
    # è¿™é‡Œä¸åŠ è½½æƒé‡ï¼Œä¸»è¦ç”¨äºï¼š
    # - è¯»å–æ¨¡å‹ç»“æ„ä¿¡æ¯
    # - åç»­ä¼°ç®—æ¨¡å‹å¤§å°
    config = AutoConfig.from_pretrained(
        model_args.model_name_or_path,
        **config_kwargs
    )

    # ============================================================
    # 5. GPU ç¯å¢ƒæ£€æµ‹ä¸æ˜¾å­˜ä¿¡æ¯æ‰“å°
    # ============================================================
    total_memory = 0
    if torch.cuda.is_available():
        num_gpus = torch.cuda.device_count()
        logger.info(f"æ£€æµ‹åˆ° {num_gpus} ä¸ªGPU")

        # éå†æ‰€æœ‰ GPUï¼Œæ‰“å°æ˜¾å­˜ä½¿ç”¨æƒ…å†µ
        for i in range(num_gpus):
            gpu_memory = torch.cuda.get_device_properties(i).total_memory / 1024 ** 3
            allocated = torch.cuda.memory_allocated(i) / 1024 ** 3
            cached = torch.cuda.memory_reserved(i) / 1024 ** 3
            free = gpu_memory - allocated

            total_memory += gpu_memory

            logger.info(
                f"GPU {i}: æ€»å†…å­˜={gpu_memory:.1f}GB, "
                f"å·²åˆ†é…={allocated:.1f}GB, "
                f"ç¼“å­˜={cached:.1f}GB, "
                f"å¯ç”¨={free:.1f}GB"
            )

        logger.info(f"æ€»GPUå†…å­˜: {total_memory:.1f}GB")

        # æ¸…ç† CUDA cacheï¼Œé¿å…å†å²ç¢ç‰‡å½±å“å¤§æ¨¡å‹åŠ è½½
        torch.cuda.empty_cache()
        logger.info("å·²æ¸…ç†GPUç¼“å­˜")

    # ============================================================
    # 6. ä¼°ç®—æ¨¡å‹å¤§å°ï¼ˆç”¨äºå¹¶è¡Œç­–ç•¥å†³ç­–ï¼‰
    # ============================================================
    # è¯¥å‡½æ•°é€šå¸¸åŸºäºï¼š
    # - hidden_size
    # - num_layers
    # - vocab_size
    # - dtype / é‡åŒ–æ–¹å¼
    estimated_model_size_gb = estimate_model_size(model_args, config)
    logger.info(f"ä¼°ç®—æ¨¡å‹å¤§å°: {estimated_model_size_gb:.1f}GB")

    # ============================================================
    # 7. å†³å®šå¹¶è¡Œç­–ç•¥ï¼ˆDDP vs Tensor Parallelï¼‰
    # ============================================================
    num_gpus = torch.cuda.device_count()

    # æ˜¯å¦æ˜¯å¤šè¿›ç¨‹ï¼ˆAccelerate / torchrunï¼‰
    is_distributed = accelerator.num_processes > 1

    # é»˜è®¤å…è®¸ä½¿ç”¨ Tensor Parallel
    use_tensor_parallel = True

    if is_distributed:
        # ---- å¤š GPU åœºæ™¯ ----
        if script_args.use_tensor_parallel and estimated_model_size_gb > 20:
            logger.info(
                f"ğŸ”§ ä½¿ç”¨å¼ é‡å¹¶è¡Œç­–ç•¥ (æ¨¡å‹å¤§å°: {estimated_model_size_gb:.1f}GB)"
            )

            # Tensor Parallel å¯¹ PyTorch ç‰ˆæœ¬æœ‰è¦æ±‚
            import pkg_resources
            torch_version = pkg_resources.get_distribution("torch").version

            if pkg_resources.parse_version(torch_version) < pkg_resources.parse_version("2.5.0"):
                logger.warning(
                    f"âš ï¸ å½“å‰PyTorchç‰ˆæœ¬ {torch_version} ä¸æ”¯æŒå¼ é‡å¹¶è¡Œï¼Œéœ€è¦ >= 2.5.0"
                )
                logger.warning("âš ï¸ è‡ªåŠ¨åˆ‡æ¢åˆ°DDPæ¨¡å¼")
                use_tensor_parallel = False
            else:
                logger.info(f"âœ… PyTorchç‰ˆæœ¬ {torch_version} æ”¯æŒå¼ é‡å¹¶è¡Œ")

        else:
            # æ¨¡å‹è¾ƒå° or ç”¨æˆ·æœªå¯ç”¨ Tensor Parallel
            logger.info(
                f"ğŸ”§ ä½¿ç”¨DDPè¿›è¡Œå¤šGPUè®­ç»ƒ (æ¨¡å‹å¤§å°: {estimated_model_size_gb:.1f}GB)"
            )
            use_tensor_parallel = False

    else:
        # ---- å•è¿›ç¨‹ï¼ˆå•å¡ / å•æœºï¼‰ ----
        logger.info("ğŸ”§ å•è¿›ç¨‹è®­ç»ƒ")

    # ============================================================
    # 8. æ„å»ºæ¨¡å‹åŠ è½½å‚æ•°
    # ============================================================
    model_kwargs = {
        "config": config,  # æ¨¡å‹ç»“æ„é…ç½®
        "dtype": dtype,  # è®¡ç®—ç²¾åº¦
        "trust_remote_code": model_args.trust_remote_code,
        "quantization_config": quantization_config,  # é‡åŒ–é…ç½®ï¼ˆå¯èƒ½ä¸º Noneï¼‰
        "low_cpu_mem_usage": True,  # å‡å°‘ CPU å†…å­˜å³°å€¼
    }

    # ============================================================
    # 9. Tensor Parallel åœºæ™¯ä¸‹çš„ device_map / max_memory
    # ============================================================
    if use_tensor_parallel:
        # device_map="auto" è®© HF è‡ªåŠ¨åˆ‡åˆ†æ¨¡å‹æƒé‡åˆ°å¤šå¼  GPU
        model_kwargs["device_map"] = "auto"

        if num_gpus > 1:
            max_memory = {}

            # ä¸ºæ¯å¼  GPU è®¾ç½®æœ€å¤§å¯ç”¨æ˜¾å­˜ï¼ˆé¢„ç•™ 20% ç»™ runtime / activationï¼‰
            for i in range(num_gpus):
                gpu_props = torch.cuda.get_device_properties(i)
                total_mem = gpu_props.total_memory
                usable_mem = int(total_mem * 0.8)

                max_memory[i] = f"{usable_mem // (1024 ** 3)}GiB"

            model_kwargs["max_memory"] = max_memory
            logger.info(
                f"ğŸ”§ å¼ é‡å¹¶è¡Œé…ç½®: device_map=auto, max_memory={max_memory}"
            )
    else:
        logger.info("ğŸ”§ DDPé…ç½®: ä¸ä½¿ç”¨device_map")

    # ============================================================
    # 10. å®é™…åŠ è½½æ¨¡å‹ï¼ˆå¸¦ Tensor Parallel å¤±è´¥å…œåº•ï¼‰
    # ============================================================
    try:
        model = AutoModelForCausalLM.from_pretrained(
            model_args.model_name_or_path,
            **model_kwargs
        )
        logger.info("âœ… æ¨¡å‹åŠ è½½å®Œæˆ")

    except OSError as e:
        # Tensor Parallel åœ¨æŸäº›æ¨¡å‹ / PyTorch ç»„åˆä¸‹ä¼šç›´æ¥æŠ¥é”™
        if "tensor parallel is only supported for" in str(e):
            logger.error(f"âŒ å¼ é‡å¹¶è¡ŒåŠ è½½å¤±è´¥: {e}")
            logger.info("ğŸ”„ å°è¯•ä½¿ç”¨DDPæ¨¡å¼é‡æ–°åŠ è½½...")

            # ç§»é™¤ Tensor Parallel ç›¸å…³å‚æ•°
            model_kwargs.pop("device_map", None)
            model_kwargs.pop("max_memory", None)

            model = AutoModelForCausalLM.from_pretrained(
                model_args.model_name_or_path,
                **model_kwargs
            )

            logger.info("âœ… ä½¿ç”¨DDPæ¨¡å¼åŠ è½½æ¨¡å‹æˆåŠŸ")
            use_tensor_parallel = False
        else:
            # å…¶ä»–é”™è¯¯ç›´æ¥æŠ›å‡º
            raise

    # ============================================================
    # 11. æ‰“å°æ¨¡å‹ç»“æ„ / åˆ†å¸ƒä¿¡æ¯ï¼ˆç”¨äº sanity checkï¼‰
    # ============================================================
    display_model_info(model)

    # ============================================================
    # 12. è¿”å›æ¨¡å‹åŠå¹¶è¡Œç­–ç•¥
    # ============================================================
    return model, use_tensor_parallel


def display_model_info(model):
    """æ˜¾ç¤ºæ¨¡å‹åˆ†å¸ƒå’ŒGPUå†…å­˜ä¿¡æ¯"""
    logger.info("ğŸ“Š æ¨¡å‹åˆ†å¸ƒæƒ…å†µ:")
    if hasattr(model, 'hf_device_map') and model.hf_device_map:
        logger.info("ğŸ”§ ä½¿ç”¨HuggingFaceè®¾å¤‡æ˜ å°„:")
        for module_name, device in model.hf_device_map.items():
            logger.info(f"  {module_name}: {device}")

        device_count = {}
        for device in model.hf_device_map.values():
            device_str = str(device)
            device_count[device_str] = device_count.get(device_str, 0) + 1

        logger.info("ğŸ“ˆ è®¾å¤‡ä½¿ç”¨ç»Ÿè®¡:")
        for device, count in device_count.items():
            logger.info(f"  {device}: {count} ä¸ªæ¨¡å—")
    else:
        device_params = {}
        total_params = 0
        for name, param in model.named_parameters():
            device = str(param.device)
            if device not in device_params:
                device_params[device] = {'count': 0, 'size': 0}
            device_params[device]['count'] += 1
            device_params[device]['size'] += param.numel()
            total_params += param.numel()

        logger.info("ğŸ“ˆ å‚æ•°è®¾å¤‡åˆ†å¸ƒ:")
        if total_params > 0:
            for device, info in device_params.items():
                param_size_gb = info['size'] * 4 / 1024 ** 3
                percentage = info['size'] / total_params * 100
                logger.info(f"  {device}: {info['count']} ä¸ªå‚æ•°ç»„, {param_size_gb:.2f}GB ({percentage:.1f}%)")
        else:
            logger.info("  æœªæ£€æµ‹åˆ°æ¨¡å‹å‚æ•°ï¼ˆå¯èƒ½ä½¿ç”¨äº†DeepSpeed ZeROç­‰ä¼˜åŒ–æŠ€æœ¯ï¼‰")

    if torch.cuda.is_available():
        logger.info("ğŸ’¾ GPUå†…å­˜ä½¿ç”¨æƒ…å†µ:")
        for i in range(torch.cuda.device_count()):
            allocated = torch.cuda.memory_allocated(i) / 1024 ** 3
            cached = torch.cuda.memory_reserved(i) / 1024 ** 3
            total = torch.cuda.get_device_properties(i).total_memory / 1024 ** 3
            logger.info(f"  GPU {i}: å·²åˆ†é…={allocated:.1f}GB, ç¼“å­˜={cached:.1f}GB, æ€»è®¡={total:.1f}GB")


def setup_peft(model, model_args, script_args, training_args):
    """
    é…ç½® PEFT / LoRA è®­ç»ƒé€»è¾‘
    - æ”¯æŒï¼š
        1ï¼‰ä»å·²æœ‰ LoRA æƒé‡ç»§ç»­è®­ç»ƒ
        2ï¼‰æ–°å»º LoRA
        3ï¼‰4bit / 8bit é‡åŒ– + LoRAï¼ˆQLoRAï¼‰
        4ï¼‰ä¸å¼€ PEFTï¼Œèµ°å…¨å‚æ•°å¾®è°ƒ
    """
    if script_args.use_peft:
        logger.info("ğŸ”§ é…ç½®LoRA")

        # ===== 1. æ˜¯å¦ä»å·²æœ‰ LoRA æƒé‡åŠ è½½ =====
        # å¸¸è§åœºæ™¯ï¼šäºŒæ¬¡å¾®è°ƒ / ç»§ç»­è®­ç»ƒ
        if script_args.peft_path is not None:
            model = PeftModel.from_pretrained(
                model,
                script_args.peft_path,
                is_trainable=True  # å…³é”®ï¼šç¡®ä¿ LoRA å‚æ•°å¯è®­ç»ƒ
            )
        else:
            # ===== 2. æ–°å»º LoRA =====

            # 2.1 å¦‚æœæ˜¯ 4bit / 8bit é‡åŒ–æ¨¡å‹
            # å¿…é¡»åšé¢å¤–å¤„ç†ï¼š
            # - å†»ç»“ base model
            # - å¤„ç† LayerNorm / embedding çš„ dtype
            # - é…åˆ gradient checkpointing
            if model_args.load_in_8bit or model_args.load_in_4bit:
                model = prepare_model_for_kbit_training(
                    model,
                    training_args.gradient_checkpointing
                )

            # 2.2 è§£æ LoRA ä½œç”¨çš„ç›®æ ‡æ¨¡å—
            # e.g. "q_proj,k_proj,v_proj"
            target_modules = (
                script_args.target_modules.split(',')
                if script_args.target_modules
                else None
            )

            # ç‰¹æ®Šå€¼ï¼šall
            # è‡ªåŠ¨æ‰«ææ¨¡å‹ä¸­æ‰€æœ‰ Linear å±‚
            # å¸¸è§äº QLoRA / ä¸æƒ³æ‰‹å†™æ¨¡å—å
            if target_modules and 'all' in target_modules:
                target_modules = find_all_linear_names(
                    model,
                    int4=model_args.load_in_4bit,
                    int8=model_args.load_in_8bit
                )

            # 2.3 ä¸€äº›æ¨¡å—ä¸èµ° LoRAï¼Œä½†éœ€è¦ä¿å­˜
            # å…¸å‹ä¾‹å­ï¼š
            # - lm_head
            # - embedding
            # - ç‰¹å®š adapter
            modules_to_save = script_args.modules_to_save
            if modules_to_save is not None:
                modules_to_save = modules_to_save.split(',')

            # 2.4 æ„å»º LoRA é…ç½®
            peft_config = LoraConfig(
                task_type=TaskType.CAUSAL_LM,  # å› æœè¯­è¨€æ¨¡å‹
                target_modules=target_modules,  # LoRA æ³¨å…¥çš„æ¨¡å—
                inference_mode=False,  # è®­ç»ƒæ¨¡å¼
                r=script_args.lora_rank,  # LoRA rankï¼ˆä½ç§©ç»´åº¦ï¼‰
                lora_alpha=script_args.lora_alpha,  # ç¼©æ”¾å› å­
                lora_dropout=script_args.lora_dropout,  # LoRA dropout
                modules_to_save=modules_to_save
            )

            # 2.5 å°† LoRA æ³¨å…¥æ¨¡å‹
            model = get_peft_model(model, peft_config)

        # ===== 3. dtype ä¿®æ­£ =====
        # å¯¹æ‰€æœ‰ã€Œå¯è®­ç»ƒå‚æ•°ã€ç»Ÿä¸€è½¬æˆ float32
        # åŸå› ï¼š
        # - é¿å… LoRA å‚æ•°åœ¨ fp16 / bf16 ä¸‹ä¸ç¨³å®š
        # - QLoRA ä¸­æ˜¯éå¸¸å¸¸è§çš„å†™æ³•
        for param in filter(lambda p: p.requires_grad, model.parameters()):
            param.data = param.data.to(torch.float32)

        # æ‰“å°å¯è®­ç»ƒå‚æ•°æ¯”ä¾‹ï¼ˆéå¸¸é‡è¦çš„ sanity checkï¼‰
        model.print_trainable_parameters()

    else:
        # ===== 4. ä¸ä½¿ç”¨ PEFTï¼šå…¨å‚æ•°å¾®è°ƒ =====
        logger.info("ğŸ”§ å…¨å‚æ•°è®­ç»ƒæ¨¡å¼")

        # æ•´ä¸ªæ¨¡å‹è½¬æˆ float32
        model = model.float()

        # æ‰“å°æ‰€æœ‰å¯è®­ç»ƒå‚æ•°
        print_trainable_parameters(model)

    return model


def prepare_datasets(raw_datasets, training_args, data_args, tokenizer, prompt_template, script_args, IGNORE_INDEX):
    """å‡†å¤‡å’Œé¢„å¤„ç†æ•°æ®é›†"""
    logger.info("ğŸ”„ å¼€å§‹é¢„å¤„ç†æ•°æ®é›†...")
    preprocess_function = create_preprocess_function(tokenizer, prompt_template, script_args, IGNORE_INDEX)

    train_dataset = None
    eval_dataset = None

    # å¤„ç†è®­ç»ƒæ•°æ®
    if training_args.do_train:
        if "train" not in raw_datasets:
            raise ValueError("--do_train requires a train dataset")
        train_dataset = raw_datasets['train'].shuffle(seed=42)

        if data_args.max_train_samples is not None and data_args.max_train_samples > 0:
            train_dataset = train_dataset.select(range(min(len(train_dataset), data_args.max_train_samples)))

        logger.debug(f"Example train_dataset[0]: {train_dataset[0]}")
        tokenized_dataset = train_dataset.map(
            preprocess_function,
            batched=True,
            num_proc=data_args.preprocessing_num_workers,
            remove_columns=train_dataset.column_names,
            load_from_cache_file=not data_args.overwrite_cache,
            desc="Running tokenizer on dataset",
        )
        train_dataset = tokenized_dataset.filter(
            lambda example: filter_empty_labels(example, IGNORE_INDEX),
            num_proc=data_args.preprocessing_num_workers
        )
        logger.debug(f"Num train_samples: {len(train_dataset)}")
        logger.debug("Tokenized training example:")
        logger.debug(f"Decode input_ids[0]:\n{tokenizer.decode(train_dataset[0]['input_ids'])}")
        replaced_labels = [label if label != IGNORE_INDEX else tokenizer.pad_token_id
                           for label in list(train_dataset[0]['labels'])]
        logger.debug(f"Decode labels[0]:\n{tokenizer.decode(replaced_labels)}")

    # å¤„ç†éªŒè¯æ•°æ®
    if training_args.do_eval:
        if "validation" not in raw_datasets:
            raise ValueError("--do_eval requires a validation dataset")
        eval_dataset = raw_datasets['validation']

        if data_args.max_eval_samples is not None and data_args.max_eval_samples > 0:
            eval_dataset = eval_dataset.select(range(min(len(eval_dataset), data_args.max_eval_samples)))

        eval_size = len(eval_dataset)
        logger.debug(f"Num eval_samples: {eval_size}")
        if eval_size > 500:
            logger.warning(f"Num eval_samples is large: {eval_size}, "
                           f"training slow, consider reduce it by `--max_eval_samples=50`")

        logger.debug(f"Example eval_dataset[0]: {eval_dataset[0]}")
        eval_dataset = eval_dataset.map(
            preprocess_function,
            batched=True,
            num_proc=data_args.preprocessing_num_workers,
            remove_columns=eval_dataset.column_names,
            load_from_cache_file=not data_args.overwrite_cache,
            desc="Running tokenizer on validation dataset",
        )
        eval_dataset = eval_dataset.filter(
            lambda example: filter_empty_labels(example, IGNORE_INDEX),
            num_proc=data_args.preprocessing_num_workers
        )
        logger.debug(f"Num eval_samples: {len(eval_dataset)}")
        logger.debug("Tokenized eval example:")
        logger.debug(tokenizer.decode(eval_dataset[0]['input_ids']))

    logger.info("âœ… æ•°æ®é›†é¢„å¤„ç†å®Œæˆ")
    return train_dataset, eval_dataset


def prepare_training_components(train_dataset, eval_dataset, model, tokenizer, training_args, IGNORE_INDEX):
    """å‡†å¤‡è®­ç»ƒç»„ä»¶ï¼šæ•°æ®åŠ è½½å™¨ã€ä¼˜åŒ–å™¨ã€å­¦ä¹ ç‡è°ƒåº¦å™¨"""
    data_collator = DataCollatorForSeq2Seq(
        tokenizer=tokenizer,
        model=model,
        label_pad_token_id=IGNORE_INDEX,
        pad_to_multiple_of=4 if tokenizer.padding_side == "right" else None,
    )

    train_dataloader = None
    eval_dataloader = None

    if training_args.do_train and train_dataset is not None:
        train_dataloader = torch.utils.data.DataLoader(
            train_dataset,
            batch_size=training_args.per_device_train_batch_size,
            shuffle=True,
            collate_fn=data_collator,
        )

    if training_args.do_eval and eval_dataset is not None:
        eval_dataloader = torch.utils.data.DataLoader(
            eval_dataset,
            batch_size=training_args.per_device_eval_batch_size,
            shuffle=False,
            collate_fn=data_collator,
        )

    optimizer = None
    lr_scheduler = None

    if training_args.do_train:
        optimizer = torch.optim.AdamW(
            filter(lambda p: p.requires_grad, model.parameters()),
            lr=training_args.learning_rate,
            weight_decay=training_args.weight_decay,
        )

        num_update_steps_per_epoch = len(train_dataloader) // training_args.gradient_accumulation_steps
        max_train_steps = training_args.num_train_epochs * num_update_steps_per_epoch

        lr_scheduler = get_linear_schedule_with_warmup(
            optimizer=optimizer,
            num_warmup_steps=int(max_train_steps * training_args.warmup_ratio),
            num_training_steps=max_train_steps,
        )

    return train_dataloader, eval_dataloader, optimizer, lr_scheduler


def prepare_accelerator_components(
        accelerator,
        model,
        train_dataloader,
        eval_dataloader,
        optimizer,
        lr_scheduler,
        training_args,
        model_is_distributed
):
    """
    ä½¿ç”¨ HuggingFace Accelerate å¯¹è®­ç»ƒç›¸å…³ç»„ä»¶è¿›è¡Œç»Ÿä¸€å°è£…å’Œåˆ†å¸ƒå¼é€‚é…

    è¿™ä¸ªå‡½æ•°çš„æ ¸å¿ƒç›®æ ‡ï¼š
    - æ ¹æ®æ¨¡å‹æ˜¯å¦å·²ç»æ˜¯â€œåˆ†å¸ƒå¼åŠ è½½â€çš„çŠ¶æ€ï¼Œé€‰æ‹©ä¸åŒçš„ prepare ç­–ç•¥
    - æ­£ç¡®å¤„ç† model / optimizer / dataloader / scheduler
    - é¿å… Accelerate å¯¹â€œå·²åˆ†å¸ƒå¼æ¨¡å‹â€é‡å¤ wrap å¯¼è‡´çš„é—®é¢˜
    """

    logger.info("ğŸ”„ å¼€å§‹å‡†å¤‡è®­ç»ƒç»„ä»¶...")

    # ============================================================
    # æƒ…å†µä¸€ï¼šæ¨¡å‹å·²ç»æ˜¯åˆ†å¸ƒå¼çš„ï¼ˆä¾‹å¦‚ï¼š
    # - ä½¿ç”¨ device_map="auto"
    # - ä½¿ç”¨ FSDP / DeepSpeed é¢„å…ˆåŒ…è£¹
    # - QLoRA + load_in_4bit + auto device mapï¼‰
    #
    # è¿™ç±»æ¨¡å‹ã€ä¸èƒ½ã€‘å†äº¤ç»™ accelerator.prepare(model)
    # å¦åˆ™ä¼šå‡ºç°ï¼š
    # - å‚æ•°é‡å¤ wrap
    # - device ä¸ä¸€è‡´
    # - è®­ç»ƒç›´æ¥æŠ¥é”™
    # ============================================================
    if model_is_distributed:
        logger.info("ğŸ”§ æ£€æµ‹åˆ°æ¨¡å‹å·²åˆ†å¸ƒåœ¨å¤šè®¾å¤‡ï¼Œä½¿ç”¨å…¼å®¹æ¨¡å¼")

        # ----------------------------
        # è®­ç»ƒæ¨¡å¼
        # ----------------------------
        if training_args.do_train:
            # âš ï¸ æ³¨æ„ï¼šè¿™é‡Œåª prepare optimizer / dataloader / scheduler
            # ä¸å† prepare model
            optimizer, train_dataloader, lr_scheduler = accelerator.prepare(
                optimizer,
                train_dataloader,
                lr_scheduler
            )

            # éªŒè¯é›† dataloader å•ç‹¬ prepare
            if eval_dataloader is not None:
                eval_dataloader = accelerator.prepare(eval_dataloader)

        # ----------------------------
        # ä»…è¯„ä¼°æ¨¡å¼ï¼ˆä¸è®­ç»ƒï¼‰
        # ----------------------------
        else:
            if eval_dataloader is not None:
                eval_dataloader = accelerator.prepare(eval_dataloader)

        # æ ¹æ®æ˜¯å¦è®­ç»ƒï¼Œæ˜¾å¼è®¾ç½®æ¨¡å‹çŠ¶æ€
        # ï¼ˆé¿å…æŸäº› wrapper åœºæ™¯ä¸‹çŠ¶æ€ä¸ä¸€è‡´ï¼‰
        model.train() if training_args.do_train else model.eval()

        logger.info("âœ… åˆ†å¸ƒå¼æ¨¡å‹è®­ç»ƒç»„ä»¶å‡†å¤‡å®Œæˆ")

    # ============================================================
    # æƒ…å†µäºŒï¼šæ ‡å‡†æ¨¡å¼
    # - æ¨¡å‹æ˜¯æ™®é€š nn.Module
    # - å°šæœªè¿›è¡Œä»»ä½•åˆ†å¸ƒå¼å°è£…
    #
    # è¿™ç§æƒ…å†µè®© Accelerate æ¥ç®¡ä¸€åˆ‡æ˜¯æœ€å®‰å…¨çš„
    # ============================================================
    else:
        logger.info("ğŸ”§ æ ‡å‡†æ¨¡å¼ï¼Œè®©Accelerateå¤„ç†æ‰€æœ‰ç»„ä»¶")

        # ----------------------------
        # è®­ç»ƒæ¨¡å¼
        # ----------------------------
        if training_args.do_train:
            # accelerator.prepare ä¼šåšçš„äº‹æƒ…åŒ…æ‹¬ï¼š
            # - model -> DDP / FSDP / DeepSpeed
            # - optimizer å‚æ•°åŒæ­¥
            # - dataloader è‡ªåŠ¨åŠ  DistributedSampler
            # - scheduler é€‚é…å¤šè¿›ç¨‹æ­¥æ•°
            model, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(
                model,
                optimizer,
                train_dataloader,
                lr_scheduler
            )

            if eval_dataloader is not None:
                eval_dataloader = accelerator.prepare(eval_dataloader)

        # ----------------------------
        # ä»…è¯„ä¼°æ¨¡å¼
        # ----------------------------
        else:
            # å³ä½¿åªè¯„ä¼°ï¼Œä¹Ÿè¦ prepare model
            # å¦åˆ™åœ¨å¤š GPU ä¸‹ forward ä¼šæœ‰é—®é¢˜
            model = accelerator.prepare(model)

            if eval_dataloader is not None:
                eval_dataloader = accelerator.prepare(eval_dataloader)

        logger.info("âœ… æ ‡å‡†è®­ç»ƒç»„ä»¶å‡†å¤‡å®Œæˆ")

    # ============================================================
    # å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹ï¼ˆGradient Checkpointingï¼‰
    #
    # ä½œç”¨ï¼š
    # - ç”¨è®¡ç®—æ¢æ˜¾å­˜
    # - å¯¹å¤§æ¨¡å‹ / LoRA / QLoRA éå¸¸å…³é”®
    #
    # âš ï¸ å¿…é¡»åœ¨ prepare ä¹‹åè°ƒç”¨ï¼š
    # - å¦åˆ™å¯èƒ½æ‹¿åˆ°çš„æ˜¯æœª wrap çš„ model
    # ============================================================
    setup_gradient_checkpointing(model, training_args)

    logger.info("ğŸ‰ Accelerateå¤šGPUè®­ç»ƒé…ç½®æˆåŠŸï¼")

    # è¿”å›æ‰€æœ‰å¯èƒ½è¢« accelerator æ›¿æ¢/åŒ…è£…åçš„å¯¹è±¡
    return model, train_dataloader, eval_dataloader, optimizer, lr_scheduler


def setup_gradient_checkpointing(model, training_args):
    """
    è®¾ç½®æ¢¯åº¦æ£€æŸ¥ç‚¹ï¼ˆGradient Checkpointingï¼‰

    ä½œç”¨ï¼š
    - ç”¨â€œç®—åŠ›æ¢æ˜¾å­˜â€ï¼Œåœ¨å‰å‘ä¼ æ’­æ—¶ä¸ä¿å­˜ä¸­é—´æ¿€æ´»
    - åå‘ä¼ æ’­æ—¶é‡æ–°è®¡ç®—å‰å‘ï¼Œä»è€Œæ˜¾è‘—é™ä½æ˜¾å­˜å ç”¨
    - å¸¸ç”¨äºå¤§æ¨¡å‹ / é•¿ä¸Šä¸‹æ–‡ / LoRA å¾®è°ƒåœºæ™¯

    æ³¨æ„ï¼š
    - å¯ç”¨ gradient checkpointing æ—¶ï¼Œå¿…é¡»å…³é—­ use_cache
    - å¦åˆ™ä¼šå¯¼è‡´æ˜¾å­˜å¼‚å¸¸æˆ–ç›´æ¥æŠ¥é”™
    """

    # åªæœ‰åœ¨ï¼š
    # 1ï¼‰è®­ç»ƒå‚æ•°ä¸­å¼€å¯äº† gradient_checkpointing
    # 2ï¼‰æ¨¡å‹æœ¬èº«æ”¯æŒ gradient checkpointing
    # æ‰çœŸæ­£å¯ç”¨
    if training_args.gradient_checkpointing and getattr(model, "supports_gradient_checkpointing", False):

        # å¯ç”¨æ¨¡å‹çš„æ¢¯åº¦æ£€æŸ¥ç‚¹åŠŸèƒ½
        model.gradient_checkpointing_enable()

        # å¦‚æœæ¨¡å‹è¢« DDP / FSDP / Accelerate åŒ…äº†ä¸€å±‚
        if hasattr(model, "module"):
            # å…³é—­ KV cacheï¼ˆå¦åˆ™ä¸ gradient checkpointing å†²çªï¼‰
            model.module.config.use_cache = False
            logger.info("Gradient checkpointing enabled for DDP model.")
        else:
            # å•å¡ / éåˆ†å¸ƒå¼åœºæ™¯
            model.config.use_cache = False
            logger.info("Gradient checkpointing enabled.")

    else:
        # æœªå¯ç”¨ gradient checkpointing çš„æƒ…å†µ
        # è¿™é‡Œæ˜¾å¼æŠŠ use_cache æ‰“å¼€ï¼Œä¿è¯æ¨ç†/è®­ç»ƒè¡Œä¸ºæ­£å¸¸
        if hasattr(model, "module"):
            model.module.config.use_cache = True
            logger.info("Gradient checkpointing disabled for DDP model.")
        else:
            model.config.use_cache = True
            logger.info("Gradient checkpointing disabled.")

    # å¼ºåˆ¶å¼€å¯ input.requires_grad
    #
    # ç›®çš„ï¼š
    # - å¯¹äº LoRA / PEFT / éƒ¨åˆ†å†»ç»“å‚æ•°çš„è®­ç»ƒéå¸¸å…³é”®
    # - ç¡®ä¿ embedding / input ç›¸å…³å¼ é‡èƒ½æ­£ç¡®å‚ä¸åå‘ä¼ æ’­
    #
    # å¦åˆ™å¯èƒ½å‡ºç°ï¼š
    # - loss.backward() ä¸æŠ¥é”™ä½†å‚æ•°ä¸æ›´æ–°
    # - LoRA æƒé‡æ¢¯åº¦ä¸º None
    if hasattr(model, "module"):
        model.module.enable_input_require_grads()
    else:
        model.enable_input_require_grads()


def train_model(
        accelerator,
        model,
        train_dataloader,
        eval_dataloader,
        optimizer,
        lr_scheduler,
        training_args,
        model_is_distributed
):
    """
    æ¨¡å‹è®­ç»ƒä¸»å¾ªç¯

    æ”¯æŒä¸¤ç§è®­ç»ƒæ¨¡å¼ï¼š
        1. model_is_distributed=True  : æ‰‹å†™åˆ†å¸ƒå¼ / å¼ é‡å¹¶è¡Œè®­ç»ƒé€»è¾‘
        2. model_is_distributed=False : ä½¿ç”¨ Accelerate æ ‡å‡†è®­ç»ƒèŒƒå¼

    Args:
        accelerator: Accelerate å¯¹è±¡ï¼Œè´Ÿè´£å¤šå¡ / æ··åˆç²¾åº¦ / æ¢¯åº¦åŒæ­¥
        model: å¾…è®­ç»ƒæ¨¡å‹
        train_dataloader: è®­ç»ƒæ•°æ®åŠ è½½å™¨
        eval_dataloader: éªŒè¯æ•°æ®åŠ è½½å™¨ï¼ˆå¯ä¸º Noneï¼‰
        optimizer: ä¼˜åŒ–å™¨
        lr_scheduler: å­¦ä¹ ç‡è°ƒåº¦å™¨
        training_args: è®­ç»ƒè¶…å‚æ•°ï¼ˆepochã€logging_steps ç­‰ï¼‰
        model_is_distributed: æ˜¯å¦ä¸ºç‰¹æ®Šåˆ†å¸ƒå¼æ¨¡å‹ï¼ˆå¦‚å¼ é‡å¹¶è¡Œï¼‰
    """
    logger.info("*** å¼€å§‹è®­ç»ƒ ***")

    # è®¾ç½®æ¨¡å‹ä¸ºè®­ç»ƒæ¨¡å¼
    model.train()

    # ç”¨äºç´¯è®¡ logging_steps å†…çš„ loss
    total_loss = 0

    # å·²å®Œæˆçš„ã€Œä¼˜åŒ–å™¨æ›´æ–°æ­¥æ•°ã€ï¼ˆä¸æ˜¯ dataloader stepï¼‰
    completed_steps = 0

    # =========================
    # åˆ›å»ºè®­ç»ƒè¿›åº¦æ¡
    # =========================
    progress_bar = tqdm(
        range(int(training_args.num_train_epochs * len(train_dataloader))),
        disable=not accelerator.is_local_main_process,  # åªåœ¨ä¸»è¿›ç¨‹æ˜¾ç¤º
        desc="Training"
    )

    # =========================
    # Epoch çº§åˆ«å¾ªç¯
    # =========================
    for epoch in range(int(training_args.num_train_epochs)):
        logger.info(
            f"å¼€å§‹ç¬¬ {epoch + 1}/{int(training_args.num_train_epochs)} è½®è®­ç»ƒ"
        )

        # =========================
        # Step çº§åˆ«å¾ªç¯ï¼ˆbatchï¼‰
        # =========================
        for step, batch in enumerate(train_dataloader):

            # =========================================================
            # æƒ…å†µä¸€ï¼šè‡ªå®šä¹‰åˆ†å¸ƒå¼ / å¼ é‡å¹¶è¡Œè®­ç»ƒé€»è¾‘
            # =========================================================
            if model_is_distributed:
                # å‰å‘ä¼ æ’­
                outputs = model(**batch)
                loss = outputs.loss

                # æ¢¯åº¦ç´¯ç§¯ï¼šloss éœ€è¦é™¤ä»¥ç´¯ç§¯æ­¥æ•°
                if training_args.gradient_accumulation_steps > 1:
                    loss = loss / training_args.gradient_accumulation_steps

                # åå‘ä¼ æ’­
                loss.backward()

                # è¾¾åˆ°æ¢¯åº¦ç´¯ç§¯æ­¥æ•°ï¼Œæ‰çœŸæ­£æ›´æ–°å‚æ•°
                if (step + 1) % training_args.gradient_accumulation_steps == 0:

                    # æ¢¯åº¦è£å‰ªï¼Œé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸
                    if training_args.max_grad_norm > 0:
                        torch.nn.utils.clip_grad_norm_(
                            model.parameters(),
                            training_args.max_grad_norm
                        )

                    # å‚æ•°æ›´æ–°
                    optimizer.step()
                    lr_scheduler.step()
                    optimizer.zero_grad()

                    completed_steps += 1
                    progress_bar.update(1)

            # =========================================================
            # æƒ…å†µäºŒï¼šä½¿ç”¨ Accelerate æ ‡å‡†è®­ç»ƒæµç¨‹
            # =========================================================
            else:
                # accelerator.accumulate ä¼šè‡ªåŠ¨å¤„ç†æ¢¯åº¦ç´¯ç§¯ & åŒæ­¥
                with accelerator.accumulate(model):

                    # å‰å‘ä¼ æ’­
                    outputs = model(**batch)
                    loss = outputs.loss

                    # Accelerate ç»Ÿä¸€ backwardï¼ˆæ”¯æŒ AMP / å¤šå¡ï¼‰
                    accelerator.backward(loss)

                    # åªæœ‰åœ¨çœŸæ­£åŒæ­¥æ¢¯åº¦çš„ step æ‰è£å‰ªæ¢¯åº¦
                    if accelerator.sync_gradients:
                        accelerator.clip_grad_norm_(
                            model.parameters(),
                            training_args.max_grad_norm
                        )

                    # å‚æ•°æ›´æ–°
                    optimizer.step()
                    lr_scheduler.step()
                    optimizer.zero_grad()

                # sync_gradients=True è¡¨ç¤ºå®Œæˆäº†ä¸€æ¬¡ optimizer step
                if accelerator.sync_gradients:
                    completed_steps += 1
                    progress_bar.update(1)

            # =========================
            # ç´¯è®¡ lossï¼ˆç”¨äº loggingï¼‰
            # =========================
            total_loss += loss.detach().float()

            # =========================
            # åˆ¤æ–­æ˜¯å¦å®Œæˆäº†ä¸€ä¸ªâ€œä¼˜åŒ–å™¨æ›´æ–° stepâ€
            # =========================
            if model_is_distributed:
                step_completed = (
                        (step + 1) % training_args.gradient_accumulation_steps == 0
                )
            else:
                step_completed = accelerator.sync_gradients

            if step_completed:

                # =========================
                # æ—¥å¿—æ‰“å°
                # =========================
                if completed_steps % training_args.logging_steps == 0:
                    avg_loss = total_loss / training_args.logging_steps
                    current_lr = (
                        lr_scheduler.get_last_lr()[0]
                        if lr_scheduler else training_args.learning_rate
                    )
                    logger.info(
                        f"Step {completed_steps}: "
                        f"loss = {avg_loss:.4f}, "
                        f"lr = {current_lr:.2e}"
                    )
                    total_loss = 0

                # =========================
                # ä¿å­˜ checkpoint
                # =========================
                if (
                        training_args.save_steps > 0 and
                        completed_steps % training_args.save_steps == 0
                ):
                    output_dir = os.path.join(
                        training_args.output_dir,
                        f"checkpoint-{completed_steps}"
                    )

                    if model_is_distributed:
                        # åˆ†å¸ƒå¼æ¨¡å‹ï¼šæ‰‹åŠ¨ä¿å­˜
                        os.makedirs(output_dir, exist_ok=True)
                        model.save_pretrained(output_dir)

                        torch.save(
                            {
                                "optimizer": optimizer.state_dict(),
                                "lr_scheduler": (
                                    lr_scheduler.state_dict()
                                    if lr_scheduler else None
                                ),
                                "completed_steps": completed_steps,
                            },
                            os.path.join(output_dir, "training_state.pt")
                        )
                    else:
                        # Accelerate æ¨èæ–¹å¼
                        accelerator.save_state(output_dir)

                    logger.info(f"ä¿å­˜æ£€æŸ¥ç‚¹åˆ°: {output_dir}")

                # =========================
                # å®šæœŸè¯„ä¼°ï¼ˆevalï¼‰
                # =========================
                if (
                        training_args.do_eval and
                        training_args.eval_steps > 0 and
                        completed_steps % training_args.eval_steps == 0 and
                        eval_dataloader is not None
                ):
                    model.eval()

                    eval_loss = 0
                    eval_steps = 0

                    for eval_batch in eval_dataloader:
                        with torch.no_grad():
                            eval_outputs = model(**eval_batch)
                            eval_loss += eval_outputs.loss.detach().float()
                            eval_steps += 1

                    avg_eval_loss = eval_loss / eval_steps

                    # å›°æƒ‘åº¦ï¼ˆPerplexityï¼‰
                    try:
                        perplexity = math.exp(avg_eval_loss)
                    except OverflowError:
                        perplexity = float("inf")

                    logger.info(
                        f"Step {completed_steps}: "
                        f"eval_loss = {avg_eval_loss:.4f}, "
                        f"perplexity = {perplexity:.2f}"
                    )

                    # åˆ‡å›è®­ç»ƒæ¨¡å¼
                    model.train()

    progress_bar.close()
    return completed_steps


def save_final_model(
        accelerator,
        model,
        tokenizer,
        training_args,
        model_is_distributed,
        completed_steps
):
    """
    ä¿å­˜æœ€ç»ˆè®­ç»ƒå®Œæˆçš„æ¨¡å‹å’Œ tokenizer

    è¯¥å‡½æ•°ä¸»è¦å¤„ç†ä»¥ä¸‹å‡ ä»¶äº‹ï¼š
    1. åªåœ¨ä¸»è¿›ç¨‹æ‰“å°æ—¥å¿—ï¼Œé¿å…åˆ†å¸ƒå¼ä¸‹æ—¥å¿—é‡å¤
    2. æ¢å¤æ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­è¢«å…³é—­çš„é…ç½®ï¼ˆå¦‚ use_cacheï¼‰
    3. æ­£ç¡®å¤„ç† Accelerate / DDP åŒ…è£…åçš„æ¨¡å‹ä¿å­˜
    4. ç¡®ä¿å¤šè¿›ç¨‹ä¹‹é—´åŒæ­¥ï¼Œé¿å…ä¿å­˜å†²çª
    """

    # åªåœ¨ä¸»è¿›ç¨‹æ‰“å°ä¿å­˜æ—¥å¿—ï¼ˆDDP / å¤šå¡ä¸‹éå¸¸é‡è¦ï¼‰
    if accelerator.is_main_process:
        logger.info(f"ä¿å­˜æ¨¡å‹åˆ°: {training_args.output_dir}")

    # ================================
    # 1. æ¢å¤æ¨¡å‹çš„æ¨ç†ç›¸å…³é…ç½®
    # ================================
    # è®­ç»ƒé˜¶æ®µï¼ˆå°¤å…¶æ˜¯æ¢¯åº¦æ£€æŸ¥ç‚¹ï¼‰é€šå¸¸ä¼šå…³é—­ use_cache
    # è®­ç»ƒç»“æŸåéœ€è¦æ¢å¤ï¼Œå¦åˆ™ä¼šå½±å“æ¨ç†æ€§èƒ½
    unwrapped = get_unwrapped_model(model)
    unwrapped.config.use_cache = True

    # å¯ç”¨è¾“å…¥æ¢¯åº¦ï¼ˆé€šå¸¸ç”¨äº LoRA / PEFT æˆ–åç»­ç»§ç»­å¾®è°ƒï¼‰
    # æœ‰äº›è®­ç»ƒæµç¨‹åœ¨ä¸­é€”ä¼šå…³é—­ input grads
    unwrapped.enable_input_require_grads()

    # ================================
    # 2. æ ¹æ®æ˜¯å¦æ˜¯åˆ†å¸ƒå¼æ¨¡å‹é€‰æ‹©ä¿å­˜æ–¹å¼
    # ================================
    if model_is_distributed:
        # ----------------------------
        # åˆ†å¸ƒå¼ï¼ˆDDPï¼‰æ¨¡å‹ä¿å­˜
        # ----------------------------
        # æŸäº›æƒ…å†µä¸‹æ¨¡å‹å·²ç»æ˜¯â€œå¯ç›´æ¥ä¿å­˜â€çš„çŠ¶æ€
        # ï¼ˆä¾‹å¦‚ FullyShardedDataParallel / ç‰¹å®š DDP é…ç½®ï¼‰
        logger.info("ğŸ”§ ä¿å­˜åˆ†å¸ƒå¼æ¨¡å‹...")

        # ç›´æ¥ä¿å­˜æ¨¡å‹å’Œ tokenizer
        model.save_pretrained(training_args.output_dir)
        tokenizer.save_pretrained(training_args.output_dir)

    else:
        # ----------------------------
        # æ ‡å‡† Accelerate ä¿å­˜æµç¨‹
        # ----------------------------
        # ç­‰å¾…æ‰€æœ‰è¿›ç¨‹åˆ°è¾¾è¿™é‡Œï¼Œé˜²æ­¢ä¸»è¿›ç¨‹æå‰ä¿å­˜
        accelerator.wait_for_everyone()

        if accelerator.is_main_process:
            # unwrap_model ä¼šå»æ‰ Accelerate / DDP çš„å¤–å±‚åŒ…è£…
            # æ‹¿åˆ°çœŸæ­£çš„ HuggingFace æ¨¡å‹å¯¹è±¡
            unwrapped_model = accelerator.unwrap_model(model)

            # ä½¿ç”¨è‡ªå®šä¹‰çš„ save_model æ–¹æ³•ä¿å­˜
            # ï¼ˆé€šå¸¸å†…éƒ¨ä¼šå¤„ç† safetensors / config / æƒé‡ï¼‰
            save_model(
                unwrapped_model,
                tokenizer,
                training_args.output_dir
            )

            logger.info("âœ… æ ‡å‡†æ¨¡å‹ä¿å­˜å®Œæˆ")


def final_evaluation(accelerator, model, eval_dataloader):
    """
    åœ¨è®­ç»ƒç»“æŸåå¯¹æ¨¡å‹è¿›è¡Œæœ€ç»ˆè¯„ä¼°ï¼ˆFinal Evaluationï¼‰

    ä¸»è¦ä½œç”¨ï¼š
    1. åœ¨éªŒè¯é›† / æµ‹è¯•é›†ä¸Šè®¡ç®—å¹³å‡ loss
    2. æ ¹æ® loss è®¡ç®— perplexityï¼ˆå›°æƒ‘åº¦ï¼‰
    3. åªåœ¨ä¸»è¿›ç¨‹ï¼ˆmain processï¼‰ä¸Šè¾“å‡ºè¯„ä¼°æ—¥å¿—ï¼Œé¿å…åˆ†å¸ƒå¼é‡å¤æ‰“å°
    """

    # å¦‚æœæ²¡æœ‰æä¾›è¯„ä¼°æ•°æ®é›†ï¼Œåˆ™ç›´æ¥è·³è¿‡æœ€ç»ˆè¯„ä¼°
    if eval_dataloader is not None:
        logger.info("*** æœ€ç»ˆè¯„ä¼° ***")

        # åˆ‡æ¢æ¨¡å‹åˆ°è¯„ä¼°æ¨¡å¼
        # - å…³é—­ dropout
        # - å›ºå®š BatchNorm / LayerNorm çš„è¡Œä¸º
        model.eval()

        # ç´¯ç§¯è¯„ä¼° lossï¼ˆæ³¨æ„ï¼šè¿™æ˜¯æ‰€æœ‰ batch çš„ loss æ±‚å’Œï¼‰
        eval_loss = 0

        # ç»Ÿè®¡è¯„ä¼°æ­¥æ•°ï¼ˆbatch æ•°ï¼‰
        eval_steps = 0

        # éå†è¯„ä¼°æ•°æ®é›†
        for eval_batch in eval_dataloader:
            # è¯„ä¼°é˜¶æ®µä¸éœ€è¦åå‘ä¼ æ’­
            # torch.no_grad() å¯ä»¥ï¼š
            # - å…³é—­æ¢¯åº¦è®¡ç®—
            # - å‡å°‘æ˜¾å­˜å ç”¨
            # - æå‡æ¨ç†é€Ÿåº¦
            with torch.no_grad():
                # å‰å‘è®¡ç®—
                # eval_batch é€šå¸¸åŒ…å«ï¼š
                # - input_ids
                # - attention_mask
                # - labels
                eval_outputs = model(**eval_batch)

                # eval_outputs.loss æ˜¯ä¸€ä¸ªæ ‡é‡ tensor
                # detach(): ä»è®¡ç®—å›¾ä¸­åˆ†ç¦»ï¼Œé˜²æ­¢æ¢¯åº¦è·Ÿè¸ª
                # float(): ç¡®ä¿æ˜¯ FP32ï¼Œé¿å…æ··åˆç²¾åº¦ä¸‹æ•°å€¼é—®é¢˜
                eval_loss += eval_outputs.loss.detach().float()

                # è¯„ä¼°æ­¥æ•° +1
                eval_steps += 1

        # è®¡ç®—å¹³å‡ loss
        # æ³¨æ„ï¼šè¿™é‡Œæ˜¯â€œbatch å¹³å‡â€ï¼Œä¸æ˜¯â€œtoken å¹³å‡â€
        avg_eval_loss = eval_loss / eval_steps

        # æ ¹æ®å¹³å‡ loss è®¡ç®—å›°æƒ‘åº¦ï¼ˆPerplexityï¼‰
        # perplexity = exp(loss)
        # å¯¹è¯­è¨€æ¨¡å‹æ¥è¯´ï¼Œppl è¶Šä½è¡¨ç¤ºæ¨¡å‹è¶Šå¥½
        try:
            perplexity = math.exp(avg_eval_loss)
        except OverflowError:
            # å½“ loss éå¸¸å¤§æ—¶ï¼Œexp(loss) å¯èƒ½æº¢å‡º
            # æ­¤æ—¶å°† perplexity è®¾ä¸ºæ— ç©·å¤§
            perplexity = float("inf")

        # åªåœ¨ä¸»è¿›ç¨‹æ‰“å°æ—¥å¿—
        # åœ¨å¤šå¡ / å¤šè¿›ç¨‹è®­ç»ƒä¸­ï¼Œé¿å…é‡å¤è¾“å‡º
        if accelerator.is_main_process:
            logger.info(
                f"æœ€ç»ˆè¯„ä¼°ç»“æœ: eval_loss = {avg_eval_loss:.4f}, "
                f"perplexity = {perplexity:.2f}"
            )


def main():
    """ä¸»å‡½æ•°"""
    # è§£æå‚æ•°
    model_args, data_args, training_args, script_args = parse_arguments()

    # åˆå§‹åŒ–Accelerator
    accelerator = setup_accelerator()

    # è®¾ç½®éšæœºç§å­
    accelerate_set_seed(training_args.seed)

    # è¾“å‡ºå‚æ•°ä¿¡æ¯
    logger.info(f"Model args: {model_args}")
    logger.info(f"Training args: {training_args}")
    logger.info(f"Script args: {script_args}")

    # é…ç½®tokenizer
    tokenizer, prompt_template = setup_tokenizer(model_args, script_args)
    IGNORE_INDEX = LabelSmoother.ignore_index if data_args.ignore_pad_token_for_loss else tokenizer.pad_token_id

    # æ£€æŸ¥å’Œä¼˜åŒ–å†…å­˜
    check_and_optimize_memory()

    # åŠ è½½å’Œé…ç½®æ¨¡å‹
    model, model_is_distributed = load_and_configure_model(model_args, script_args, accelerator)

    # é…ç½®PEFT
    model = setup_peft(model, model_args, script_args, training_args)

    # åŠ è½½æ•°æ®é›†
    logger.info("ğŸ”„ å¼€å§‹åŠ è½½æ•°æ®é›†...")
    raw_datasets = load_datasets(data_args, model_args)

    # å‡†å¤‡æ•°æ®é›†
    train_dataset, eval_dataset = prepare_datasets(raw_datasets, training_args, data_args, tokenizer, prompt_template,
                                                   script_args, IGNORE_INDEX)

    # å‡†å¤‡è®­ç»ƒç»„ä»¶
    train_dataloader, eval_dataloader, optimizer, lr_scheduler = prepare_training_components(train_dataset,
                                                                                             eval_dataset, model,
                                                                                             tokenizer, training_args,
                                                                                             IGNORE_INDEX)

    # ä½¿ç”¨Accelerateå‡†å¤‡æ‰€æœ‰ç»„ä»¶
    model, train_dataloader, eval_dataloader, optimizer, lr_scheduler = prepare_accelerator_components(accelerator,
                                                                                                       model,
                                                                                                       train_dataloader,
                                                                                                       eval_dataloader,
                                                                                                       optimizer,
                                                                                                       lr_scheduler,
                                                                                                       training_args,
                                                                                                       model_is_distributed)

    # å¼€å§‹è®­ç»ƒ
    completed_steps = 0
    if training_args.do_train:
        completed_steps = train_model(accelerator, model, train_dataloader, eval_dataloader, optimizer, lr_scheduler,
                                      training_args, model_is_distributed)
        save_final_model(accelerator, model, tokenizer, training_args, model_is_distributed, completed_steps)

    # æœ€ç»ˆè¯„ä¼°
    if training_args.do_eval:
        final_evaluation(accelerator, model, eval_dataloader)


if __name__ == "__main__":
    main()
