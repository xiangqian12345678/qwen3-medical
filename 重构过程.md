# 核心目的

    1.分布式可训练
    2.收集所有可用医疗代码
    3.项目脱敏，不要包含客户项目的任何信息

# 研发步骤

    1.代码调通&注释补充&单机训练脚本  [OK]
      数据集： None的时候处理       [OK]
    2.补充分布式训练脚本            [OK]     
        完善READM.md 
        如何添加开源数据集，这部分直接在REEADME.md中完整给出样例
        1.增量预训练    [ok]  zero2+zero3+cpu_offload  
        2.微调训练      [ok]  zero2+zero3+cpu_offload
        3.强化学习       
            DPO        [ok]  zero2+zero3+cpu_offload
            GRPO       [ok]  zero2+zero3+cpu_offload  (确认下）
            ORPO       [ok]  zero2+zero3+cpu_offload  (确认下）
            PPO        [ok]  zero2
            RM         [ok]  zero2+zero3+cpu_offload    
        4.注意
            Zero3支持：    ORPO，DPO，RM,GRPO(最好用zero2)
            Zero3不支持：  PPO（不稳定，不要使用）
            Zero2支持：    GRPO，PPO，ORPO，DPO，RM
    3.规范化处理脚本               [OK]
    4.规范化数据结构&数据收集整理    []
    5.完善研究：
        代码规优化： 中间补充的优化代码整理和注释
        非必须代码： 兼容qwen模型之外模型的代码注释与整理
        核心算法：   注释与公式补充
    6.数据处理代码开发

# 执行脚本

    要构造三类并行计算脚本，例如：
    # torchrun
    torchrun --nproc_per_node=4 ppo_training.py
    
    # deepspeed
    deepspeed --num_gpus=4 ppo_training.py
    
    # accelerate
    accelerate launch --multi_gpu ppo_training.py

### 基于大模型写脚本

    参考脚本
    参考run_orpo.sh,orpo_training.py
        1.基于torchrun实现分布式训练脚本 
        2.支持deepspeed zero2,zero3模式
        3.zero3模式支持cpu_offload
        4.训练脚本写到DPO目录中的ORPO/torchrun_multi_node_multi_gpu.sh中

    参考run_rm.sh,rm_training.py，为rm_training.py写训练脚本：
        1.基于torchrun实现分布式训练脚本 
        2.支持deepspeed zero2,zero3,cpu_offload模式
        3.配置写到RM/ds_config.json中
        4.训练脚本写到RM目录中的RM/torchrun_multi_node_multi_gpu.sh中
        5.参考DPO/torchrun/目录下的ds_config.json,torchrun_multi_node_multi_gpu.sh


    
    
